{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T10:37:56.278Z",
    "slug": "ariegoldkin-evidence-verification",
    "source_url": "https://github.com/ArieGoldkin/ai-agent-hub/tree/main/skills/evidence-verification",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "c6e926d9589b67f47bc5b9fe6b6bea11a5117b907eb1b373bfa153671cbbad43",
    "tree_hash": "3b0add40974c2fe3c1d29535e021d40e1694e10e8a7dd29f801f5105f73b176a"
  },
  "skill": {
    "name": "Evidence-Based Verification Skill",
    "description": "**Version:** 1.0.0",
    "summary": "**Version:** 1.0.0",
    "icon": "üîç",
    "version": "1.0.0",
    "author": "ArieGoldkin",
    "license": "MIT",
    "category": "security",
    "tags": [
      "quality-assurance",
      "verification",
      "testing",
      "deployment",
      "documentation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation and prompt-based skill. Contains only markdown templates and workflow documentation for evidence collection. No executable code, scripts, network calls, or file system access.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 1700,
    "audit_model": "claude",
    "audited_at": "2026-01-10T10:37:56.278Z"
  },
  "content": {
    "user_title": "Verify work with evidence before completion",
    "value_statement": "AI agents often claim tasks are complete without proof. This skill ensures every claim is backed by executable evidence like test results, build status, and coverage metrics. It prevents hallucinations and ensures production quality.",
    "seo_keywords": [
      "evidence verification",
      "AI quality assurance",
      "test verification",
      "Claude Code verification",
      "code completion verification",
      "deployment verification",
      "test coverage validation",
      "build verification",
      "code review checklist",
      "production readiness"
    ],
    "actual_capabilities": [
      "Collect test execution evidence with exit codes and coverage metrics",
      "Document build success including artifacts and error counts",
      "Run linter and type checker verification checks",
      "Create structured evidence reports for task completion",
      "Verify deployment health and smoke test results",
      "Store evidence in shared context for team visibility"
    ],
    "limitations": [
      "Skill provides templates and guidance but does not execute commands itself",
      "Evidence collection depends on the AI agent running verification commands",
      "Does not automatically fix failing tests or build errors",
      "Requires user to have test, build, and lint commands configured"
    ],
    "use_cases": [
      {
        "target_user": "AI Developers",
        "title": "Task Completion Verification",
        "description": "Ensure AI agents provide proof of work before marking tasks complete with structured evidence reports."
      },
      {
        "target_user": "QA Engineers",
        "title": "Quality Gate Enforcement",
        "description": "Enforce minimum quality standards by requiring test coverage, build success, and lint checks before deployment."
      },
      {
        "target_user": "DevOps Teams",
        "title": "Deployment Verification",
        "description": "Verify production deployments with health checks, smoke tests, and rollback capability confirmation."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Test Verification",
        "scenario": "Verify tests pass",
        "prompt": "Run the test suite and document the evidence. Capture the exit code, number of tests passed, failed, and skipped. Include coverage percentage if available. Store this evidence before marking the task complete."
      },
      {
        "title": "Build Verification",
        "scenario": "Verify build succeeds",
        "prompt": "Run the build command and document the evidence. Capture the exit code, any errors or warnings, and list the artifacts created. Note the bundle size and build duration."
      },
      {
        "title": "Code Quality Check",
        "scenario": "Run linting and type checks",
        "prompt": "Execute the linter and type checker. Document the exit codes, error counts, and warning counts. Only proceed if both checks pass with zero errors."
      },
      {
        "title": "Full Evidence Report",
        "scenario": "Complete verification package",
        "prompt": "Run tests, build, linter, and type checker. Create a combined evidence report showing exit codes, coverage metrics, artifact details, and quality standard achieved. Include timestamp and environment details."
      }
    ],
    "output_examples": [
      {
        "input": "Verify the user authentication feature is complete",
        "output": [
          "‚úÖ Tests: Exit 0 (24 passed, 0 failed, coverage 87.5%)",
          "‚úÖ Build: Exit 0 (bundle created: 2.6 MB)",
          "‚úÖ Linter: Exit 0 (0 errors, 3 warnings)",
          "‚úÖ Types: Exit 0 (no type errors)",
          "üìã Quality Standard: Production-Grade ‚úÖ",
          "Timestamp: 2025-11-02 14:30:22",
          "Task complete with verification evidence."
        ]
      }
    ],
    "best_practices": [
      "Always capture exit codes - zero means success, non-zero indicates failure",
      "Run verification commands fresh for each change - do not rely on stale results",
      "Store evidence in shared context so team members can review verification status",
      "Reject task completion if any evidence shows failure - do not fake or ignore results"
    ],
    "anti_patterns": [
      "Skipping evidence collection and assuming the code works correctly",
      "Recording 'tests passed' without actually running the test command",
      "Ignoring failed evidence because 'the code looks correct'",
      "Reusing old evidence from previous sessions without re-running verification"
    ],
    "faq": [
      {
        "question": "Which AI tools support this skill?",
        "answer": "This skill works with Claude, Codex, and Claude Code. It provides structured prompts for evidence collection."
      },
      {
        "question": "What minimum evidence is required?",
        "answer": "At minimum, capture one verification type (tests, build, or lint) with exit code and timestamp recorded."
      },
      {
        "question": "How does evidence integrate with my workflow?",
        "answer": "Evidence is stored in shared context under quality_evidence. Other skills can read this to verify completion status."
      },
      {
        "question": "Is my data safe with this skill?",
        "answer": "Yes. This skill only reads verification command outputs you run. No data is sent externally or stored beyond your project."
      },
      {
        "question": "What if tests or build fail?",
        "answer": "Document the failure with evidence. Do not mark the task complete. Fix the issues and re-run verification until all evidence passes."
      },
      {
        "question": "How is this different from a test runner?",
        "answer": "This skill does not run tests. It provides templates and prompts for documenting evidence from tests, builds, and checks you run."
      }
    ]
  },
  "file_structure": [
    {
      "name": "examples",
      "type": "dir",
      "path": "examples",
      "children": [
        {
          "name": "evidence-example.json",
          "type": "file",
          "path": "examples/evidence-example.json"
        }
      ]
    },
    {
      "name": "templates",
      "type": "dir",
      "path": "templates",
      "children": [
        {
          "name": "build-evidence.md",
          "type": "file",
          "path": "templates/build-evidence.md"
        },
        {
          "name": "evidence-checklist.md",
          "type": "file",
          "path": "templates/evidence-checklist.md"
        },
        {
          "name": "test-evidence.md",
          "type": "file",
          "path": "templates/test-evidence.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
