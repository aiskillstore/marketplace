{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T11:31:46.146Z",
    "slug": "brownbull-data-orchestrator",
    "source_url": "https://github.com/Brownbull/ayni_core/tree/main/.claude/skills/data-orchestrator",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "c9649b6306012295a7e87d0542d8bb371d6805ccd0fd4e47f0b8e41707557e3c",
    "tree_hash": "3301e9d4942baf8aba0f59293a1ed6ec2278900b7cdfbc922447181d2e863d36"
  },
  "skill": {
    "name": "data-orchestrator",
    "description": "Coordinates data pipeline tasks (ETL, analytics, feature engineering). Use when implementing data ingestion, transformations, quality checks, or analytics. Applies data-quality-standard.md (95% minimum).",
    "summary": "Coordinates data pipeline tasks (ETL, analytics, feature engineering). Use when implementing data in...",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "Brownbull",
    "license": "MIT",
    "category": "data",
    "tags": [
      "data-pipelines",
      "etl",
      "analytics",
      "feature-engineering"
    ],
    "supported_tools": [
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure prompt-based skill definition containing only documentation and guidance. No executable code, scripts, network calls, file system access, or command execution capabilities exist in this skill.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 236,
    "audit_model": "claude",
    "audited_at": "2026-01-10T11:31:46.146Z"
  },
  "content": {
    "user_title": "Coordinate data pipelines and analytics tasks",
    "value_statement": "Building data pipelines and ensuring quality requires coordinating ETL processes, feature engineering, and validation checks. This skill acts as your data engineering lead, managing the entire pipeline lifecycle from ingestion to quality assurance.",
    "seo_keywords": [
      "data orchestrator",
      "ETL pipeline",
      "data quality",
      "feature engineering",
      "Claude Code",
      "data pipeline management",
      "analytics coordination",
      "data governance"
    ],
    "actual_capabilities": [
      "Coordinates ETL and ELT data pipeline execution",
      "Manages feature engineering and model integration",
      "Validates data quality against six quality dimensions",
      "Tracks data lineage and schema changes",
      "Monitors pipeline performance and resource usage"
    ],
    "limitations": [
      "This is a prompt-based orchestration skill without direct execution capabilities",
      "Requires external data processing tools for actual computation",
      "Cannot access external databases or cloud services directly"
    ],
    "use_cases": [
      {
        "target_user": "Data Engineers",
        "title": "Build data pipelines",
        "description": "Design and coordinate ETL workflows with proper validation, checkpointing, and monitoring"
      },
      {
        "target_user": "Analytics Teams",
        "title": "Coordinate feature engineering",
        "description": "Manage feature creation pipelines with dependency tracking and quality gates"
      },
      {
        "target_user": "Data Governance Teams",
        "title": "Ensure data quality",
        "description": "Validate data across completeness, accuracy, consistency, timeliness, uniqueness, and validity"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic pipeline setup",
        "scenario": "Create a simple ETL pipeline",
        "prompt": "Set up a daily ETL pipeline that extracts data from my API, transforms it to calculate daily metrics, and loads results to the data warehouse with proper validation checks"
      },
      {
        "title": "Feature creation",
        "scenario": "Engineer new features",
        "prompt": "Create a feature engineering pipeline for customer churn prediction including feature calculation, versioning, and monitoring setup"
      },
      {
        "title": "Quality validation",
        "scenario": "Validate data quality",
        "prompt": "Implement a data quality framework that checks completeness, accuracy, consistency, and timeliness for my transaction data with automated alerting"
      },
      {
        "title": "Pipeline troubleshooting",
        "scenario": "Debug pipeline failures",
        "prompt": "Analyze my failing data pipeline. Review the error logs, identify the root cause, implement a fix, and add monitoring to prevent recurrence"
      }
    ],
    "output_examples": [
      {
        "input": "Create a pipeline to process customer transactions and calculate revenue metrics",
        "output": [
          "Pipeline design: Extract transactions from source, validate records, calculate daily/monthly revenue features, load to feature store",
          "Quality gates: Completeness check (no null required fields), Accuracy check (amounts > 0), Timeliness check (data within 24 hours)",
          "Monitoring: Track records processed, validation failures, processing duration, quality score per run"
        ]
      }
    ],
    "best_practices": [
      "Always implement input validation before processing any data pipeline",
      "Add checkpoint and recovery mechanisms for long-running pipelines",
      "Track data lineage from source to final output for compliance and debugging"
    ],
    "anti_patterns": [
      "Processing data without validation checks - always validate before and after transformations",
      "Running pipelines without monitoring or alerting for failures",
      "Missing data lineage documentation - cannot trace issues without tracking"
    ],
    "faq": [
      {
        "question": "What data sources can this skill work with?",
        "answer": "This skill coordinates pipelines for any data source. Actual connectivity depends on your execution environment and configured connectors."
      },
      {
        "question": "What quality standards does this skill enforce?",
        "answer": "It validates against six dimensions: completeness, accuracy, consistency, timeliness, uniqueness, and validity. Minimum 95% score required."
      },
      {
        "question": "Can this skill execute Python or SQL code?",
        "answer": "No. This skill provides coordination and guidance. Actual code execution requires integration with your data processing tools."
      },
      {
        "question": "How does this skill ensure data privacy?",
        "answer": "It enforces privacy compliance through schema management, access control guidelines, and data handling procedures during pipeline design."
      },
      {
        "question": "What happens when a pipeline fails?",
        "answer": "The skill guides you through failure analysis, root cause identification, and implementing fixes with recovery mechanisms."
      },
      {
        "question": "How is this different from a data orchestrator like Airflow?",
        "answer": "This skill provides AI-guided coordination and best practices. Airflow provides scheduler infrastructure. They complement each other."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
