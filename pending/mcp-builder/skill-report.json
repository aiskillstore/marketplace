{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2025-12-31T08:40:41.325Z",
    "slug": "mcp-builder",
    "source_url": "https://github.com/anthropics/skills/tree/main/skills/mcp-builder",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "official"
  },
  "skill": {
    "name": "mcp-builder",
    "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
    "summary": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact wi...",
    "icon": "ðŸ§­",
    "version": "1.0.0",
    "license": "Complete terms in LICENSE.txt",
    "category": "documentation",
    "tags": [
      "mcp",
      "servers",
      "evaluation",
      "typescript",
      "python"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network",
      "env_access"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is an official skill and receives a minimal audit. No risky patterns were identified in the reviewed files. It is safe to publish.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 10,
    "total_lines": 3526,
    "audit_model": "codex",
    "audited_at": "2025-12-31T08:40:41.324Z"
  },
  "content": {
    "user_title": "Build MCP servers with clear guidance and checks",
    "value_statement": "You need MCP servers that tools can use reliably. This guide gives a phased workflow, checklists, and evaluation tooling to validate quality.",
    "seo_keywords": [
      "MCP server guide",
      "Model Context Protocol",
      "Claude",
      "Codex",
      "Claude Code",
      "TypeScript MCP SDK",
      "Python FastMCP",
      "MCP evaluation harness",
      "tool schema design",
      "streamable HTTP"
    ],
    "actual_capabilities": [
      "Outlines a four phase workflow from research to evaluation.",
      "Provides TypeScript and Python implementation guidance for tool schemas, errors, and pagination.",
      "Includes an evaluation harness to run XML QA pairs using Claude.",
      "Supports stdio, SSE, and streamable HTTP connections for evaluations.",
      "Generates evaluation reports with accuracy, timing, and tool call metrics."
    ],
    "limitations": [
      "Does not generate MCP server code automatically.",
      "Evaluation harness requires a running MCP server and Anthropic credentials.",
      "Guidance assumes you can access external API documentation.",
      "No built in security scanning beyond best practice advice."
    ],
    "use_cases": [
      {
        "target_user": "API integrator",
        "title": "Design tool coverage",
        "description": "Plan comprehensive tool coverage and naming for a new MCP server."
      },
      {
        "target_user": "ML engineer",
        "title": "Validate server quality",
        "description": "Run evaluations to measure accuracy and tool usage behavior."
      },
      {
        "target_user": "QA analyst",
        "title": "Build stable test set",
        "description": "Create verifiable evaluation questions and expected answers."
      }
    ],
    "prompt_templates": [
      {
        "title": "Start a new MCP plan",
        "scenario": "You need a basic build plan",
        "prompt": "Create a phase by phase plan to build an MCP server for a customer support API. Include key tools and data models."
      },
      {
        "title": "Define tool schemas",
        "scenario": "You want clear input and output",
        "prompt": "Draft tool names and input schemas for a ticketing API. Keep names action based and include pagination guidance."
      },
      {
        "title": "Prepare evaluations",
        "scenario": "You need QA pairs",
        "prompt": "Generate 10 read only evaluation questions for a billing MCP server. Ensure each answer is stable and verifiable."
      },
      {
        "title": "Set up evaluation run",
        "scenario": "You want to run the harness",
        "prompt": "Show how to run the evaluation script for a local stdio MCP server with a custom model and an output report file."
      }
    ],
    "output_examples": [
      {
        "input": "Create a plan for an MCP server that integrates a CRM API.",
        "output": [
          "Phase 1: Review CRM API endpoints and auth flows",
          "Phase 2: Define tool list and schemas with pagination",
          "Phase 3: Implement client, error handling, and tools",
          "Phase 4: Build evaluations and run the harness"
        ]
      }
    ],
    "best_practices": [
      "Use clear tool names with action prefixes and consistent naming.",
      "Return focused responses and support pagination for large lists.",
      "Add evaluation questions that are realistic and stable."
    ],
    "anti_patterns": [
      "Creating vague tool names that hide intent.",
      "Returning massive unfiltered data without pagination.",
      "Using evaluation questions that rely on changing data."
    ],
    "faq": [
      {
        "question": "What MCP transports does this cover?",
        "answer": "It covers stdio, SSE, and streamable HTTP for server connections."
      },
      {
        "question": "What are the main limits of the guidance?",
        "answer": "It does not replace API docs or generate full server code for you."
      },
      {
        "question": "How does it integrate with my MCP server?",
        "answer": "Use the evaluation harness to connect to your server and run XML QA tasks."
      },
      {
        "question": "Is my data sent anywhere by default?",
        "answer": "Only evaluation requests sent to your MCP server and Claude are used."
      },
      {
        "question": "What if the evaluation script fails?",
        "answer": "Check server transport settings, credentials, and XML formatting."
      },
      {
        "question": "How does it compare to ad hoc testing?",
        "answer": "It provides structured QA pairs and measurable accuracy metrics."
      }
    ]
  },
  "file_structure": [
    {
      "name": "reference",
      "type": "dir",
      "path": "reference",
      "children": [
        {
          "name": "evaluation.md",
          "type": "file",
          "path": "reference/evaluation.md"
        },
        {
          "name": "mcp_best_practices.md",
          "type": "file",
          "path": "reference/mcp_best_practices.md"
        },
        {
          "name": "node_mcp_server.md",
          "type": "file",
          "path": "reference/node_mcp_server.md"
        },
        {
          "name": "python_mcp_server.md",
          "type": "file",
          "path": "reference/python_mcp_server.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "connections.py",
          "type": "file",
          "path": "scripts/connections.py"
        },
        {
          "name": "evaluation.py",
          "type": "file",
          "path": "scripts/evaluation.py"
        },
        {
          "name": "example_evaluation.xml",
          "type": "file",
          "path": "scripts/example_evaluation.xml"
        },
        {
          "name": "requirements.txt",
          "type": "file",
          "path": "scripts/requirements.txt"
        }
      ]
    },
    {
      "name": "LICENSE.txt",
      "type": "file",
      "path": "LICENSE.txt"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}