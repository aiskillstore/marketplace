{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-06T07:39:17.342Z",
    "slug": "obra-verification-before-completion",
    "source_url": "https://github.com/obra/superpowers/tree/main/skills/verification-before-completion",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "963f6b63d58d88e029181126528a1c720f76c4f8cfaec3532ece14795aef5b77",
    "tree_hash": "a47ef4ef5b04c2bc29852a290adc9789c5480fde2db005d4f45b021355653408"
  },
  "skill": {
    "name": "verification-before-completion",
    "description": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always",
    "summary": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - req...",
    "icon": "üõ°Ô∏è",
    "version": "1.0.0",
    "author": "obra",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "verification",
      "quality",
      "testing",
      "best-practices"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a prompt-only skill containing guidelines and documentation. No executable code, scripts, network calls, filesystem access, or environment variable reads. Pure policy/guideline content.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 140,
    "audit_model": "claude",
    "audited_at": "2026-01-06T07:39:17.342Z"
  },
  "content": {
    "user_title": "Verify before claiming completion",
    "value_statement": "AI assistants often claim completion without evidence, damaging trust. This skill enforces a verification gate that requires running actual verification commands and citing fresh evidence before making any success claims.",
    "seo_keywords": [
      "verification",
      "testing",
      "code review",
      "quality assurance",
      "Claude",
      "Codex",
      "Claude Code",
      "completion claims",
      "test verification",
      "build verification"
    ],
    "actual_capabilities": [
      "Requires running verification commands before claiming success",
      "Enforces evidence-based completion claims with command output",
      "Prevents partial or assumed verification",
      "Validates build, test, and linter results independently",
      "Applies to commits, PRs, and task completion"
    ],
    "limitations": [
      "Does not execute verification commands itself",
      "Does not know which verification commands to run for your project",
      "Requires user to provide specific verification commands",
      "Does not replace code review or human oversight"
    ],
    "use_cases": [
      {
        "target_user": "Software developers",
        "title": "Validate tests pass",
        "description": "Run actual test commands and verify exit codes before claiming tests pass"
      },
      {
        "target_user": "Code reviewers",
        "title": "Confirm build succeeds",
        "description": "Execute build commands and cite exit code evidence before PR approval"
      },
      {
        "target_user": "QA engineers",
        "title": "Verify bug fixes",
        "description": "Reproduce original bug symptoms and confirm fix with fresh test results"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic completion check",
        "scenario": "Before claiming work is done",
        "prompt": "Run the test command and show me the full output. Does it pass? What is the exact exit code?"
      },
      {
        "title": "Build verification",
        "scenario": "Before committing code",
        "prompt": "What is the build command for this project? Run it and report the exit status and any error output."
      },
      {
        "title": "Regression test cycle",
        "scenario": "Verifying TDD tests",
        "prompt": "Run the test in red state to confirm failure, then after your fix run it again to confirm green. Show both outputs."
      },
      {
        "title": "Agent verification",
        "scenario": "Delegating to sub-agents",
        "prompt": "The agent reports success. Check the VCS diff to verify actual changes. What specific files were modified?"
      }
    ],
    "output_examples": [
      {
        "input": "The feature is complete and all tests pass",
        "output": [
          "Tests pass: 127/127 tests passing",
          "Exit code: 0",
          "Build successful",
          "All verification commands passed"
        ]
      }
    ],
    "best_practices": [
      "Always run the exact verification command rather than assuming success",
      "Check the actual exit code and count failures explicitly",
      "Cite specific command output as evidence for any completion claim"
    ],
    "anti_patterns": [
      "Claiming tests pass without running them",
      "Trusting agent success reports without independent verification",
      "Using partial checks like linter passing as proof of build success"
    ],
    "faq": [
      {
        "question": "Does this skill run tests automatically?",
        "answer": "No. This skill enforces that you must run verification commands yourself and cite their output."
      },
      {
        "question": "What verification commands should I use?",
        "answer": "Use your project's standard commands like npm test, pytest, make test, or cargo build."
      },
      {
        "question": "Does this work with any programming language?",
        "answer": "Yes. The skill applies to any project with verifiable test, build, or lint commands."
      },
      {
        "question": "Is my test data sent anywhere?",
        "answer": "No. This skill only provides guidelines. Test output remains in your local environment."
      },
      {
        "question": "How is this different from normal testing?",
        "answer": "This skill prevents shortcut claims. You must show actual command output, not assumptions."
      },
      {
        "question": "Can I combine this with other skills?",
        "answer": "Yes. Use this skill alongside testing frameworks to ensure verification evidence is always provided."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
