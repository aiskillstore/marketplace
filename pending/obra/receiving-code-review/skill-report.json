{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-06T07:28:48.926Z",
    "slug": "obra-receiving-code-review",
    "source_url": "https://github.com/obra/superpowers/tree/main/skills/receiving-code-review",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "cbf580a8b2b65fba03646a47223264654d7037c8b1901236c1f485a005fb64df",
    "tree_hash": "24d30aebd5b261a7bd3e481f9390dbb4fed8e9762fc2423ced2528a7977a9465"
  },
  "skill": {
    "name": "receiving-code-review",
    "description": "Use when receiving code review feedback, before implementing suggestions, especially if feedback seems unclear or technically questionable - requires technical rigor and verification, not performative agreement or blind implementation",
    "summary": "Use when receiving code review feedback, before implementing suggestions, especially if feedback see...",
    "icon": "üëÅÔ∏è",
    "version": "1.0.0",
    "author": "obra",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "code review",
      "feedback",
      "technical rigor",
      "verification"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure prompt-based skill containing only markdown documentation. No executable code, no file system access, no network calls, no environment variable access. Contains behavioral guidelines only.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 214,
    "audit_model": "claude",
    "audited_at": "2026-01-06T07:28:48.926Z"
  },
  "content": {
    "user_title": "Receive code review with technical rigor",
    "value_statement": "Code review feedback often contains unclear or technically questionable suggestions. This skill provides a verification framework to evaluate feedback, ask clarifying questions, and respond with technical reasoning rather than blind agreement.",
    "seo_keywords": [
      "code review",
      "feedback handling",
      "Claude Code",
      "technical verification",
      "code review best practices",
      "push back on feedback",
      "code review workflow",
      "developer productivity",
      "software engineering",
      "code quality"
    ],
    "actual_capabilities": [
      "Verifies code review feedback against codebase reality before implementation",
      "Identifies unclear feedback items requiring clarification",
      "Evaluates suggestions for technical correctness and YAGNI violations",
      "Provides technical pushback frameworks with reasoning",
      "Guides one-at-a-time implementation with individual testing",
      "Distinguishes trusted partner feedback from external reviewer suggestions"
    ],
    "limitations": [
      "Does not execute code or run tests automatically",
      "Does not access external documentation or APIs",
      "Does not modify code files directly",
      "Requires human partner for architectural decisions"
    ],
    "use_cases": [
      {
        "target_user": "New developers",
        "title": "Learning proper review etiquette",
        "description": "Developers learning to receive feedback technically rather than emotionally, avoiding blind agreement"
      },
      {
        "target_user": "Senior engineers",
        "title": "Evaluating external suggestions",
        "description": "Senior engineers skeptical of external reviewer suggestions who need verification frameworks"
      },
      {
        "target_user": "Code review authors",
        "title": "Understanding feedback patterns",
        "description": "Team members who want to understand how their feedback will be received and processed"
      }
    ],
    "prompt_templates": [
      {
        "title": "Unclear feedback",
        "scenario": "Feedback has ambiguous items",
        "prompt": "I received code review feedback with items 1-6. Items 1, 2, 3, and 6 are clear. Items 4 and 5 are ambiguous. How should I proceed?"
      },
      {
        "title": "Technically questionable",
        "scenario": "Reviewer suggests breaking changes",
        "prompt": "A reviewer suggested removing legacy code. How do I verify if this breaks backward compatibility before implementing?"
      },
      {
        "title": "YAGNI check",
        "scenario": "Reviewer requests new features",
        "prompt": "The reviewer wants me to implement metrics tracking with database, date filters, and CSV export. How do I check if this is actually needed?"
      },
      {
        "title": "External feedback",
        "scenario": "External reviewer suggestions",
        "prompt": "An external reviewer suggested changes that might conflict with my human partner's prior architectural decisions. What is the verification process?"
      }
    ],
    "output_examples": [
      {
        "input": "Reviewer said to fix items 1-6, but items 4 and 5 are unclear. How do I handle this?",
        "output": [
          "STOP - do not implement any items yet",
          "Ask for clarification on items 4 and 5 before proceeding",
          "Items may be related, so partial understanding leads to wrong implementation",
          "Example response: 'I understand items 1, 2, 3, and 6. Need clarification on 4 and 5 before proceeding.'"
        ]
      }
    ],
    "best_practices": [
      "Always verify feedback against codebase reality before implementing any changes",
      "Ask clarifying questions when any item is unclear rather than making assumptions",
      "Push back with technical reasoning when suggestions are technically incorrect for the codebase"
    ],
    "anti_patterns": [
      "Performative agreement with phrases like 'You're absolutely right!' or 'Great point!'",
      "Blind implementation without first checking against codebase reality",
      "Batching multiple items without testing each one individually"
    ],
    "faq": [
      {
        "question": "Is this skill compatible with all Claude models?",
        "answer": "Yes. This skill works with Claude, Codex, and Claude Code. It provides behavioral guidelines that work across all AI assistants."
      },
      {
        "question": "How many feedback items can this skill handle at once?",
        "answer": "There is no limit on the number of items. The skill emphasizes clarifying unclear items before proceeding with any implementation."
      },
      {
        "question": "Does this skill integrate with GitHub or other code hosting platforms?",
        "answer": "This skill provides response patterns. Integration with platforms like GitHub requires separate tools or the gh CLI."
      },
      {
        "question": "Is my code data safe when using this skill?",
        "answer": "Yes. This skill only provides guidelines. It does not read, write, or transmit any code data externally."
      },
      {
        "question": "What if I cannot verify a suggestion?",
        "answer": "State your limitation clearly. Say: 'I cannot verify this without [X]. Should I [investigate/ask/proceed]?'"
      },
      {
        "question": "How does this differ from standard code review tools?",
        "answer": "This skill focuses on how to receive and evaluate feedback, not the review process itself. It complements tools like GitHub PR comments or GitLab merge requests."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
