{
  "skill": {
    "name": "LatchBio Integration",
    "description": "Latch platform for bioinformatics workflows. Build pipelines with Latch SDK, @workflow/@task decorators, deploy serverless workflows, LatchFile/LatchDir, Nextflow/Snakemake integration.",
    "summary": "Build and deploy serverless bioinformatics pipelines with the Latch SDK",
    "icon": "ðŸ§¬",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "Unknown",
    "category": "data",
    "tags": ["bioinformatics", "workflows", "pipelines", "data-science"],
    "supported_tools": ["claude", "codex", "claude-code"]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Documentation-only skill with no executable code. All 260 static findings are false positives caused by the scanner misinterpreting Python code examples in markdown documentation. The backticks flagged as 'shell backtick execution' are markdown syntax delimiters, not Ruby code. Secrets management references use legitimate Latch SDK APIs. No actual security risks present.",
    "static_findings_evaluation": [
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution (201 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "The scanner incorrectly interprets backtick characters (`) in markdown code blocks as Ruby/shell backtick execution. These are Python code examples in documentation files (SKILL.md, references/*.md), not actual executable code. All code blocks contain legitimate Latch SDK examples for bioinformatics workflows."
      },
      {
        "finding": "[MEDIUM] filesystem: Python file write/append at references/data-management.md:64",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "This is a documentation example showing standard Python file I/O operations in a workflow context. No actual file system operations are performed by the skill itself - this is documentation teaching users how to use LatchFile for cloud storage."
      },
      {
        "finding": "[MEDIUM] filesystem: Temp directory access at references/resource-configuration.md:276",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation example explaining ephemeral storage configuration in the Latch platform. This is legitimate documentation about temporary file storage, not a security concern."
      },
      {
        "finding": "[HIGH] env_access: Generic API/secret keys at references/data-management.md:380",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "References to `get_secret('api_key')` use the legitimate Latch SDK secrets management API. This is the recommended secure way to handle credentials in workflows, not a vulnerability."
      },
      {
        "finding": "[MEDIUM] scripts: Dynamic import() expression at references/verified-workflows.md:13",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "This is a standard Python import statement `from latch.verified import (...)`, not a dynamic import(). The scanner incorrectly flagged the import syntax in documentation."
      },
      {
        "finding": "[LOW] network: Hardcoded URL (8 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "All URLs are public documentation links (docs.latch.bio, GitHub repositories, Slack community). No sensitive endpoints or credential exfiltration targets."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm (multiple locations)",
        "verdict": "false_positive",
        "confidence": "medium",
        "reasoning": "The scanner incorrectly flags Python decorators and function patterns as 'weak cryptographic algorithms'. No cryptographic operations are performed by this documentation skill."
      },
      {
        "finding": "[CRITICAL] obfuscation: DANGEROUS COMBINATION Code execution + Network + Credential access",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "This heuristic triggers on legitimate bioinformatics workflow documentation. The combination of code execution, network access, and credential handling is inherent to cloud-based workflow platforms. This is documented best practice, not obfuscation or malicious behavior."
      }
    ],
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 1954
  },
  "content": {
    "user_title": "Build bioinformatics pipelines with Latch SDK",
    "value_statement": "Deploy production-ready bioinformatics workflows without managing infrastructure. Create serverless pipelines using Python decorators with automatic containerization, GPU support, and integrated cloud storage.",
    "seo_keywords": [
      "Latch bioinformatics workflows",
      "Claude scientific skills",
      "Codex data pipelines",
      "workflow automation",
      "bioinformatics platform",
      "Snakemake Nextflow integration",
      "AlphaFold ColabFold",
      "RNA-seq pipeline",
      "serverless workflows",
      "GPU-accelerated analysis"
    ],
    "actual_capabilities": [
      "Create serverless bioinformatics workflows using @workflow and @task decorators",
      "Deploy Python, Nextflow, or Snakemake pipelines with automatic Docker containerization",
      "Configure compute resources including CPU, memory, and GPU (K80, V100, A100)",
      "Integrate pre-built verified workflows for RNA-seq, AlphaFold, DESeq2, and single-cell analysis",
      "Manage cloud data with LatchFile and LatchDir abstractions",
      "Organize experimental data using the Registry system with Projects, Tables, and Records"
    ],
    "limitations": [
      "Requires Latch account and Docker installation for workflow registration",
      "Verified workflows are platform-specific and may require additional configuration",
      "Task resource decorators must be statically defined at registration time"
    ],
    "use_cases": [
      {
        "target_user": "Bioinformatics researchers",
        "title": "Deploy RNA-seq analysis pipelines",
        "description": "Build complete transcriptomics workflows from quality control through differential expression analysis."
      },
      {
        "target_user": "Computational biologists",
        "title": "Run protein structure prediction",
        "description": "Execute AlphaFold or ColabFold jobs with automatic GPU resource allocation and cloud storage."
      },
      {
        "target_user": "Data scientists",
        "title": "Integrate verified bioinformatics tools",
        "description": "Combine pre-built workflows with custom steps for specialized analysis pipelines."
      }
    ],
    "prompt_templates": [
      {
        "title": "Create basic workflow",
        "scenario": "Build your first Latch workflow",
        "prompt": "Create a Latch workflow that processes files using the @small_task decorator and returns a LatchFile result."
      },
      {
        "title": "Configure GPU resources",
        "scenario": "Set up GPU-accelerated tasks",
        "prompt": "Configure a Latch task to use an A100 GPU for deep learning model execution with custom resource specifications."
      },
      {
        "title": "Import existing pipeline",
        "scenario": "Convert Nextflow/Snakemake pipelines",
        "prompt": "Show how to register an existing Nextflow or Snakemake pipeline to the Latch platform."
      },
      {
        "title": "Build multi-step pipeline",
        "scenario": "Create complex analysis workflows",
        "prompt": "Create a complete RNA-seq pipeline with quality control, alignment, and quantification tasks using Latch decorators."
      }
    ],
    "output_examples": [
      {
        "input": "Create a Latch workflow for protein structure prediction",
        "output": [
          "âœ“ Use @large_gpu_task decorator with nvidia-tesla-v100 GPU",
          "âœ“ Import alphafold from latch.verified module",
          "âœ“ Configure input via LatchFile for FASTA sequence",
          "âœ“ Set output directory with LatchDir for PDB results",
          "âœ“ Platform automatically handles Docker containerization",
          "âœ“ Monitor execution via Latch dashboard"
        ]
      }
    ],
    "best_practices": [
      "Start with standard task decorators (@small_task, @large_task) and scale resources only when profiling shows need",
      "Use type annotations and docstrings for all parameters to auto-generate workflow interfaces",
      "Test workflows locally with Docker before registering to the platform"
    ],
    "anti_patterns": [
      "Avoid over-provisioning resources - GPU tasks cost significantly more than CPU tasks",
      "Do not use dynamic resource configuration at runtime - decorators must be static",
      "Avoid mixing multiple workflow frameworks in a single pipeline without clear separation"
    ],
    "faq": [
      {
        "question": "What bioinformatics tools are available as verified workflows?",
        "answer": "Latch provides verified workflows for bulk RNA-seq, DESeq2, AlphaFold, ColabFold, MAFTT, Trim Galore, ArchR, scVelo, CRISPResso2, and phylogenetics."
      },
      {
        "question": "How do I configure GPU resources for my workflow?",
        "answer": "Use @small_gpu_task or @large_gpu_task decorators, or specify gpu and gpu_type parameters in @custom_task for precise control."
      },
      {
        "question": "Can I import existing Nextflow or Snakemake pipelines?",
        "answer": "Yes, use latch register --nextflow or latch register --snakemake commands to import existing pipelines with automatic containerization."
      },
      {
        "question": "How does LatchFile differ from local file paths?",
        "answer": "LatchFile is a cloud storage reference. The SDK automatically downloads files to local paths during execution and uploads results back to cloud storage."
      },
      {
        "question": "What compute resources are available?",
        "answer": "CPU up to 96 cores, memory up to 768 GB, GPU options include K80, V100, and A100, with configurable ephemeral storage."
      },
      {
        "question": "How do I organize experimental data in the Registry?",
        "answer": "Create Projects containing Tables with Records. Use column types like string, number, file, link, and enum to structure your data model."
      }
    ]
  }
}
