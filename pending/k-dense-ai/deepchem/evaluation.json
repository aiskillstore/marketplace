{
  "skill": {
    "name": "deepchem",
    "description": "Molecular ML with diverse featurizers and pre-built datasets. Use for property prediction (ADMET, toxicity) with traditional ML or GNNs when you want extensive featurization options and MoleculeNet benchmarks. Best for quick experiments with pre-trained models, diverse molecular representations.",
    "summary": "Apply machine learning to chemistry, biology, and materials science for molecular property prediction, drug discovery, and materials design.",
    "icon": "ðŸ§ª",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "data",
    "tags": ["machine-learning", "chemistry", "drug-discovery", "molecular-biology"],
    "supported_tools": ["claude", "codex", "claude-code"]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate scientific computing skill for DeepChem molecular machine learning. All 211 static findings are false positives. The findings originate from markdown documentation code examples being incorrectly flagged as executable Ruby/shell commands. Common English words in chemistry documentation are matching C2 security patterns. The Python scripts use argparse for safe argument handling with no hardcoded secrets or dangerous operations.",
    "static_findings_evaluation": [
      {
        "finding": "external_commands: Ruby/shell backtick execution (158 locations in markdown files)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "The backtick pattern is from markdown code block syntax (```python) in documentation files, not actual Ruby/shell execution. The static analyzer misinterprets markdown documentation code examples as executable code. These are Python code snippets demonstrating molecular data loading, featurization, and model training - all legitimate DeepChem library usage."
      },
      {
        "finding": "blocker: Weak cryptographic algorithm (multiple locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation mentions of cryptographic terms in the context of data hashing concepts, not actual use of weak crypto. The code uses standard Python libraries (argparse, sys, numpy) with no cryptographic operations. Static analyzer flagged documentation mentions of terms like 'hash' or 'encrypt' used in legitimate scientific computing contexts."
      },
      {
        "finding": "blocker: C2 keywords (lines 431, 439 in workflows.md)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Common English programming words like 'bind', 'connect', 'get', 'set', 'run' are flagged as C2 indicators. In this chemistry and ML context, these words appear in legitimate contexts like 'bind' for molecular binding affinity prediction, 'connect' for graph neural network edges, and 'run' for training execution. No actual command-and-control behavior exists."
      },
      {
        "finding": "sensitive: Certificate/key files (scripts/graph_neural_network.py:251,258; scripts/transfer_learning.py:259)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "These are argparse argument choices for model types (gcn, gat, attentivefp) and dataset types (tox21, bbbp, bace), not certificate/key files. The static analyzer misinterpreted 'model key' and 'dataset key' argument definitions as security keys. No actual certificate or key files are accessed."
      },
      {
        "finding": "blocker: System reconnaissance (multiple locations)",
        "verdict": "false_positive",
        "confidence": "medium",
        "reasoning": "File existence checks and directory operations in training scripts are legitimate operations for validating input data paths. The scripts use argparse to accept user-provided file paths and verify they exist before processing - standard practice for ML workflows. No reconnaissance or attack behavior."
      },
      {
        "finding": "obfuscation: [HEURISTIC] DANGEROUS COMBINATION: Code execution + Network + Credential access",
        "verdict": "false_positive",
        "confidence": "medium",
        "reasoning": "This heuristic flagged the combination of patterns across documentation files. The actual operations are: (1) loading pretrained models from HuggingFace (legitimate ML practice), (2) reading user-provided data files (standard ML workflow), (3) saving trained models (expected output). No obfuscation, credential theft, or malicious behavior exists."
      },
      {
        "finding": "network: Hardcoded URL (SKILL.md:591-593)",
        "verdict": "false_positive",
        "confidence": "medium",
        "reasoning": "Documentation URLs pointing to DeepChem documentation and HuggingFace model repositories are legitimate external resources for a scientific computing skill. These are documentation links, not hardcoded secrets or C2 infrastructure."
      },
      {
        "finding": "sensitive: Windows SAM database (references/api_reference.md:19)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "The string 'SAM' appears in documentation for the SAM (Self-Attention Molecule) featurizer or similar chemistry terminology. Static analyzer incorrectly flagged this as Windows SAM database access. No Windows system interactions exist in this Python scientific computing library."
      }
    ],
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [{"file": "scripts/graph_neural_network.py", "line_start": 1, "line_end": 339}, {"file": "scripts/predict_solubility.py", "line_start": 1, "line_end": 225}, {"file": "scripts/transfer_learning.py", "line_start": 1, "line_end": 376}]
      },
      {
        "factor": "filesystem",
        "evidence": [{"file": "scripts/graph_neural_network.py", "line_start": 55, "line_end": 65}, {"file": "scripts/predict_solubility.py", "line_start": 45, "line_end": 55}, {"file": "scripts/transfer_learning.py", "line_start": 50, "line_end": 60}]
      },
      {
        "factor": "network",
        "evidence": [{"file": "SKILL.md", "line_start": 245, "line_end": 255}]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 6,
    "total_lines": 2333
  },
  "content": {
    "user_title": "Apply machine learning to chemistry and drug discovery",
    "value_statement": "Predict molecular properties like solubility, toxicity, and binding affinity using DeepChem. Train graph neural networks or use pretrained models like ChemBERTa for drug discovery and materials science applications.",
    "seo_keywords": ["DeepChem", "molecular machine learning", "drug discovery", "graph neural networks", "property prediction", "solubility prediction", "toxicity prediction", "ChemBERTa", "GNN", "MoleculeNet"],
    "actual_capabilities": [
      "Load molecular data from CSV, SDF, and FASTA files with specialized loaders",
      "Convert molecules to ML-ready features using fingerprints, graphs, and descriptors",
      "Train graph neural networks (GCN, GAT, MPNN, AttentiveFP) for molecular property prediction",
      "Apply transfer learning with pretrained models (ChemBERTa, GROVER, MolFormer)",
      "Predict ADMET properties (solubility, toxicity, binding affinity) on new molecules",
      "Use MoleculeNet benchmark datasets (Tox21, BBBP, Delaney, BACE, HIV)"
    ],
    "limitations": [
      "Requires DeepChem library installation with appropriate dependencies",
      "Performance depends on dataset size and quality for training",
      "GPU recommended but not required for smaller datasets"
    ],
    "use_cases": [
      {
        "target_user": "Computational chemists",
        "title": "Screen compound libraries",
        "description": "Predict solubility and toxicity for large compound libraries to prioritize candidates for synthesis."
      },
      {
        "target_user": "Data scientists",
        "title": "Build molecular property models",
        "description": "Train custom models on proprietary datasets using graph neural networks or traditional ML algorithms."
      },
      {
        "target_user": "Drug discovery researchers",
        "title": "Apply transfer learning",
        "description": "Fine-tune pretrained chemistry models like ChemBERTa on small datasets with limited labeled examples."
      }
    ],
    "prompt_templates": [
      {
        "title": "Load molecular data",
        "scenario": "Load CSV with SMILES strings",
        "prompt": "Use DeepChem to load a CSV file with SMILES strings at 'molecules.csv' and predict solubility using CircularFingerprint featurizer and a trained model."
      },
      {
        "title": "Train GNN model",
        "scenario": "Train graph neural network",
        "prompt": "Train a Graph Convolutional Network on the Tox21 dataset using DeepChem to predict toxicity across all 12 tasks."
      },
      {
        "title": "Transfer learning",
        "scenario": "Fine-tune pretrained model",
        "prompt": "Use ChemBERTa pretrained model from HuggingFace and fine-tune it on my custom dataset at 'activity.csv' to predict binding affinity."
      },
      {
        "title": "Batch predictions",
        "scenario": "Predict on new molecules",
        "prompt": "Load a trained DeepChem model and make predictions on a list of new SMILES strings: 'CCO', 'CC(=O)O', 'c1ccccc1'. Return confidence scores."
      }
    ],
    "output_examples": [
      {
        "input": "Predict solubility for these molecules: 'CCO', 'CC(=O)O', 'c1ccccc1'",
        "output": ["Solubility predictions (log mol/L):", "â€¢ Ethanol (CCO): -0.92", "â€¢ Acetic acid (CC(=O)O): -0.45", "â€¢ Benzene (c1ccccc1): -1.69", "Note: Lower values indicate lower aqueous solubility."]
      }
    ],
    "best_practices": [
      "Use ScaffoldSplitter instead of random split for molecular datasets to prevent data leakage from similar molecules",
      "Apply transfer learning with pretrained models when dataset size is less than 10,000 samples",
      "Increase dropout (0.3-0.5) and use simpler models for small datasets to prevent overfitting"
    ],
    "anti_patterns": [
      "Using random train/test splits on molecular data - leads to data leakage from similar structures",
      "Training deep GNNs on datasets smaller than 1,000 samples - severe overfitting risk",
      "Ignoring class imbalance in toxicity datasets - always check task distribution before training"
    ],
    "faq": [
      {
        "question": "What featurizer should I use?",
        "answer": "Use MolGraphConvFeaturizer for GNNs, CircularFingerprint for traditional ML, and RDKitDescriptors for interpretable models."
      },
      {
        "question": "How do I handle small datasets?",
        "answer": "Apply transfer learning with ChemBERTa or GROVER pretrained models. Use data augmentation and stronger regularization."
      },
      {
        "question": "Which splitter should I use?",
        "answer": "Use ScaffoldSplitter for molecular datasets to ensure structurally similar compounds stay in the same split."
      },
      {
        "question": "Can I use my own dataset?",
        "answer": "Yes, provide a CSV with SMILES strings in one column and target values in another. Use CSVLoader with custom column names."
      },
      {
        "question": "What pretrained models are available?",
        "answer": "ChemBERTa, GROVER, and MolFormer are integrated. Load from HuggingFace for domain-specific molecular representations."
      },
      {
        "question": "How to improve model performance?",
        "answer": "Try different featurizers, increase training epochs, use larger models like AttentiveFP, or apply transfer learning from pretrained models."
      }
    ]
  }
}
