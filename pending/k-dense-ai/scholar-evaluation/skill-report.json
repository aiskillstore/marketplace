{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-12T16:47:36.830Z",
    "slug": "k-dense-ai-scholar-evaluation",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/scholar-evaluation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "84f46c9092b5b791caa76bb607797003eda5e52cd8b109b9089566034fe518dc",
    "tree_hash": "bbe43dd38e8aca11272ff816456dea2529ed940b912d3f1e9dc5186694c158ba"
  },
  "skill": {
    "name": "scholar-evaluation",
    "description": "Systematically evaluate scholarly work using the ScholarEval framework, providing structured assessment across research quality dimensions including problem formulation, methodology, analysis, and writing with quantitative scoring and actionable feedback.",
    "summary": "Systematically evaluate scholarly work using the ScholarEval framework, providing structured assessm...",
    "icon": "ðŸŽ“",
    "version": "1.0.0",
    "author": "K-Dense-AI",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "academic",
      "evaluation",
      "research",
      "peer-review"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "filesystem",
      "external_commands",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a documentation-only skill for academic evaluation. All 58 static findings are FALSE POSITIVES. The scanner misinterpreted academic terminology as security patterns. No cryptographic code, no actual shell execution, no credential access, and no malicious patterns exist. This skill processes academic text and produces evaluation reports.",
    "risk_factor_evidence": [
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 291,
            "line_end": 291
          },
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 236,
            "line_end": 236
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 40,
            "line_end": 42
          },
          {
            "file": "SKILL.md",
            "line_start": 42,
            "line_end": 87
          },
          {
            "file": "SKILL.md",
            "line_start": 87,
            "line_end": 162
          },
          {
            "file": "SKILL.md",
            "line_start": 162,
            "line_end": 230
          },
          {
            "file": "SKILL.md",
            "line_start": 230,
            "line_end": 232
          },
          {
            "file": "SKILL.md",
            "line_start": 232,
            "line_end": 251
          },
          {
            "file": "SKILL.md",
            "line_start": 251,
            "line_end": 270
          },
          {
            "file": "SKILL.md",
            "line_start": 270,
            "line_end": 270
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 295,
            "line_end": 295
          },
          {
            "file": "SKILL.md",
            "line_start": 295,
            "line_end": 295
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 1343,
    "audit_model": "claude",
    "audited_at": "2026-01-12T16:47:36.830Z"
  },
  "content": {
    "user_title": "Evaluate scholarly papers and research work",
    "value_statement": "This skill provides structured academic evaluation using the ScholarEval framework. It helps researchers and reviewers assess research quality across multiple dimensions including methodology, analysis, and writing. Perfect for peer review preparation and quality benchmarking.",
    "seo_keywords": [
      "scholar evaluation",
      "academic paper review",
      "research quality assessment",
      "peer review tool",
      "scientific writing evaluation",
      "methodology assessment",
      "Claude Code",
      "Claude",
      "Codex"
    ],
    "actual_capabilities": [
      "Evaluate research papers across 8 quality dimensions",
      "Generate quantitative scores using weighted averaging",
      "Provide actionable feedback with strengths and weaknesses",
      "Assess methodology rigor and reproducibility",
      "Analyze literature review comprehensiveness",
      "Review scholarly writing and citation quality"
    ],
    "limitations": [
      "Does not access external databases or perform live literature searches",
      "Cannot verify factual claims against real-world data sources",
      "Does not replace domain expert human review",
      "Limited to text-based evaluation without visual inspection of figures"
    ],
    "use_cases": [
      {
        "target_user": "Researchers preparing manuscripts",
        "title": "Pre-submission quality check",
        "description": "Self-evaluate research papers before journal submission to identify areas for improvement."
      },
      {
        "target_user": "Peer reviewers and editors",
        "title": "Structured review assessment",
        "description": "Apply consistent evaluation criteria across manuscript reviews with quantitative scoring."
      },
      {
        "target_user": "Graduate students",
        "title": "Thesis and dissertation feedback",
        "description": "Get structured feedback on academic writing quality and methodology rigor."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick paper evaluation",
        "scenario": "Evaluate a research paper",
        "prompt": "Evaluate this research paper using the ScholarEval framework. Focus on problem formulation, methodology, and analysis rigor."
      },
      {
        "title": "Literature review assessment",
        "scenario": "Assess literature review quality",
        "prompt": "Assess the literature review section of this paper using ScholarEval criteria. Check comprehensiveness, synthesis, and gap identification."
      },
      {
        "title": "Methodology review",
        "scenario": "Evaluate research methodology",
        "prompt": "Review the methodology section using ScholarEval. Evaluate design appropriateness, validity, and reproducibility."
      },
      {
        "title": "Full comprehensive review",
        "scenario": "Complete manuscript evaluation",
        "prompt": "Conduct a full ScholarEval assessment of this manuscript across all 8 dimensions. Provide scores and detailed feedback."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate this research paper on machine learning for drug discovery",
        "output": [
          "Overall Assessment: Strong (4.2/5.0)",
          "Key Strengths: Clear research questions, appropriate deep learning methods, reproducible code",
          "Areas for Improvement: Limited dataset diversity, needs more validation on external data",
          "Methodology Score: 4.5/5.0",
          "Writing Quality: 4.0/5.0"
        ]
      }
    ],
    "best_practices": [
      "Provide the full text or substantial excerpt of the work to enable comprehensive evaluation",
      "Specify the target venue or publication stage for context-appropriate assessment",
      "Use the scoring script to track improvements across multiple revision rounds",
      "Combine ScholarEval with human expert review for final publication decisions"
    ],
    "anti_patterns": [
      "Do not use for evaluating non-academic content like business proposals or marketing materials",
      "Do not rely solely on scores without reading detailed feedback and recommendations",
      "Do not expect domain-specific expertise beyond the evaluation framework criteria"
    ],
    "faq": [
      {
        "question": "What dimensions does ScholarEval assess?",
        "answer": "It evaluates 8 dimensions: problem formulation, literature review, methodology, data collection, analysis, results, writing, and citations."
      },
      {
        "question": "How are scores calculated?",
        "answer": "The skill uses weighted averaging with configurable weights. Default weights prioritize methodology and analysis at 20% each."
      },
      {
        "question": "Can I use this for grant proposals?",
        "answer": "Yes, adjust the evaluation context to emphasize feasibility, impact, and significance for proposal assessment."
      },
      {
        "question": "Does this access external databases?",
        "answer": "No, this is a text-based evaluation framework. It does not perform live literature searches or fact-checking."
      },
      {
        "question": "How accurate are the scores?",
        "answer": "Scores provide structured quantitative assessment but should complement, not replace, expert human review."
      },
      {
        "question": "What file formats work best?",
        "answer": "Plain text or markdown work best. Provide clear section markers for the most accurate dimensional assessment."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "evaluation_framework.md",
          "type": "file",
          "path": "references/evaluation_framework.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "calculate_scores.py",
          "type": "file",
          "path": "scripts/calculate_scores.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
