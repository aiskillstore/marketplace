{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-12T16:19:33.612Z",
    "slug": "k-dense-ai-stable-baselines3",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/stable-baselines3",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "800be32e288ccfea6834356a7d8e16fc87fe4e557e22c63153b1d13b52083009",
    "tree_hash": "0ffdd856d3aaef2dec1df259960910bc86ffbc0b727bd41c433514a594ef8e91"
  },
  "skill": {
    "name": "stable-baselines3",
    "description": "Production-ready reinforcement learning algorithms (PPO, SAC, DQN, TD3, DDPG, A2C) with scikit-learn-like API. Use for standard RL experiments, quick prototyping, and well-documented algorithm implementations. Best for single-agent RL with Gymnasium environments. For high-performance parallel training, multi-agent systems, or custom vectorized environments, use pufferlib instead.",
    "summary": "Production-ready reinforcement learning algorithms (PPO, SAC, DQN, TD3, DDPG, A2C) with scikit-learn...",
    "icon": "ðŸ¤–",
    "version": "1.0.0",
    "author": "K-Dense-AI",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "pytorch",
      "gymnasium"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate documentation/reference skill for the Stable Baselines3 reinforcement learning library. All 399 static findings are false positives caused by the analyzer misinterpreting markdown code formatting, RL algorithm names, and standard ML patterns. No malicious behavior, credential access, or data exfiltration was found.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/train_rl_agent.py",
            "line_start": 15,
            "line_end": 15
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/train_rl_agent.py",
            "line_start": 50,
            "line_end": 53
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "references/algorithms.md",
            "line_start": 1,
            "line_end": 330
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 3013,
    "audit_model": "claude",
    "audited_at": "2026-01-12T16:19:33.612Z"
  },
  "content": {
    "user_title": "Train reinforcement learning agents with Stable Baselines3",
    "value_statement": "Stable Baselines3 provides production-ready RL algorithms including PPO, SAC, DQN, and TD3. This skill offers comprehensive training templates, algorithm selection guides, and custom environment development patterns for single-agent reinforcement learning tasks.",
    "seo_keywords": [
      "stable-baselines3",
      "reinforcement learning",
      "PPO SAC DQN TD3",
      "Claude Code",
      "Gymnasium",
      "PyTorch",
      "machine learning",
      "robotics AI",
      "agent training"
    ],
    "actual_capabilities": [
      "Train agents using PPO, SAC, DQN, TD3, A2C, DDPG, and HER algorithms",
      "Create custom Gymnasium environments following SB3 conventions",
      "Implement monitoring callbacks with TensorBoard integration",
      "Set up vectorized environments for parallel training",
      "Save, load, and evaluate trained RL models",
      "Generate video recordings of agent behavior"
    ],
    "limitations": [
      "Designed for single-agent RL only (not multi-agent systems)",
      "Requires Gymnasium-compatible environments",
      "Does not include pre-trained models (only training templates)",
      "Performance depends on PyTorch and environment simulation speed"
    ],
    "use_cases": [
      {
        "target_user": "ML researchers",
        "title": "Algorithm experimentation",
        "description": "Compare PPO, SAC, DQN performance on custom tasks with proper hyperparameter guidance"
      },
      {
        "target_user": "Robotics engineers",
        "title": "Continuous control training",
        "description": "Train agents for robotic manipulation using SAC and TD3 with vectorized environments"
      },
      {
        "target_user": "Game AI developers",
        "title": "Discrete action agents",
        "description": "Build and evaluate game-playing agents using DQN with Atari-compatible environments"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic training setup",
        "scenario": "Quick start with PPO",
        "prompt": "Show me how to train a PPO agent on CartPole-v1 using stable-baselines3 with a complete training script example."
      },
      {
        "title": "Custom environment",
        "scenario": "Create RL environment",
        "prompt": "Help me create a custom Gymnasium environment for stable-baselines3. What methods must I implement and what should the observation and action spaces look like?"
      },
      {
        "title": "Algorithm selection",
        "scenario": "Choose right algorithm",
        "prompt": "I have a continuous control task. Should I use SAC or TD3? What are the key differences and which hyperparameters should I tune?"
      },
      {
        "title": "Advanced monitoring",
        "scenario": "Custom callbacks",
        "prompt": "Create a custom callback for stable-baselines3 that logs custom metrics to TensorBoard and saves checkpoints every 10000 steps."
      }
    ],
    "output_examples": [
      {
        "input": "How do I train a SAC agent for continuous control?",
        "output": [
          "SAC (Soft Actor-Critic) is ideal for continuous control tasks. Key setup:",
          "â€¢ Use MlpPolicy with continuous action spaces (Box space)",
          "â€¢ Set learning_rate around 3e-4 with batch_size 256",
          "â€¢ For off-policy algorithms like SAC, set gradient_steps=-1 with vectorized envs",
          "â€¢ Store replay buffer (default size 1M transitions)",
          "â€¢ Use target_entropy='auto' for automatic entropy tuning",
          "See scripts/train_rl_agent.py for a complete training template with callbacks."
        ]
      }
    ],
    "best_practices": [
      "Always validate custom environments with check_env() before training",
      "Use vectorized environments (SubprocVecEnv) for faster training on multi-core systems",
      "Set eval_freq adjusted by n_envs when using parallel training environments",
      "Save VecNormalize statistics separately if using observation/reward normalization"
    ],
    "anti_patterns": [
      "Do not use Discrete/MultiDiscrete action spaces with start != 0 (SB3 limitation)",
      "Do not forget that replay buffer is NOT saved with model.save() for off-policy algorithms",
      "Do not set n_envs too high without monitoring memory usage (replay buffers multiply)",
      "Do not evaluate on training environment - use separate evaluation environment"
    ],
    "faq": [
      {
        "question": "What algorithm should I start with?",
        "answer": "PPO is the most versatile choice. It works with all action space types, is stable, and easy to tune. Start here for most tasks."
      },
      {
        "question": "How do I save and load models?",
        "answer": "Use model.save('path') to save and PPO.load('path', env=env) to load. Replay buffers save separately."
      },
      {
        "question": "Can I use images as observations?",
        "answer": "Yes, use CnnPolicy for image observations. Images must be np.uint8 in range [0, 255] with channel-first format."
      },
      {
        "question": "How do I monitor training progress?",
        "answer": "Use tensorboard_log parameter and run 'tensorboard --logdir ./logs/' to view metrics in real-time."
      },
      {
        "question": "What environments work with SB3?",
        "answer": "Any Gymnasium-compatible environment. SB3 validates with check_env() before training."
      },
      {
        "question": "How do I handle goal-conditioned tasks?",
        "answer": "Use HER (Hindsight Experience Replay) with SAC or TD3. Requires Dict observation space with achieved_goal and desired_goal."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "algorithms.md",
          "type": "file",
          "path": "references/algorithms.md"
        },
        {
          "name": "callbacks.md",
          "type": "file",
          "path": "references/callbacks.md"
        },
        {
          "name": "custom_environments.md",
          "type": "file",
          "path": "references/custom_environments.md"
        },
        {
          "name": "vectorized_envs.md",
          "type": "file",
          "path": "references/vectorized_envs.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "custom_env_template.py",
          "type": "file",
          "path": "scripts/custom_env_template.py"
        },
        {
          "name": "evaluate_agent.py",
          "type": "file",
          "path": "scripts/evaluate_agent.py"
        },
        {
          "name": "train_rl_agent.py",
          "type": "file",
          "path": "scripts/train_rl_agent.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
