{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T04:17:51.100Z",
    "slug": "K-Dense-AI-stable-baselines3",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/stable-baselines3",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "stable-baselines3",
    "description": "Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DDPG, A2C, etc.), creating custom Gym environments, implementing callbacks for monitoring and control, using vectorized environments for parallel training, and integrating with deep RL workflows. This skill should be used when users request RL algorithm implementation, agent training, environment design, or RL experimentation.",
    "summary": "Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DD...",
    "icon": "ðŸ¤–",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "reinforcement-learning",
      "stable-baselines3",
      "gymnasium",
      "training",
      "evaluation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No credential access, environment harvesting, or network exfiltration patterns were found. The code provides local training and evaluation templates without remote execution or persistence behavior.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 3003,
    "audit_model": "codex",
    "audited_at": "2026-01-02T04:17:51.100Z"
  },
  "content": {
    "user_title": "Train Stable Baselines3 agents faster",
    "value_statement": "Training RL agents can be slow and hard to structure. This skill gives proven templates for training, evaluation, and custom environments.",
    "seo_keywords": [
      "Stable Baselines3",
      "reinforcement learning",
      "Gymnasium environments",
      "PPO training",
      "RL evaluation",
      "Claude",
      "Codex",
      "Claude Code",
      "vectorized environments",
      "SB3 callbacks"
    ],
    "actual_capabilities": [
      "Train SB3 agents with vectorized environments and callbacks",
      "Create custom Gymnasium environments with validation guidance",
      "Evaluate models with mean and standard deviation rewards",
      "Record evaluation videos with VecVideoRecorder",
      "Compare multiple saved models in one run"
    ],
    "limitations": [
      "Requires stable-baselines3 and gymnasium installed locally",
      "Templates do not include automatic hyperparameter tuning",
      "Video recording depends on environment rendering support",
      "No distributed training beyond local subprocess vectorization"
    ],
    "use_cases": [
      {
        "target_user": "researcher",
        "title": "Benchmark algorithms quickly",
        "description": "Train PPO or SAC with consistent callbacks and compare results across models."
      },
      {
        "target_user": "engineer",
        "title": "Build a custom Gym environment",
        "description": "Start from a validated environment template and adapt observations and rewards."
      },
      {
        "target_user": "student",
        "title": "Evaluate a trained agent",
        "description": "Load a model, run episodes, and record a short video for reports."
      }
    ],
    "prompt_templates": [
      {
        "title": "Train a first agent",
        "scenario": "New to SB3 and need a baseline run",
        "prompt": "Train PPO on CartPole-v1 with 4 environments and default callbacks. Save logs and the final model."
      },
      {
        "title": "Custom environment setup",
        "scenario": "I need a template for my task",
        "prompt": "Create a custom Gymnasium environment template with reset and step implemented. Include check_env validation and a short test loop."
      },
      {
        "title": "Evaluate and record",
        "scenario": "I have a saved model to review",
        "prompt": "Evaluate ./models/best_model.zip on CartPole-v1 for 10 episodes and record a video to ./videos."
      },
      {
        "title": "Compare model runs",
        "scenario": "I have several checkpoints",
        "prompt": "Compare three SB3 model files and report mean and standard deviation rewards for each."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate my saved PPO model and summarize the results",
        "output": [
          "Loaded model from ./models/best_model.zip",
          "Evaluated 10 episodes on CartPole-v1",
          "Mean reward 195.3 with standard deviation 6.8"
        ]
      }
    ],
    "best_practices": [
      "Validate custom environments with check_env before training",
      "Adjust eval and save frequency when using multiple environments",
      "Save VecNormalize statistics when you use normalization"
    ],
    "anti_patterns": [
      "Training without evaluation or checkpoints",
      "Using SubprocVecEnv for very lightweight environments",
      "Ignoring observation and reward normalization in continuous control tasks"
    ],
    "faq": [
      {
        "question": "Is this compatible with the latest Stable Baselines3?",
        "answer": "Yes, the templates use standard SB3 APIs and Gymnasium patterns."
      },
      {
        "question": "What are the main limits of the templates?",
        "answer": "They are examples only and do not cover full project setup or tuning."
      },
      {
        "question": "Can I integrate this with my existing codebase?",
        "answer": "Yes, the scripts are modular and can be copied into your project."
      },
      {
        "question": "Does the skill access my data or credentials?",
        "answer": "No, it only reads local model files you provide."
      },
      {
        "question": "What should I do if training crashes?",
        "answer": "Run check_env and verify observation and action space definitions."
      },
      {
        "question": "How does this compare to a full RL framework?",
        "answer": "This provides templates and guidance, not a full training platform."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "algorithms.md",
          "type": "file",
          "path": "references/algorithms.md"
        },
        {
          "name": "callbacks.md",
          "type": "file",
          "path": "references/callbacks.md"
        },
        {
          "name": "custom_environments.md",
          "type": "file",
          "path": "references/custom_environments.md"
        },
        {
          "name": "vectorized_envs.md",
          "type": "file",
          "path": "references/vectorized_envs.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "custom_env_template.py",
          "type": "file",
          "path": "scripts/custom_env_template.py"
        },
        {
          "name": "evaluate_agent.py",
          "type": "file",
          "path": "scripts/evaluate_agent.py"
        },
        {
          "name": "train_rl_agent.py",
          "type": "file",
          "path": "scripts/train_rl_agent.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}