{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T03:31:32.952Z",
    "slug": "K-Dense-AI-pufferlib",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/pufferlib",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "pufferlib",
    "description": "This skill should be used when working with reinforcement learning tasks including high-performance RL training, custom environment development, vectorized parallel simulation, multi-agent systems, or integration with existing RL environments (Gymnasium, PettingZoo, Atari, Procgen, etc.). Use this skill for implementing PPO training, creating PufferEnv environments, optimizing RL performance, or developing policies with CNNs/LSTMs.",
    "summary": "This skill should be used when working with reinforcement learning tasks including high-performance ...",
    "icon": "ðŸš€",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "reinforcement-learning",
      "pytorch",
      "vectorization",
      "multi-agent",
      "ppo"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No credential access, environment harvesting, or network exfiltration patterns were found. The files are documentation and templates without hidden execution or persistence behavior.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 3711,
    "audit_model": "codex",
    "audited_at": "2026-01-02T03:31:32.951Z"
  },
  "content": {
    "user_title": "Build fast RL training with PufferLib",
    "value_statement": "Training RL agents at scale is slow and complex. This skill provides templates and guidance for fast PPO training, vectorization, and custom environments.",
    "seo_keywords": [
      "PufferLib",
      "reinforcement learning",
      "PPO training",
      "vectorized environments",
      "multi-agent RL",
      "Gymnasium integration",
      "Claude",
      "Codex",
      "Claude Code",
      "PyTorch policy"
    ],
    "actual_capabilities": [
      "Provides a PPO training script template with PuffeRL, logging, and checkpointing",
      "Defines a PyTorch policy template with actor and critic heads using layer_init",
      "Includes single-agent and multi-agent PufferEnv templates with space definitions",
      "Documents vectorized environment creation with pufferlib.make and emulate",
      "Lists integration patterns for Gymnasium and PettingZoo environments"
    ],
    "limitations": [
      "Templates require you to implement task-specific environment logic and rewards",
      "External loggers need separate accounts and API tokens",
      "Performance depends on hardware and vectorization settings",
      "No built in evaluation suite beyond training loop examples"
    ],
    "use_cases": [
      {
        "target_user": "RL researcher",
        "title": "Prototype PPO baselines",
        "description": "Start a reproducible PPO training loop with vectorized environments and logging."
      },
      {
        "target_user": "Game AI developer",
        "title": "Create custom environments",
        "description": "Build single or multi agent PufferEnv tasks with defined spaces and reset logic."
      },
      {
        "target_user": "ML engineer",
        "title": "Scale training throughput",
        "description": "Configure workers and environment counts for higher steps per second."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick start PPO",
        "scenario": "Beginner setup",
        "prompt": "Create a minimal training plan using train_template.py for procgen coinrun with CPU settings and basic hyperparameters."
      },
      {
        "title": "Custom environment",
        "scenario": "Single agent task",
        "prompt": "Outline a PufferEnv skeleton for a grid world with discrete actions and vector observations."
      },
      {
        "title": "Multi agent design",
        "scenario": "Parallel agents",
        "prompt": "Describe a multi agent PufferEnv structure with per agent observations and shared policy."
      },
      {
        "title": "Performance tuning",
        "scenario": "Advanced scaling",
        "prompt": "Recommend vectorization settings for 1024 environments across 16 workers and explain tradeoffs."
      }
    ],
    "output_examples": [
      {
        "input": "Help me set up PPO training for gym CartPole with logging.",
        "output": [
          "Use the training template with env name gym-CartPole-v1",
          "Set num_envs to a small value for CPU testing",
          "Choose wandb logger and add project and run name",
          "Start with default PPO hyperparameters and adjust later"
        ]
      }
    ],
    "best_practices": [
      "Start with template scripts and change one parameter at a time",
      "Profile steps per second before changing architecture",
      "Validate environment reset and step outputs early"
    ],
    "anti_patterns": [
      "Copying templates without adjusting environment specific logic",
      "Scaling num_envs without enough CPU or memory",
      "Ignoring reward and done semantics when vectorizing"
    ],
    "faq": [
      {
        "question": "Is this compatible with Gymnasium and PettingZoo?",
        "answer": "Yes. The references document integration patterns for both frameworks."
      },
      {
        "question": "What are the limits of the templates?",
        "answer": "They are examples and must be customized for your environment and policy."
      },
      {
        "question": "Can I integrate external loggers?",
        "answer": "Yes. The training template shows WandB and Neptune loggers."
      },
      {
        "question": "Does it access my data or credentials?",
        "answer": "No. The files are documentation and templates without data collection."
      },
      {
        "question": "What if training is slow?",
        "answer": "Reduce environment count first, then tune workers and batch size."
      },
      {
        "question": "How does it compare to generic PPO code?",
        "answer": "It focuses on vectorized environments and throughput using PufferLib."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "environments.md",
          "type": "file",
          "path": "references/environments.md"
        },
        {
          "name": "integration.md",
          "type": "file",
          "path": "references/integration.md"
        },
        {
          "name": "policies.md",
          "type": "file",
          "path": "references/policies.md"
        },
        {
          "name": "training.md",
          "type": "file",
          "path": "references/training.md"
        },
        {
          "name": "vectorization.md",
          "type": "file",
          "path": "references/vectorization.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "env_template.py",
          "type": "file",
          "path": "scripts/env_template.py"
        },
        {
          "name": "train_template.py",
          "type": "file",
          "path": "scripts/train_template.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}