{
  "skill": {
    "name": "advanced-evaluation",
    "description": "This skill should be used when the user asks to implement LLM-as-judge, compare model outputs, create evaluation rubrics, mitigate evaluation bias, or mentions direct scoring, pairwise comparison, position bias, evaluation pipelines, or automated quality assessment.",
    "summary": "Production-grade techniques for evaluating LLM outputs using LLMs as judges with bias mitigation",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "Muratcan Koylan",
    "license": "MIT",
    "category": "data",
    "tags": ["llm-evaluation", "quality-assessment", "bias-mitigation", "model-comparison"],
    "supported_tools": ["claude", "codex", "claude-code"]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure prompt-based skill containing evaluation methodologies and pseudocode examples. No network calls, file I/O, or external command execution. Safe for publication.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 1730
  },
  "content": {
    "user_title": "Evaluate LLM outputs with reliable scoring",
    "value_statement": "Manual evaluation of LLM outputs is slow and inconsistent. This skill provides production-grade techniques for automated evaluation using LLMs as judges. It includes direct scoring, pairwise comparison, and bias mitigation strategies.",
    "seo_keywords": [
      "llm evaluation",
      "llm-as-judge",
      "automated quality assessment",
      "pairwise comparison",
      "direct scoring",
      "bias mitigation",
      "Claude evaluation",
      "AI model comparison",
      "evaluation rubric",
      "machine learning evaluation"
    ],
    "actual_capabilities": [
      "Implement direct scoring with weighted criteria",
      "Perform pairwise comparison with position swap",
      "Generate domain-specific evaluation rubrics",
      "Detect and mitigate evaluation biases",
      "Calibrate confidence scores based on evidence",
      "Design multi-model evaluation panels"
    ],
    "limitations": [
      "Requires an LLM API for actual scoring",
      "Cannot access external files or databases",
      "Does not perform human evaluation",
      "Bias mitigation reduces but does not eliminate all bias"
    ],
    "use_cases": [
      {
        "target_user": "Machine learning engineers",
        "title": "Automate model comparison",
        "description": "Compare multiple model outputs to select the best response for a given prompt"
      },
      {
        "target_user": "AI researchers",
        "title": "Evaluate new techniques",
        "description": "Assess whether new prompting strategies improve output quality"
      },
      {
        "target_user": "Data science teams",
        "title": "Monitor output quality",
        "description": "Track quality metrics across production deployments over time"
      }
    ],
    "prompt_templates": [
      {
        "title": "Direct scoring",
        "scenario": "Rate response against criteria",
        "prompt": "Evaluate this response on a 1-5 scale for accuracy, clarity, and completeness. Response: {response}. Provide scores with evidence and improvement suggestions."
      },
      {
        "title": "Pairwise comparison",
        "scenario": "Compare two responses",
        "prompt": "Compare Response A and Response B for the following prompt. Ignore length. Return which is better and why: Prompt: {prompt}, A: {response_a}, B: {response_b}."
      },
      {
        "title": "Rubric generation",
        "scenario": "Create scoring criteria",
        "prompt": "Create a 1-5 rubric for evaluating {criterion} in {domain}. Include level descriptions and specific characteristics for each score."
      },
      {
        "title": "Bias assessment",
        "scenario": "Detect evaluation bias",
        "prompt": "Analyze this evaluation for position bias, length bias, or authority bias. Check if the judge followed the rubric consistently: {evaluation_result}."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate this response on accuracy, clarity, and completeness: 'Quantum entanglement is like two connected coins. When one shows heads, the other shows tails instantly.'",
        "output": [
          "Accuracy: 4/5 - Correctly describes correlated outcomes but oversimplifies quantum mechanics",
          "Clarity: 5/5 - Excellent analogy for general audience",
          "Completeness: 3/5 - Missing key details about quantum systems",
          "Overall: 4/5 - Good explanation for beginners but could include more scientific depth"
        ]
      }
    ],
    "best_practices": [
      "Always require justification before scores to improve reliability by 15-25%",
      "Use position swapping in pairwise comparison to mitigate position bias",
      "Separate objective criteria (use direct scoring) from subjective preferences (use pairwise comparison)"
    ],
    "anti_patterns": [
      "Single-pass pairwise comparison without position swap (introduces position bias)",
      "Scoring without requiring evidence or justification",
      "Using the same model for generation and evaluation (self-enhancement bias)"
    ],
    "faq": [
      {
        "question": "Which models work best for LLM-as-Judge?",
        "answer": "GPT-4 and Claude 3 show highest agreement with human judges. Use smaller models only for screening."
      },
      {
        "question": "How many criteria should evaluation rubrics have?",
        "answer": "Limit to 5-7 criteria per rubric. More criteria reduce reliability and increase cognitive load."
      },
      {
        "question": "Can I use this skill with my own evaluation API?",
        "answer": "Yes. This skill provides the prompts and patterns. Integrate with your LLM API for actual scoring."
      },
      {
        "question": "Does this skill send my data to external servers?",
        "answer": "No. This skill generates evaluation prompts only. No network calls or data transmission occur."
      },
      {
        "question": "Why do pairwise comparisons sometimes result in ties?",
        "answer": "Ties indicate position bias was detected or responses are genuinely equivalent. Return tie with reduced confidence."
      },
      {
        "question": "How does this compare to human evaluation?",
        "answer": "LLM evaluation correlates well with humans for many tasks but cannot replace human judgment for subjective or safety-critical decisions."
      }
    ]
  }
}