{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-23T02:25:30.228Z",
    "slug": "emillindfors-object-store-best-practices",
    "source_url": "https://github.com/EmilLindfors/claude-marketplace/tree/main/plugins/rust-data-engineering/skills/object-store-best-practices",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "685e8f556dc32a11dcb0fd2dde33653f0511a318dcf8868ea3f52fa9c5b041cd",
    "tree_hash": "6cb9a8c15910ee47d74c2d1646f3ffebc2ba69bc7575fabe62e8f5d39833521c"
  },
  "skill": {
    "name": "object-store-best-practices",
    "description": "Ensures proper cloud storage operations with retry logic, error handling, streaming, and efficient I/O patterns. Activates when users work with object_store for S3, Azure, or GCS operations.",
    "summary": "Expert guidance for robust cloud storage operations using the object_store Rust crate",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "EmilLindfors",
    "license": "MIT",
    "tags": [
      "cloud-storage",
      "aws-s3",
      "azure-blob",
      "gcs",
      "rust"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 83 static findings are false positives. The scanner misinterpreted Rust code examples in documentation as security threats. This is a legitimate documentation skill providing best practices for the object_store Rust crate. Findings include tokio::spawn (async tasking, not process execution), tempfile usage (local testing only), and YAML frontmatter misidentified as cryptographic patterns.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 576,
    "audit_model": "claude",
    "audited_at": "2026-01-23T02:25:30.228Z",
    "risk_factors": []
  },
  "content": {
    "user_title": "Implement cloud storage best practices",
    "value_statement": "Cloud storage operations fail intermittently due to network issues, rate limiting, and timeouts. This skill provides expert guidance on implementing robust object storage with proper retry logic, error handling, streaming patterns, and efficient I/O for S3, Azure Blob, and Google Cloud Storage.",
    "seo_keywords": [
      "Claude object store skill",
      "Claude Code cloud storage",
      "AWS S3 best practices Rust",
      "Azure Blob storage retry logic",
      "GCS object store tutorial",
      "cloud storage error handling",
      "Rust object_store crate guide",
      "multipart upload large files"
    ],
    "actual_capabilities": [
      "Review object_store code for missing retry configuration and suggest proper RetryConfig settings",
      "Identify improper error handling patterns like unwrap() and recommend thiserror-based error types",
      "Detect memory-intensive loading patterns and suggest streaming for files over 100MB",
      "Recommend multipart upload patterns for large file reliability and resumable uploads",
      "Suggest efficient listing with prefixes to reduce costs and improve performance",
      "Promote atomic write patterns using temporary files and rename operations"
    ],
    "limitations": [
      "Cannot execute or test actual object_store code - only provides guidance and suggestions",
      "Does not integrate with cloud providers to validate configurations or credentials",
      "Cannot monitor production systems for storage issues or performance problems",
      "Focuses on Rust object_store crate - not applicable to other languages or libraries"
    ],
    "use_cases": [
      {
        "title": "Production S3 integration review",
        "description": "Review Rust code deploying to AWS S3 and ensure proper retry logic, connection pooling, and error handling are configured for production reliability.",
        "target_user": "Rust developers building cloud-native applications"
      },
      {
        "title": "Multi-cloud storage migration",
        "description": "Refactor code to use object_store abstraction for portable cloud storage across S3, Azure, and GCS with consistent error handling patterns.",
        "target_user": "Platform engineers migrating between cloud providers"
      },
      {
        "title": "Large file upload optimization",
        "description": "Optimize memory usage for large file uploads by implementing streaming and multipart upload patterns that handle network interruptions gracefully.",
        "target_user": "Data engineers processing large datasets"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic S3 configuration",
        "prompt": "I am using the object_store crate with AmazonS3Builder. Please review my configuration and suggest improvements for production use:",
        "scenario": "User wants feedback on S3 client setup"
      },
      {
        "title": "Error handling improvement",
        "prompt": "My object_store code is using unwrap() on storage operations and crashing on errors. Show me how to properly handle ObjectStoreError types with custom error enums.",
        "scenario": "User needs guidance on error handling patterns"
      },
      {
        "title": "Memory optimization for large files",
        "prompt": "I am loading entire files into memory with bytes().await for files that can be several gigabytes. How do I use streaming to process these files incrementally without memory issues?",
        "scenario": "User needs to optimize memory usage for large files"
      },
      {
        "title": "Multi-cloud portability",
        "prompt": "I want to make my object_store code work with both S3 and Azure Blob Storage. What patterns should I use to ensure my code remains portable across providers?",
        "scenario": "User wants to support multiple cloud providers"
      }
    ],
    "output_examples": [
      {
        "input": "Review my S3 upload code for production issues",
        "output": "I found several reliability concerns in your upload code:\n\n**Critical Issues:**\n- No retry configuration - transient network errors will fail the upload\n- Using unwrap() on async operations - panics on errors instead of proper error handling\n- Loading entire file into memory before upload - memory issues for large files\n\n**Recommended Fix:**\n```rust\nlet s3 = AmazonS3Builder::from_env()\n    .with_retry(RetryConfig {\n        max_retries: 3,\n        retry_timeout: Duration::from_secs(10),\n        ..Default::default()\n    })\n    .with_bucket_name(&bucket)\n    .build()?;\n```\n\nFor files over 100MB, use put_multipart() for reliable uploads."
      },
      {
        "input": "How do I handle 503 SlowDown errors from S3?",
        "output": "S3 returns 503 SlowDown when you exceed request throughput limits. Here's how to handle it:\n\n1. Configure retry with exponential backoff:\n```rust\n.with_retry(RetryConfig {\n    max_retries: 5,\n    retry_timeout: Duration::from_secs(60),\n    ..Default::default()\n})\n```\n\n2. Distribute requests across S3 prefixes to reduce hot spots\n\n3. Implement client-side exponential backoff for rapid retries\n\n4. Consider requesting higher throughput limits from AWS for high-volume workloads"
      }
    ],
    "best_practices": [
      "Always configure RetryConfig with 3-5 retries and timeout of 10-30 seconds for production cloud storage operations to handle transient failures",
      "Use streaming with into_stream() for files over 100MB to avoid loading entire files into memory and prevent out-of-memory errors",
      "Never hardcode credentials - use AmazonS3Builder::from_env() or IAM roles for secure credential management in production"
    ],
    "anti_patterns": [
      "Using unwrap() or expect() on object_store operations - this crashes on errors instead of graceful error handling",
      "Listing entire buckets without prefixes - this is expensive and slow for buckets with many objects",
      "Writing directly to final paths - use temporary files with atomic rename to prevent readers from seeing partial data"
    ],
    "faq": [
      {
        "question": "What retry configuration does object_store recommend for production?",
        "answer": "object_store recommends configuring RetryConfig with max_retries of 3-5 and retry_timeout of 10-30 seconds. This handles transient network issues, S3 rate limiting (503 SlowDown), and temporary service outages."
      },
      {
        "question": "When should I use multipart upload instead of put()?",
        "answer": "Use multipart upload for files larger than 100MB. Benefits include: resume capability if uploads fail, better memory efficiency, improved reliability for unstable networks, and parallel upload performance gains."
      },
      {
        "question": "How do I avoid loading entire files into memory?",
        "answer": "Use streaming with store.get(&path).await?.into_stream(). This returns a stream of chunks that you process incrementally without loading the complete file into memory at once."
      },
      {
        "question": "What is the recommended way to handle errors in object_store?",
        "answer": "Create custom error types using thiserror that wrap ObjectStoreError. Use match statements to handle specific errors like NotFound differently from network errors. Avoid unwrap() and expect() in production code."
      },
      {
        "question": "How do I optimize listing operations to reduce costs?",
        "answer": "Always use prefixes to limit LIST operations to relevant objects. Avoid store.list(None) which lists the entire bucket. For large buckets, use streaming with pagination and filter on the client side."
      },
      {
        "question": "How do I make my code work with multiple cloud providers?",
        "answer": "Use the ObjectStore trait for all operations and avoid provider-specific APIs. Configure each provider using their respective builders (AmazonS3Builder, MicrosoftAzureBuilder, GoogleCloudStorageBuilder) and pass dyn ObjectStore through your application."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 576
    }
  ]
}
