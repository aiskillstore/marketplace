{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T11:04:27.792Z",
    "slug": "bauplanlabs-creating-bauplan-pipelines",
    "source_url": "https://github.com/BauplanLabs/bauplan-mcp-server/tree/main/skills/new-pipeline",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "49ef233c918e38ac3ec860ca6da40f180a7e66bf493ad1769b9643f6c3de0827",
    "tree_hash": "4a4c4c4483e6ddbf274971526f89ff5ec14186c99761a5f76048b05e84443e0c"
  },
  "skill": {
    "name": "creating-bauplan-pipelines",
    "description": "Creates bauplan data pipeline projects with SQL and Python models. Use when starting a new pipeline, defining DAG transformations, writing models, or setting up bauplan project structure from scratch.",
    "summary": "Creates bauplan data pipeline projects with SQL and Python models. Use when starting a new pipeline,...",
    "icon": "ðŸ”§",
    "version": "1.0.0",
    "author": "BauplanLabs",
    "license": "MIT",
    "category": "data",
    "tags": [
      "data-pipelines",
      "sql",
      "python",
      "dag",
      "lakehouse"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a documentation-only skill providing guidance for creating bauplan data pipelines. The skill contains no executable code, no network calls initiated by the skill itself, and no data exfiltration capabilities. It guides users through pipeline creation using bauplan CLI commands. Risk factors are minimal and appropriate for a documentation skill.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 253,
            "line_end": 269
          },
          {
            "file": "SKILL.md",
            "line_start": 74,
            "line_end": 81
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 90,
            "line_end": 100
          },
          {
            "file": "SKILL.md",
            "line_start": 102,
            "line_end": 110
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 744,
    "audit_model": "claude",
    "audited_at": "2026-01-10T11:04:27.792Z"
  },
  "content": {
    "user_title": "Create bauplan data pipelines with SQL and Python models",
    "value_statement": "Creating data pipelines from scratch requires understanding DAG architecture, materialization strategies, and project structure. This skill guides you through setting up bauplan projects with proper branch safety, SQL first-node patterns, and Python transformation models.",
    "seo_keywords": [
      "bauplan pipelines",
      "data pipeline creation",
      "SQL models",
      "Python models",
      "DAG transformations",
      "lakehouse ETL",
      "Claude Code",
      "Claude",
      "Codex",
      "data engineering"
    ],
    "actual_capabilities": [
      "Create new bauplan project folders with bauplan_project.yml configuration",
      "Write SQL models for first-node pipeline stages reading from lakehouse tables",
      "Write Python models with decorators for data transformations",
      "Define data quality expectations and validation checks",
      "Configure materialization strategies (REPLACE, APPEND, OVERWRITE_PARTITIONS)",
      "Set up multi-input models joining multiple source tables"
    ],
    "limitations": [
      "This skill provides guidance only. Actual pipeline execution requires bauplan CLI installed locally",
      "Does not connect to external databases or APIs directly",
      "Does not generate credentials or manage authentication"
    ],
    "use_cases": [
      {
        "target_user": "Data engineers",
        "title": "Set up new pipeline projects",
        "description": "Initialize new bauplan projects with proper DAG structure and SQL/Python models for data transformations"
      },
      {
        "target_user": "Analytics engineers",
        "title": "Create transformation models",
        "description": "Write validated transformation models with column specifications and data quality expectations"
      },
      {
        "target_user": "ML engineers",
        "title": "Build feature pipelines",
        "description": "Create multi-stage pipelines that aggregate data for machine learning feature engineering"
      }
    ],
    "prompt_templates": [
      {
        "title": "New pipeline project",
        "scenario": "Start a new data pipeline from scratch",
        "prompt": "Help me create a new bauplan pipeline project. I want to read from the taxi_fhvhv and taxi_zones tables in the lakehouse and create a daily_summary output table."
      },
      {
        "title": "Python transformation",
        "scenario": "Write a Python model with validation",
        "prompt": "Write a Python model that joins trips with zone information. Use I/O pushdown with columns and filter parameters. Add output column validation."
      },
      {
        "title": "Multi-input model",
        "scenario": "Create a model joining multiple tables",
        "prompt": "Create a Python model that takes multiple input tables. Show how to specify multiple bauplan.Model() parameters for a join transformation."
      },
      {
        "title": "Data quality checks",
        "scenario": "Add expectations to validate data",
        "prompt": "Add data quality expectations to my pipeline. I need checks for no null values, unique IDs, and reasonable value ranges."
      }
    ],
    "output_examples": [
      {
        "input": "Create a new bauplan pipeline project with a Python model that aggregates daily sales",
        "output": [
          "Your pipeline will be created in a new project folder with:",
          "- bauplan_project.yml with a unique UUID",
          "- models.py with the daily_sales aggregation model",
          "- Proper decorators for materialization and Python version",
          "- Columns specification for output validation"
        ]
      }
    ],
    "best_practices": [
      "Use SQL models only for first nodes that read directly from lakehouse tables",
      "Always specify the columns parameter in @bauplan.model() for output validation",
      "Use columns and filter parameters in bauplan.Model() for I/O pushdown performance"
    ],
    "anti_patterns": [
      "Using SQL models for complex transformations beyond first nodes",
      "Omitting the columns parameter in @bauplan.model() declarations",
      "Running pipelines on the main branch instead of a development branch"
    ],
    "faq": [
      {
        "question": "Does this skill require bauplan CLI installed?",
        "answer": "Yes. The skill guides you through creating pipeline files, but you need bauplan CLI installed locally to run and test pipelines."
      },
      {
        "question": "What are the limits on pipeline complexity?",
        "answer": "Bauplan supports arbitrary DAG complexity. However, each model produces exactly one output table and can take multiple inputs."
      },
      {
        "question": "Can I integrate with existing data tools?",
        "answer": "Yes. The skill shows how to use DuckDB and Polars in Python models. Bauplan integrates with your existing lakehouse infrastructure."
      },
      {
        "question": "Is my data safe with this skill?",
        "answer": "Yes. The skill is read-only documentation. It never accesses your data directly. All operations go through the bauplan CLI which you control."
      },
      {
        "question": "Why is my pipeline failing on dry run?",
        "answer": "Check that source tables exist in the lakehouse, columns match your Model() specifications, and you are on a development branch."
      },
      {
        "question": "How does this compare to dbt?",
        "answer": "Bauplan uses a similar DAG model but focuses on Python/SQL flexibility with built-in I/O pushdown and strict mode validation."
      }
    ]
  },
  "file_structure": [
    {
      "name": "examples.md",
      "type": "file",
      "path": "examples.md"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
