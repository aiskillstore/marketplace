{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T11:07:10.779Z",
    "slug": "bauplanlabs-wap-ingestion",
    "source_url": "https://github.com/BauplanLabs/bauplan-mcp-server/tree/main/skills/wap",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "dfe58331cffa6a5ef4e683135f295ad2c8e8b631a1728907c663d062d0f2404c",
    "tree_hash": "511536ec221c4bb2703b166526a923bea270859ebc92bf40adba2f3b245c1e0a"
  },
  "skill": {
    "name": "wap-ingestion",
    "description": "Ingest data from S3 into bauplan using the Write-Audit-Publish pattern for safe data loading. Use when loading new data from S3, performing safe data ingestion, or when the user mentions WAP, data ingestion, importing parquet/csv/jsonl files, or needs to safely load data with quality checks.",
    "summary": "Ingest data from S3 into bauplan using the Write-Audit-Publish pattern for safe data loading. Use wh...",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "BauplanLabs",
    "license": "MIT",
    "category": "data",
    "tags": [
      "data-ingestion",
      "s3",
      "bauplan",
      "branching",
      "data-quality"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "filesystem",
      "scripts",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Legitimate data ingestion skill implementing the Write-Audit-Publish pattern. Uses the bauplan SDK for safe branch-based data loading with quality verification. No suspicious patterns, obfuscation, or credential access detected.",
    "risk_factor_evidence": [
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "wap_template.py",
            "line_start": 57,
            "line_end": 71
          }
        ]
      },
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "wap_template.py",
            "line_start": 1,
            "line_end": 123
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "wap_template.py",
            "line_start": 34,
            "line_end": 71
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 263,
    "audit_model": "claude",
    "audited_at": "2026-01-10T11:07:10.779Z"
  },
  "content": {
    "user_title": "Ingest S3 data with Write-Audit-Publish pattern",
    "value_statement": "Load data from S3 safely by staging to a temporary branch first. Verify quality before merging to production. Prevents bad data from reaching your main tables.",
    "seo_keywords": [
      "WAP pattern",
      "S3 data ingestion",
      "bauplan",
      "data pipeline",
      "branch-based loading",
      "parquet import",
      "CSV import",
      "data quality",
      "Claude Code",
      "Clue"
    ],
    "actual_capabilities": [
      "Create temporary branch for safe data staging",
      "Import parquet, CSV, or JSONL files from S3",
      "Run quality audit to verify row count before publishing",
      "Atomic merge to main branch or manual inspection",
      "Append data to existing tables safely"
    ],
    "limitations": [
      "Requires bauplan account and configured S3 access",
      "Cannot modify or overwrite existing table schemas",
      "Schema inferred from S3 files, not user-defined",
      "Single-table or multi-table operations only via separate WAP runs"
    ],
    "use_cases": [
      {
        "target_user": "Data engineers",
        "title": "Safe production data loading",
        "description": "Load new data batches with automatic quality verification before exposing to downstream systems."
      },
      {
        "target_user": "Analytics teams",
        "title": "Staged data updates",
        "description": "Import research data to temporary branch for review before merging to main analytics tables."
      },
      {
        "target_user": "ML engineers",
        "title": "Model feature ingestion",
        "description": "Safely load new feature data from S3 buckets with audit checks before production use."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic S3 ingestion",
        "scenario": "Load parquet files from S3",
        "prompt": "Use wap-ingestion to load data from s3://my-bucket/events/*.parquet into a table called events in the bauplan namespace."
      },
      {
        "title": "Auto-merge after import",
        "prompt": "Use wap-ingestion to import customer_data.csv from s3://data-bucket/exports/ into the customers table with on_success=merge."
      },
      {
        "title": "Append new data batch",
        "prompt": "Use wap-ingestion to append new monthly data from s3://bucket/2024-12/*.parquet to the existing sales table."
      },
      {
        "title": "Manual review before merge",
        "prompt": "Use wap-ingestion to load data from s3://bucket/new-data/ into the products table with on_success=inspect so I can review before merging."
      }
    ],
    "output_examples": [
      {
        "input": "Ingest parquet files from s3://analytics-data/user-events/ into a table called user_events",
        "output": [
          "âœ“ Created branch: alice.wap_user_events_1704067200",
          "âœ“ Imported 15,234 rows from S3",
          "âœ“ Quality check passed: 15,234 rows imported",
          "Branch ready for inspection",
          "To merge: client.merge_branch(source_ref='alice.wap_user_events_1704067200', into_branch='main')"
        ]
      }
    ],
    "best_practices": [
      "Use on_success='inspect' for initial data loads to verify schema and content before merging",
      "Keep on_failure='keep' to preserve branches for debugging when imports fail",
      "Review branch changes with bauplan queries before merging to catch data quality issues early"
    ],
    "anti_patterns": [
      "Do not use WAP for overwriting existing tables - it only appends or creates new tables",
      "Do not skip the audit phase - the row count check catches empty imports",
      "Do not run multiple WAP operations simultaneously on the same table name"
    ],
    "faq": [
      {
        "question": "What file formats are supported?",
        "answer": "Parquet, CSV, and JSONL files from S3 are supported. Schema is automatically inferred from the source files."
      },
      {
        "question": "What happens if the import fails?",
        "answer": "On failure, the branch is preserved by default for inspection. Set on_failure='delete' to auto-cleanup failed branches."
      },
      {
        "question": "Can I modify existing tables?",
        "answer": "WAP can append new rows to existing tables but cannot modify table schemas or overwrite existing data."
      },
      {
        "question": "Is my data safe during ingestion?",
        "answer": "Yes. Data is written to an isolated temporary branch first. It only reaches main after quality checks pass and merge is executed."
      },
      {
        "question": "How do I handle very large datasets?",
        "answer": "Large imports are supported. The quality audit counts rows to verify successful import. Consider partitioning S3 paths for better performance."
      },
      {
        "question": "How is this different from direct imports?",
        "answer": "WAP provides a safety net by staging data on a branch first. Bad imports never reach production unless you explicitly merge them."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    },
    {
      "name": "wap_template.py",
      "type": "file",
      "path": "wap_template.py"
    }
  ]
}
