{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-23T02:19:52.096Z",
    "slug": "enactprotocol-test-failing-test",
    "source_url": "https://github.com/EnactProtocol/enact/tree/main/test-tools/failing-test",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "e80ee6ca5ae01eda5f691624b7f55f49f3d1270d095f2eae938f82f2c9a0de5e",
    "tree_hash": "a07f303dea8e56840c7fea0c7d9619b31c97aea6ab260024d29073164678e612"
  },
  "skill": {
    "name": "test/failing-test",
    "description": "A test tool that fails with visible output to verify error visibility in AI tool environments",
    "summary": "Test tool that prints output and exits with error to validate error visibility",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "EnactProtocol",
    "license": "MIT",
    "tags": [
      "testing",
      "error-handling",
      "debugging",
      "validation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Static analysis flagged 2 potential security issues regarding weak cryptographic algorithms at SKILL.md:4 and SKILL.md:12. Investigation reveals these are FALSE POSITIVES. Line 4 contains a YAML description field ('A test tool that fails with visible output') and line 12 contains another description field ('Message to print before failing'). Both are plain text with no cryptographic content. The scanner incorrectly flagged the word 'fail' as a cryptographic weakness. The script.js file is a simple test tool that prints output and exits with error code 1 - no malicious behavior detected. Minor risk factor: script execution with command-line arguments.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "script.js",
            "line_start": 1,
            "line_end": 19
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [
      {
        "title": "False Positive: Weak Cryptographic Algorithm Flag",
        "description": "Static analyzer incorrectly flagged SKILL.md:4 as containing a weak cryptographic algorithm. The line contains only 'description: A test tool that fails with visible output' - a plain YAML description field with no cryptographic content. This is a pattern-matching false positive.",
        "locations": [
          {
            "file": "SKILL.md",
            "line_start": 4,
            "line_end": 4
          }
        ],
        "verdict": "FALSE_POSITIVE",
        "confidence": 0.95,
        "confidence_reasoning": "Line contains only plain text description in YAML format with no cryptographic code or algorithms"
      },
      {
        "title": "False Positive: Weak Cryptographic Algorithm Flag",
        "description": "Static analyzer incorrectly flagged SKILL.md:12 as containing a weak cryptographic algorithm. The line contains only 'description: Message to print before failing' - a plain YAML description field with no cryptographic content. This is a pattern-matching false positive.",
        "locations": [
          {
            "file": "SKILL.md",
            "line_start": 12,
            "line_end": 12
          }
        ],
        "verdict": "FALSE_POSITIVE",
        "confidence": 0.95,
        "confidence_reasoning": "Line contains only plain text description in YAML format with no cryptographic code or algorithms"
      }
    ],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 40,
    "audit_model": "claude",
    "audited_at": "2026-01-23T02:19:52.096Z",
    "risk_factors": [
      "scripts"
    ]
  },
  "content": {
    "user_title": "Test error visibility",
    "value_statement": "Developers need reliable ways to test error handling and output visibility in AI tool environments. This test tool simulates failures with visible stdout and stderr output to validate error reporting workflows.",
    "seo_keywords": [
      "Claude",
      "Codex",
      "Claude Code",
      "testing tool",
      "error visibility",
      "fail testing",
      "stderr output",
      "debugging tool",
      "test automation",
      "error handling"
    ],
    "actual_capabilities": [
      "Prints custom messages to stdout and stderr",
      "Generates simulated error with stack trace",
      "Exits with error code 1 for failure validation",
      "Accepts custom message via command-line argument"
    ],
    "limitations": [
      "Does not perform any actual work beyond output generation",
      "Cannot recover from errors - always exits with failure",
      "Limited to text output - no file operations or network calls",
      "Intentionally designed for testing, not production use"
    ],
    "use_cases": [
      {
        "title": "Validate AI tool error handling",
        "description": "Test how AI assistants respond to tool failures by invoking a tool that predictably fails with visible output.",
        "target_user": "AI tool developers"
      },
      {
        "title": "Debug error visibility workflows",
        "description": "Verify that stderr and stdout are properly captured and displayed in your development environment.",
        "target_user": "DevOps engineers"
      },
      {
        "title": "Test CI/CD error pipelines",
        "description": "Use in automated tests to ensure error conditions are properly handled in continuous integration workflows.",
        "target_user": "QA engineers"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic failure test",
        "prompt": "Use the failing-test tool with the message 'Testing error visibility'",
        "scenario": "Simple invocation to verify tool execution and output capture"
      },
      {
        "title": "Custom error message",
        "prompt": "Call the failing-test tool with a custom message describing a simulated scenario",
        "scenario": "Testing error message customization and display"
      },
      {
        "title": "Error chain testing",
        "prompt": "Use failing-test multiple times with different messages to test error handling in workflows",
        "scenario": "Validating error handling across multiple tool calls"
      },
      {
        "title": "Integration error testing",
        "prompt": "Use failing-test in combination with other tools to verify error propagation behavior",
        "scenario": "Testing how errors affect subsequent tool calls"
      }
    ],
    "output_examples": [
      {
        "input": "Use failing-test with message 'Connection timeout'",
        "output": [
          "=== Starting Test Tool ===",
          "Message: Connection timeout",
          "This is stdout output before the error",
          "=== Error Output ===",
          "This is stderr output",
          "Error: Simulated failure for testing error visibility",
          "Stack trace:",
          "Error: Simulated failure for testing error visibility",
          "    at Object.<anonymous> (/workspace/script.js:13:15)",
          "    at Module._compile (node:internal/modules/cjs:893:27)",
          "    at Object.Module._extensions..js (node:internal/modules/cjs:1105:10)"
        ]
      },
      {
        "input": "Use failing-test with message 'Authentication failed'",
        "output": [
          "=== Starting Test Tool ===",
          "Message: Authentication failed",
          "This is stdout output before the error",
          "=== Error Output ===",
          "This is stderr output",
          "Error: Simulated failure for testing error visibility"
        ]
      }
    ],
    "best_practices": [
      "Use custom messages to distinguish between different failure scenarios in tests",
      "Capture both stdout and stderr to verify complete error visibility",
      "Check for error code 1 to confirm intentional failure behavior"
    ],
    "anti_patterns": [
      "Do not use in production workflows - this tool always fails by design",
      "Do not expect this tool to perform any useful work beyond error generation",
      "Do not rely on specific stack trace content as it may vary by environment"
    ],
    "faq": [
      {
        "question": "What does the failing-test tool actually do?",
        "answer": "It prints a custom message to stdout, generates a simulated error with stack trace to stderr, and exits with error code 1. It is designed solely for testing error visibility."
      },
      {
        "question": "Why would I use a tool that always fails?",
        "answer": "Testing error handling is critical. This tool helps verify that your AI assistant, debugging tools, or CI/CD pipelines properly capture and display error conditions."
      },
      {
        "question": "Can I customize the error message?",
        "answer": "Yes, pass a custom message as the 'message' parameter. This appears in the output and helps identify which test scenario triggered the failure."
      },
      {
        "question": "Does this tool write to files or make network calls?",
        "answer": "No, this is a minimal test tool that only outputs text to stdout and stderr. It has no file system or network access."
      },
      {
        "question": "What exit code does it use?",
        "answer": "It always exits with code 1, which is the standard convention for command-line errors."
      },
      {
        "question": "Is this tool safe to use in any environment?",
        "answer": "Yes, it is a read-only test tool with no side effects. It cannot modify files, make network requests, or access sensitive data."
      }
    ]
  },
  "file_structure": [
    {
      "name": "script.js",
      "type": "file",
      "path": "script.js",
      "lines": 20
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 20
    }
  ]
}
