# ğŸ“š æ–‡æœ¬åˆ†æä¸ç»“æ„åŒ–æå–æ•™ç¨‹ (v3.1 - å®Œæ•´ä¸­é—´ä»¶ä¼˜åŒ–ç‰ˆ)

## ğŸ¯ æ–‡æ¡£ç›®æ ‡
ä¸ºAIåŠ©æ‰‹æä¾›ä¸€å¥—**ä¸ToolExecutionMiddlewareå®Œå…¨å…¼å®¹**ã€**å®‰å…¨å¯é **çš„æ–‡æœ¬åˆ†æè§£å†³æ–¹æ¡ˆï¼Œä¸“é—¨ç”¨äºå¤„ç†å·²è·å–çš„ç½‘é¡µå†…å®¹ã€æ–‡æ¡£æ•°æ®ç­‰ç»“æ„åŒ–ä¿¡æ¯æå–ã€‚

---

## ğŸ§  æ ¸å¿ƒè®¾è®¡åŸåˆ™ (ä¸ä¸­é—´ä»¶å¯¹é½)

### âœ… å¿…é¡»éµå®ˆ
1. **é›¶ç½‘ç»œä¾èµ–** - æ‰€æœ‰åˆ†æåŸºäºå·²æä¾›çš„æ–‡æœ¬æ•°æ®
2. **å®‰å…¨ç¬¬ä¸€** - ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“å’Œé¢„è£…å®‰å…¨åº“
3. **æ ¼å¼æ ‡å‡†åŒ–** - è¾“å‡ºå¿…é¡»ç¬¦åˆç³»ç»Ÿå¯è¯†åˆ«çš„JSONç»“æ„
4. **é”™è¯¯åŒ…å®¹æ€§** - æå–å¤±è´¥æ—¶æä¾›åˆç†çš„é»˜è®¤å€¼
5. **å‡½æ•°å¼ç¼–ç¨‹** - é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œä¸ä¸­é—´ä»¶ä¼˜åŒ–ä¿æŒä¸€è‡´
6. **ä¸­æ–‡æ ‡ç‚¹è§„é¿** - ä»£ç ä¸­ç¦æ­¢ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼Œåªä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
7. **å¼ºåˆ¶è¾“å‡ºæ ¼å¼** - å¿…é¡»åŒ…å«typeå­—æ®µï¼Œä½¿ç”¨json.dumpsè¾“å‡º

### âŒ å¿…é¡»é¿å…
1. ç½‘ç»œè¯·æ±‚ã€APIè°ƒç”¨
2. æ–‡ä»¶ç³»ç»Ÿè¶Šæƒè®¿é—®
3. éå®‰å…¨çš„åº“å¯¼å…¥
4. æ— é™å¾ªç¯æˆ–èµ„æºè€—å°½æ“ä½œ
5. ç±»å®šä¹‰ï¼ˆä¸­é—´ä»¶ä¼˜åŒ–åæ›´åŠ ä¸¥æ ¼ï¼‰
6. ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼ˆä¼šå¼•èµ·SyntaxErrorï¼‰
7. ç¼ºå°‘typeå­—æ®µçš„JSONè¾“å‡º

---

## ğŸš€ å¿«é€Ÿå¼€å§‹æ¨¡æ¿ï¼ˆä¸ä¸­é—´ä»¶å…¼å®¹ï¼‰

### åœºæ™¯ä¸€ï¼šç›´æ¥åˆ†æç½‘é¡µæŠ“å–å†…å®¹
```python
# ===================== åŸºç¡€åˆ†ææ¨¡æ¿ï¼ˆä¸­é—´ä»¶ä¼˜åŒ–ç‰ˆï¼‰=====================
import json
import re
from datetime import datetime

def analyze_webpage_content(text_content: str) -> dict:
    """
    åŸºç¡€ç½‘é¡µå†…å®¹åˆ†æå™¨ - ä¸ToolExecutionMiddlewareå®Œå…¨å…¼å®¹
    è¾“å…¥ï¼šä»»ä½•ç½‘é¡µçš„æ–‡æœ¬å†…å®¹
    è¾“å‡ºï¼šç»“æ„åŒ–æå–ç»“æœ
    """
    # åˆå§‹åŒ–æ ‡å‡†è¾“å‡ºç»“æ„ - å¿…é¡»åŒ…å«typeå­—æ®µ
    result = {
        "type": "analysis_report",  # ğŸš¨ å…³é”®ï¼šå¿…é¡»å­—æ®µï¼Œä¸­é—´ä»¶ä¾èµ–æ­¤å­—æ®µ
        "title": "ç½‘é¡µå†…å®¹åˆ†ææŠ¥å‘Š",
        "timestamp": datetime.now().isoformat(),
        "metadata": {
            "text_length": len(text_content),
            "analysis_method": "regex_extraction",
            "language": "mixed"
        },
        "data": {
            "basic_info": {},
            "pricing_info": {},
            "specifications": {},
            "extracted_summary": ""
        }
    }
    
    # 1. åŸºæœ¬ä¿¡æ¯æå–ï¼ˆç¤ºä¾‹ï¼‰
    if "äº§å“" in text_content or "Product" in text_content:
        result["data"]["basic_info"]["page_type"] = "product_page"
    
    # 2. ä»·æ ¼æå–ï¼ˆå¤šå¸ç§æ”¯æŒï¼‰ - ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
    price_patterns = {
        "USD": r'\$\s*(\d+[,\d]*\.?\d*)',
        "CNY": r'Â¥\s*(\d+[,\d]*)',
        "HKD": r'HK\$\s*(\d+[,\d]*\.?\d*)',
        "EUR": r'â‚¬\s*(\d+[,\d]*\.?\d*)'
    }
    
    for currency, pattern in price_patterns.items():
        match = re.search(pattern, text_content)
        if match:
            result["data"]["pricing_info"][currency] = match.group(1)
    
    # 3. å…³é”®ä¿¡æ¯æ‘˜è¦ - é™åˆ¶é•¿åº¦é¿å…ä¸­é—´ä»¶æˆªæ–­
    lines = text_content.split('\n')
    key_lines = [line.strip() for line in lines if len(line.strip()) > 20][:5]
    result["data"]["extracted_summary"] = " | ".join(key_lines)
    
    # 4. ç¡®ä¿æ•°æ®ä¸ä¸ºç©º
    if not result["data"]["pricing_info"]:
        result["data"]["pricing_info"] = {"status": "no_price_found"}
    
    return result

# ===================== æ‰§è¡Œç¤ºä¾‹ï¼ˆä¸­é—´ä»¶è¦æ±‚ï¼‰=====================
if __name__ == "__main__":
    # å°†æ‚¨çš„data_contextç²˜è´´åœ¨è¿™é‡Œ
    sample_text = """
    äº§å“åç§°: Jimmy Choo DIDI 45
    ä»·æ ¼: $299.99
    æè´¨: çš®é©é‹é¢, ç»¸ç¼å†…è¡¬
    è·Ÿé«˜: 45mm
    ç‰¹ç‚¹: å°–å¤´è®¾è®¡, ä¼˜é›…å¥³æ€§é‹å±¥
    """
    
    analysis_result = analyze_webpage_content(sample_text)
    
    # ğŸš¨ å…³é”®ï¼šå¿…é¡»ä½¿ç”¨printè¾“å‡ºJSONæ ¼å¼ï¼Œensure_ascii=Falseæ”¯æŒä¸­æ–‡
    # ä¸­é—´ä»¶ä¾èµ–æ­¤æ ¼å¼è¿›è¡Œè§£æå’Œå­˜å‚¨
    print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
```

### åœºæ™¯äºŒï¼šå¤šé¡µé¢æ‰¹é‡åˆ†æï¼ˆä¸­é—´ä»¶å…¼å®¹ï¼‰
```python
import json
import re
from datetime import datetime

def analyze_multiple_pages(pages_data: str) -> dict:
    """
    å¤„ç†åŒ…å«å¤šä¸ªé¡µé¢çš„æ–‡æœ¬æ•°æ® - ä¸­é—´ä»¶ä¼˜åŒ–ç‰ˆ
    æ ¼å¼ï¼šä»¥"## é¡µé¢"åˆ†éš”çš„ä¸åŒé¡µé¢
    """
    results = []
    
    # åˆ†å‰²é¡µé¢
    if "## é¡µé¢" in pages_data:
        pages = pages_data.split("## é¡µé¢")[1:]
        
        for i, page_content in enumerate(pages[:3]):  # é™åˆ¶å‰3é¡µ
            # è°ƒç”¨å•é¡µåˆ†æå™¨
            page_result = analyze_webpage_content(page_content)
            page_result["page_number"] = i + 1
            results.append(page_result)
    else:
        # å•é¡µæƒ…å†µ
        results.append(analyze_webpage_content(pages_data))
    
    final_output = {
        "type": "multi_page_analysis",  # ğŸš¨ å…³é”®ï¼šä¸­é—´ä»¶è¯†åˆ«å­—æ®µ
        "total_pages": len(results),
        "pages": results,
        "summary": f"æˆåŠŸåˆ†æ {len(results)} ä¸ªé¡µé¢",
        "metadata": {
            "analysis_timestamp": datetime.now().isoformat(),
            "version": "v3.1-middleware-compatible"
        }
    }
    
    return final_output

# æ‰§è¡Œç¤ºä¾‹
if __name__ == "__main__":
    multi_page_text = """
    ## é¡µé¢1
    äº§å“A ä»·æ ¼ $100
    è§„æ ¼: 10x20cm
    
    ## é¡µé¢2  
    äº§å“B ä»·æ ¼ $200
    è§„æ ¼: 15x25cm
    """
    
    result = analyze_multiple_pages(multi_page_text)
    print(json.dumps(result, ensure_ascii=False, indent=2))
```

---

## ğŸ“Š è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆç³»ç»Ÿå¼ºåˆ¶è¦æ±‚ - ä¸­é—´ä»¶ä¾èµ–ï¼‰

### âœ… æ­£ç¡®æ ¼å¼ç¤ºä¾‹ï¼ˆä¸­é—´ä»¶å¯è§£æï¼‰
```json
{
    "type": "analysis_report",  // ğŸš¨ å¿…é¡»å­—æ®µï¼Œä¸­é—´ä»¶ä¾èµ–æ­¤å­—æ®µè¯†åˆ«è¾“å‡ºç±»å‹
    "title": "åˆ†ææŠ¥å‘Šæ ‡é¢˜",     // ç”¨æˆ·å¯è§çš„æ ‡é¢˜
    "timestamp": "2024-01-01T12:00:00",  // æ¨èæ·»åŠ æ—¶é—´æˆ³
    "metadata": {               // å…ƒæ•°æ®ï¼ˆå¯é€‰ä½†æ¨èï¼‰
        "analysis_method": "regex",
        "version": "1.0"
    },
    "data": {                  // å®é™…åˆ†ææ•°æ®
        "field1": "value1",
        "field2": ["item1", "item2"]
    }
}
```

### âŒ é”™è¯¯æ ¼å¼ç¤ºä¾‹ï¼ˆä¸­é—´ä»¶æ— æ³•æ­£ç¡®å¤„ç†ï¼‰
```python
# é”™è¯¯1ï¼šç›´æ¥æ‰“å°å­—å…¸ï¼ˆä¸­é—´ä»¶æ— æ³•è§£æï¼‰
print(analysis_result)  

# é”™è¯¯2ï¼šéJSONå­—ç¬¦ä¸²ï¼ˆä¸­é—´ä»¶æ— æ³•ç»“æ„åŒ–å¤„ç†ï¼‰
print("ä»·æ ¼: $299.99")  

# é”™è¯¯3ï¼šç¼ºå°‘typeå­—æ®µï¼ˆä¸­é—´ä»¶æ— æ³•è¯†åˆ«ç±»å‹ï¼‰
{"data": {...}}  

# é”™è¯¯4ï¼šä½¿ç”¨ç±»å®šä¹‰ï¼ˆä¸­é—´ä»¶ä¼˜åŒ–åæ›´ä¸¥æ ¼ï¼‰
class Extractor:  # é¿å…ä½¿ç”¨ç±»
    def extract(self): pass

# é”™è¯¯5ï¼šä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ï¼ˆä¼šå¼•èµ·SyntaxErrorï¼‰
def åˆ†æå‡½æ•°(text):  # âŒ ä½¿ç”¨ä¸­æ–‡å‡½æ•°å
    result = {"ä»·æ ¼": "100"}  # âŒ ä½¿ç”¨ä¸­æ–‡å†’å·
    return result
```

---

## ğŸ› ï¸ ä¸“ä¸šåˆ†æå·¥å…·ç®±ï¼ˆä¸­é—´ä»¶ä¼˜åŒ–ç‰ˆï¼‰

### 1. ä»·æ ¼æå–å™¨ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ç‰ˆæœ¬ï¼‰
```python
import re
import json

def extract_price_info(text: str) -> dict:
    """ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ä¿¡æ¯ - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ"""
    # æ³¨æ„ï¼šä½¿ç”¨è‹±æ–‡æ ‡ç‚¹ï¼Œæ­£åˆ™è¡¨è¾¾å¼ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
    price_patterns = [
        r'(\$\d+(?:\.\d+)?)\s*per\s*1[kK]\s*tokens?',  # $0.50 per 1k tokens
        r'(\d+(?:\.\d+)?)\s*USD\s*per\s*1[kK]\s*tokens?',  # 0.50 USD per 1k tokens
        r'è¾“å…¥\s*:\s*(\$\d+\.\d+)\s*è¾“å‡º\s*:\s*(\$\d+\.\d+)',  # è¾“å…¥: $0.10 è¾“å‡º: $0.20
        r'(\$\d+(?:\.\d+)?)\s*/\s*1[kK]\s*tokens?',  # $0.30/1k tokens
        r'ä»·æ ¼\s*[:ï¼š]\s*[ï¿¥Â¥$â‚¬]?\s*(\d+(?:\.\d+)?)',  # ä»·æ ¼: $100
        r'å”®ä»·\s*[:ï¼š]\s*[ï¿¥Â¥$â‚¬]?\s*(\d+(?:\.\d+)?)',  # å”®ä»·: Â¥999
        r'cost\s*[:ï¼š]\s*[ï¿¥Â¥$â‚¬]?\s*(\d+(?:\.\d+)?)',  # cost: $50
    ]
    
    prices = []
    for pattern in price_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            # å¤„ç†å…ƒç»„åŒ¹é…ï¼ˆå¤šç»„æ•è·ï¼‰
            for match in matches:
                if isinstance(match, tuple):
                    prices.extend([m for m in match if m])
                else:
                    prices.append(match)
    
    # æ„å»ºä¸­é—´ä»¶å…¼å®¹çš„è¾“å‡ºç»“æ„
    return {
        'type': 'price_extraction',
        'extraction_method': 'regex',
        'price_matches': prices[:10],  # é™åˆ¶ç»“æœæ•°é‡
        'sample_text': text[:500] if len(text) > 500 else text,  # ä¿ç•™æ ·æœ¬ç”¨äºéªŒè¯
        'confidence': 'high' if prices else 'low',
        'currency_types': list(set([p[0] for p in prices if p and isinstance(p, str)]))  # æå–è´§å¸ç¬¦å·
    }

# ä½¿ç”¨ç¤ºä¾‹ - æ³¨æ„ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
if __name__ == "__main__":
    text_content = "ä»æ‰€æœ‰æ­¥éª¤æ”¶é›†çš„æ–‡æœ¬...ä»·æ ¼: $299.99, å”®ä»·: Â¥1999"
    price_info = extract_price_info(text_content)
    
    # ğŸš¨ å…³é”®ï¼šå¿…é¡»ä½¿ç”¨print(json.dumps())æ ¼å¼
    print(json.dumps(price_info, ensure_ascii=False, indent=2))
```

### 2. æŠ€æœ¯å‚æ•°æå–å™¨ï¼ˆä¸­é—´ä»¶å…¼å®¹ç‰ˆï¼‰
```python
import re
import json

def extract_tech_specs(text: str) -> dict:
    """æå–æŠ€æœ¯å‚æ•° - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ"""
    specs = {}
    
    # å‚æ•°æ•°é‡ - æ³¨æ„æ­£åˆ™è¡¨è¾¾å¼ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
    param_patterns = [
        r'(\d+(?:\.\d+)?)\s*ä¸‡äº¿?\s*å‚æ•°',  # 3.5ä¸‡äº¿å‚æ•°
        r'(\d+(?:\.\d+)?)\s*[Tt]rillion\s*parameters',  # 3.5 trillion parameters
        r'å‚æ•°\s*[:ï¼š]\s*(\d+(?:\.\d+)?)\s*ä¸‡äº¿?',  # å‚æ•°: 3.5ä¸‡äº¿
    ]
    
    for pattern in param_patterns:
        match = re.search(pattern, text)
        if match:
            specs['parameter_count'] = match.group(1) + ' trillion'
            break
    
    # ä¸Šä¸‹æ–‡é•¿åº¦
    context_patterns = [
        r'(\d+(?:,\d+)?[kK]?)\s*tokens?\s*ä¸Šä¸‹æ–‡',  # 128K tokensä¸Šä¸‹æ–‡
        r'ä¸Šä¸‹æ–‡\s*[:ï¼š]\s*(\d+[kK]?)',  # ä¸Šä¸‹æ–‡: 128K
        r'context\s*[:ï¼š]\s*(\d+[kK]?)',  # context: 128k
    ]
    
    for pattern in context_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            specs['context_length'] = match.group(1)
            break
    
    # MMLU åˆ†æ•°
    mmlu_patterns = [
        r'MMLU\s*[:ï¼š]?\s*(\d+(?:\.\d+)?)',  # MMLU: 85.2
        r'MMLU\s*åˆ†æ•°\s*[:ï¼š]\s*(\d+(?:\.\d+)?)',  # MMLUåˆ†æ•°: 85.2
        r'MMLU\s*score\s*[:ï¼š]\s*(\d+(?:\.\d+)?)',  # MMLU score: 85.2
    ]
    
    for pattern in mmlu_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            try:
                specs['mmlu_score'] = float(match.group(1))
            except ValueError:
                specs['mmlu_score'] = match.group(1)
            break
    
    # è¿”å›ä¸­é—´ä»¶å…¼å®¹çš„æ ¼å¼
    return {
        "type": "tech_specs_extraction",
        "specifications": specs,
        "has_parameters": 'parameter_count' in specs,
        "has_context": 'context_length' in specs,
        "has_mmlu": 'mmlu_score' in specs,
        "extraction_timestamp": datetime.now().isoformat()  # æ·»åŠ æ—¶é—´æˆ³
    }

# ä½¿ç”¨ç¤ºä¾‹ - æ³¨æ„ä»£ç ä¸­ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
if __name__ == "__main__":
    text_content = "æŸæ¨¡å‹å…·æœ‰3.5ä¸‡äº¿å‚æ•°, æ”¯æŒ128K tokensä¸Šä¸‹æ–‡é•¿åº¦, MMLUåˆ†æ•°ä¸º85.2"
    tech_specs = extract_tech_specs(text_content)
    print(json.dumps(tech_specs, ensure_ascii=False, indent=2))
```

### 3. è§„æ ¼æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ - ä¸­é—´ä»¶ä¼˜åŒ–ï¼‰
```python
import re
import json

def extract_dimensions(text: str) -> dict:
    """äº§å“è§„æ ¼ä¿¡æ¯æå– - å‡½æ•°å¼ç‰ˆæœ¬ï¼ˆä¸­é—´ä»¶å…¼å®¹ï¼‰"""
    dimensions = {}
    
    # æå–å°ºå¯¸ä¿¡æ¯ - æ³¨æ„ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
    patterns = {
        "height": [
            r'é«˜åº¦\s*[:ï¼š]\s*(\d+(?:\.\d+)?)\s*(cm|mm|m)',  # é«˜åº¦: 45mm
            r'(\d+(?:\.\d+)?)\s*(cm|mm|m)\s*é«˜',  # 45mmé«˜
            r'height\s*[:ï¼š]\s*(\d+(?:\.\d+)?)\s*(cm|mm|m)',  # height: 45mm
        ],
        "width": [
            r'å®½åº¦\s*[:ï¼š]\s*(\d+(?:\.\d+)?)\s*(cm|mm|m)',  # å®½åº¦: 30cm
            r'(\d+(?:\.\d+)?)\s*(cm|mm|m)\s*å®½',  # 30cmå®½
            r'width\s*[:ï¼š]\s*(\d+(?:\.\d+)?)\s*(cm|mm|m)',  # width: 30cm
        ],
        "weight": [
            r'é‡é‡\s*[:ï¼š]\s*(\d+(?:\.\d+)?)\s*(kg|g)',  # é‡é‡: 2.5kg
            r'(\d+(?:\.\d+)?)\s*(kg|g)\s*é‡',  # 2.5kgé‡
            r'weight\s*[:ï¼š]\s*(\d+(?:\.\d+)?)\s*(kg|g)',  # weight: 2.5kg
        ]
    }
    
    for dim, pattern_list in patterns.items():
        for pattern in pattern_list:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                # å¤„ç†åŒ¹é…ç»„
                value = match.group(1)
                unit = match.group(2) if len(match.groups()) > 1 else ""
                dimensions[dim] = f"{value} {unit}".strip()
                break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±åœæ­¢
    
    return dimensions

def extract_all_specs(text: str) -> dict:
    """æå–æ‰€æœ‰è§„æ ¼å‚æ•° - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ"""
    specs = {}
    
    # æè´¨æå–
    material_match = re.search(r'æè´¨\s*[:ï¼š]\s*([^\nï¼Œã€‚]+)', text)
    if material_match:
        specs['material'] = material_match.group(1).strip()
    
    # é¢œè‰²æå–
    color_match = re.search(r'é¢œè‰²\s*[:ï¼š]\s*([^\nï¼Œã€‚]+)', text)
    if color_match:
        specs['color'] = color_match.group(1).strip()
    
    # å°ºå¯¸ç»„åˆ
    dimensions = extract_dimensions(text)
    if dimensions:
        specs['dimensions'] = dimensions
    
    # å‹å·æå–
    model_match = re.search(r'å‹å·\s*[:ï¼š]\s*([A-Za-z0-9\-_]+)', text)
    if model_match:
        specs['model'] = model_match.group(1)
    
    # è¿”å›ä¸­é—´ä»¶å…¼å®¹æ ¼å¼
    return {
        "type": "specifications_extraction",
        "extracted_specs": specs,
        "has_material": 'material' in specs,
        "has_color": 'color' in specs,
        "has_dimensions": 'dimensions' in specs,
        "text_sample": text[:300] + "..." if len(text) > 300 else text
    }

# ä½¿ç”¨ç¤ºä¾‹ - æ³¨æ„ä»£ç ä¸­ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
if __name__ == "__main__":
    text_content = "äº§å“å°ºå¯¸: é«˜åº¦45mm, å®½åº¦30cm, é‡é‡2.5kg, æè´¨: çš®é©"
    specs = extract_all_specs(text_content)
    print(json.dumps(specs, ensure_ascii=False, indent=2))
```

### 4. å…³é”®è¯åˆ†æå™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ - ä¸­é—´ä»¶å…¼å®¹ï¼‰
```python
import json

def categorize_content(text: str) -> list:
    """åŸºäºå…³é”®è¯çš„åˆ†ç±»åˆ†æ - å‡½æ•°å¼ç‰ˆæœ¬ï¼ˆä¸­é—´ä»¶å…¼å®¹ï¼‰"""
    # æ³¨æ„ï¼šä½¿ç”¨è‹±æ–‡æ ‡ç‚¹å®šä¹‰å­—å…¸
    CATEGORY_KEYWORDS = {
        "luxury": ["å¥¢ä¾ˆ", "é«˜ç«¯", "premium", "luxury", "designer"],
        "electronics": ["ç”µå­", "æ™ºèƒ½", "tech", "digital", "gadget"],
        "clothing": ["æœè£…", "é‹", "wear", "apparel", "footwear"],
        "home_goods": ["å®¶å±…", "å®¶å…·", "home", "furniture", "decor"]
    }
    
    text_lower = text.lower()
    categories = []
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(keyword.lower() in text_lower for keyword in keywords):
            categories.append(category)
    
    return categories if categories else ["uncategorized"]

def categorize_with_confidence(text: str) -> dict:
    """å¸¦ç½®ä¿¡åº¦çš„å†…å®¹åˆ†ç±» - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ"""
    CATEGORY_KEYWORDS = {
        "luxury": ["å¥¢ä¾ˆ", "é«˜ç«¯", "premium", "luxury", "designer", "è±ªå", "å°Šäº«"],
        "electronics": ["ç”µå­", "æ™ºèƒ½", "tech", "digital", "gadget", "æ‰‹æœº", "ç”µè„‘", "æ•°ç "],
        "clothing": ["æœè£…", "é‹", "wear", "apparel", "footwear", "æœé¥°", "ç©¿æˆ´"],
        "home_goods": ["å®¶å±…", "å®¶å…·", "home", "furniture", "decor", "å®¶ç”¨", "æ‘†è®¾"],
        "beauty": ["ç¾å¦†", "æŠ¤è‚¤", "åŒ–å¦†å“", "ç¾å®¹", "skincare", "makeup"]
    }
    
    text_lower = text.lower()
    scores = {}
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        score = sum(1 for keyword in keywords if keyword.lower() in text_lower)
        if score > 0:
            scores[category] = min(score / 5, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
    
    if scores:
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_categories = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        result = {
            "type": "content_categorization",
            "primary_category": sorted_categories[0][0],
            "confidence": round(sorted_categories[0][1], 2),
            "all_categories": {cat: round(conf, 2) for cat, conf in sorted_categories[:3]},
            "total_categories_found": len(scores)
        }
    else:
        result = {
            "type": "content_categorization",
            "primary_category": "uncategorized",
            "confidence": 0.0,
            "all_categories": {},
            "total_categories_found": 0
        }
    
    return result

# ä½¿ç”¨ç¤ºä¾‹ - æ³¨æ„ä»£ç ä¸­ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
if __name__ == "__main__":
    text_content = "è¿™æ¬¾å¥¢ä¾ˆå“æ‰‹è¡¨é‡‡ç”¨é«˜ç«¯è®¾è®¡, é€‚åˆå•†åŠ¡åœºåˆ"
    categorization = categorize_with_confidence(text_content)
    print(json.dumps(categorization, ensure_ascii=False, indent=2))
```

### 5. HTMLç»“æ„åŒ–æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ - æ·±åº¦ç ”ç©¶åœºæ™¯ä¼˜åŒ–ï¼‰
```python
import re
import json
from datetime import datetime

def extract_html_title_and_links(html_content: str) -> dict:
    """
    æå–HTMLé¡µé¢æ ‡é¢˜å’Œé“¾æ¥ - v3.1æ·±åº¦ç ”ç©¶åœºæ™¯ä¼˜åŒ–ç‰ˆ
    æ³¨æ„ï¼šåœ¨æ·±åº¦ç ”ç©¶ä¸­ï¼Œå½“éœ€è¦åˆ†æåŸå§‹HTMLç»“æ„æ—¶ä½¿ç”¨
    ä½¿ç”¨é™åˆ¶ï¼šä»…é€‚ç”¨äºç®€å•HTMLï¼Œå¤æ‚é¡µé¢å»ºè®®ä½¿ç”¨crawl4aié¢„å¤„ç†
    """
    try:
        # 1. ä½¿ç”¨å®‰å…¨çš„æ ‡é¢˜æå– - é™åˆ¶æå–æ·±åº¦
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
        title = title_match.group(1).strip() if title_match else "no_title_found"
        
        # 2. å®‰å…¨çš„é“¾æ¥æå– - é™åˆ¶å¤„ç†é•¿åº¦ï¼ˆæ·±åº¦ç ”ç©¶åœºæ™¯é€šå¸¸å…³æ³¨å…³é”®é“¾æ¥ï¼‰
        links = []
        # æ·±åº¦ç ”ç©¶åœºæ™¯ï¼šé€šå¸¸åªéœ€è¦åˆ†æå…³é”®éƒ¨åˆ†ï¼Œé™åˆ¶å‰50Kå­—ç¬¦
        safe_html = html_content[:50000]  
        
        # ä½¿ç”¨æ›´å®‰å…¨çš„æ­£åˆ™ï¼Œé¿å…æ€§èƒ½é—®é¢˜
        link_pattern = r'<a\s+[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
        
        for match in re.finditer(link_pattern, safe_html, re.IGNORECASE):
            href = match.group(1)
            text = match.group(2).strip()
            
            # æ·±åº¦ç ”ç©¶è¿‡æ»¤ï¼šåªå…³æ³¨æœ‰æ„ä¹‰çš„é“¾æ¥
            if href and len(href) > 1 and href not in ['#', 'javascript:void(0)']:
                # åˆ†ç±»é“¾æ¥ç±»å‹ï¼ˆæ·±åº¦ç ”ç©¶æœ‰ç”¨ï¼‰
                link_type = "unknown"
                if href.startswith('http://') or href.startswith('https://'):
                    link_type = "external"
                elif href.startswith('/') or href.startswith('./'):
                    link_type = "internal"
                elif href.startswith('mailto:'):
                    link_type = "email"
                elif href.startswith('tel:'):
                    link_type = "phone"
                
                # æ·±åº¦ç ”ç©¶å…³æ³¨ï¼šå¯¼èˆªé“¾æ¥ã€æ–‡æ¡£é“¾æ¥ã€äº§å“é“¾æ¥ç­‰
                link_category = "general"
                if any(keyword in text.lower() for keyword in ['äº§å“', 'product', 'è¯¦æƒ…', 'detail']):
                    link_category = "product"
                elif any(keyword in text.lower() for keyword in ['ä¸‹è½½', 'download', 'æ–‡æ¡£', 'document']):
                    link_category = "resource"
                elif any(keyword in text.lower() for keyword in ['è”ç³»', 'contact', 'å…³äº', 'about']):
                    link_category = "contact"
                
                links.append({
                    "text": text[:100] if text else "link",
                    "href": href[:500],
                    "type": link_type,
                    "category": link_category
                })
            
            # æ·±åº¦ç ”ç©¶é€šå¸¸ä¸éœ€è¦æ‰€æœ‰é“¾æ¥ï¼Œé™åˆ¶æ•°é‡
            if len(links) >= 15:  
                break
        
        # 3. æ„å»ºæ·±åº¦ç ”ç©¶å‹å¥½çš„è¾“å‡º
        return {
            "type": "html_link_extraction",  # ğŸš¨ å…³é”®ï¼šå¿…é¡»å­—æ®µ
            "title": "HTMLé“¾æ¥æå–æŠ¥å‘Š",
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "html_length": len(html_content),
                "processed_length": len(safe_html),
                "extraction_method": "safe_regex_for_research",
                "scenario": "deep_research",
                "limitations": "ä»…æå–ç®€å•HTMLé“¾æ¥ï¼Œå¤æ‚é¡µé¢å»ºè®®é¢„å¤„ç†"
            },
            "data": {
                "page_title": title,
                "links": links,
                "statistics": {
                    "total_links_found": len(links),
                    "external_links": sum(1 for link in links if link["type"] == "external"),
                    "internal_links": sum(1 for link in links if link["type"] == "internal"),
                    "product_links": sum(1 for link in links if link["category"] == "product"),
                    "resource_links": sum(1 for link in links if link["category"] == "resource")
                }
            }
        }
        
    except Exception as e:
        # æ·±åº¦ç ”ç©¶ä¸­çš„é”™è¯¯å¤„ç†ï¼šæä¾›è¶³å¤Ÿä¿¡æ¯ç»§ç»­åˆ†æ
        return {
            "type": "html_extraction_error",
            "title": "HTMLæå–å¤±è´¥",
            "error_message": str(e),
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "scenario": "deep_research_fallback",
                "recommendation": "è¯·ä½¿ç”¨crawl4aiå·¥å…·é¢„å¤„ç†HTMLæˆ–ç®€åŒ–HTMLå†…å®¹"
            },
            "data": {
                "input_sample": html_content[:1000] if html_content else "empty"
            }
        }

def extract_simple_table_data(html_content: str) -> dict:
    """
    ç®€å•æå–HTMLè¡¨æ ¼æ•°æ® - v3.1æ·±åº¦ç ”ç©¶ä¼˜åŒ–ç‰ˆ
    åœ¨æ·±åº¦ç ”ç©¶ä¸­ï¼Œè¡¨æ ¼å¸¸åŒ…å«å…³é”®æ•°æ®ï¼ˆä»·æ ¼è¡¨ã€è§„æ ¼è¡¨ã€å¯¹æ¯”è¡¨ç­‰ï¼‰
    ä½¿ç”¨é™åˆ¶ï¼šä»…æ”¯æŒç®€å•è¡¨æ ¼ç»“æ„
    """
    try:
        tables = []
        
        # æ·±åº¦ç ”ç©¶åœºæ™¯ï¼šå…³æ³¨æ•°æ®è¡¨æ ¼ï¼Œé™åˆ¶å¤„ç†é•¿åº¦
        safe_html = html_content[:100000]
        
        # æŸ¥æ‰¾è¡¨æ ¼ - æ·±åº¦ç ”ç©¶å¯èƒ½å…³æ³¨ç‰¹å®šç±»å‹çš„è¡¨æ ¼
        table_patterns = {
            "general": r'<table[^>]*>(.*?)</table>',
            "with_border": r'<table[^>]*border[^>]*>(.*?)</table>',
            "with_class": r'<table[^>]*class="[^"]*table[^"]*"[^>]*>(.*?)</table>'
        }
        
        table_count = 0
        
        for table_type, pattern in table_patterns.items():
            for table_match in re.finditer(pattern, safe_html, re.IGNORECASE | re.DOTALL):
                if table_count >= 10:  # æ·±åº¦ç ”ç©¶ï¼šæœ€å¤šå¤„ç†10ä¸ªè¡¨æ ¼
                    break
                    
                table_html = table_match.group(1)
                
                # æ·±åº¦ç ”ç©¶ï¼šè·³è¿‡è¿‡å¤§çš„è¡¨æ ¼ï¼ˆå¯èƒ½æ˜¯å¸ƒå±€è¡¨æ ¼ï¼‰
                if len(table_html) > 20000:
                    continue
                    
                rows = []
                row_count = 0
                
                # æå–è¡Œ
                row_pattern = r'<tr[^>]*>(.*?)</tr>'
                
                for row_match in re.finditer(row_pattern, table_html, re.IGNORECASE | re.DOTALL):
                    if row_count >= 30:  # æ·±åº¦ç ”ç©¶ï¼šé™åˆ¶æ¯è¡¨æœ€å¤§è¡Œæ•°
                        break
                        
                    row_html = row_match.group(1)
                    cells = []
                    
                    # æå–å•å…ƒæ ¼ - æ·±åº¦ç ”ç©¶å…³æ³¨æ•°æ®
                    cell_pattern = r'<t[dh][^>]*>(.*?)</t[dh]>'
                    
                    for cell_match in re.finditer(cell_pattern, row_html, re.IGNORECASE | re.DOTALL):
                        # æ¸…ç†HTMLæ ‡ç­¾ï¼Œä¿ç•™é‡è¦æ•°æ®
                        cell_content = re.sub(r'<[^>]+>', '', cell_match.group(1))
                        cell_content = re.sub(r'\s+', ' ', cell_content).strip()
                        
                        # æ·±åº¦ç ”ç©¶ï¼šè¯†åˆ«æ•°æ®ç±»å‹
                        cell_type = "text"
                        if re.search(r'^\$?\d+(?:\.\d+)?%?$', cell_content):
                            cell_type = "number"
                        elif re.search(r'^\d{4}-\d{2}-\d{2}$', cell_content):
                            cell_type = "date"
                        
                        if cell_content:
                            cells.append({
                                "content": cell_content[:200],
                                "type": cell_type
                            })
                    
                    if cells:
                        rows.append(cells)
                        row_count += 1
                
                if rows and row_count > 0:
                    # æ·±åº¦ç ”ç©¶åˆ†æï¼šåˆ¤æ–­è¡¨æ ¼ç±»å‹
                    table_purpose = "unknown"
                    headers = rows[0] if rows else []
                    
                    # æ ¹æ®è¡¨å¤´å†…å®¹åˆ¤æ–­è¡¨æ ¼ç”¨é€”
                    header_text = " ".join([cell["content"] for cell in headers])
                    if any(keyword in header_text.lower() for keyword in ['ä»·æ ¼', 'price', 'cost', 'ï¿¥', '$']):
                        table_purpose = "pricing"
                    elif any(keyword in header_text.lower() for keyword in ['è§„æ ¼', 'spec', 'å‚æ•°', 'parameter']):
                        table_purpose = "specifications"
                    elif any(keyword in header_text.lower() for keyword in ['å¯¹æ¯”', 'compare', 'vs', 'å·®å¼‚']):
                        table_purpose = "comparison"
                    elif any(keyword in header_text.lower() for keyword in ['æ—¶é—´', 'date', 'æ—¥æœŸ', 'schedule']):
                        table_purpose = "timeline"
                    
                    tables.append({
                        "table_index": table_count,
                        "table_type": table_type,
                        "purpose": table_purpose,
                        "row_count": len(rows),
                        "col_count": len(rows[0]) if rows else 0,
                        "headers": [cell["content"] for cell in headers] if headers else [],
                        "data_sample": [[cell["content"] for cell in row] for row in rows[:5]],  # å‰5è¡Œæ ·æœ¬
                        "data_types": list(set([cell["type"] for row in rows[:3] for cell in row])) if rows else []
                    })
                    
                    table_count += 1
        
        # æ„å»ºæ·±åº¦ç ”ç©¶å‹å¥½çš„è¾“å‡º
        return {
            "type": "html_table_extraction",
            "title": "HTMLè¡¨æ ¼æå–æŠ¥å‘Š",
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "html_length": len(html_content),
                "tables_found": len(tables),
                "extraction_method": "research_optimized_regex",
                "scenario": "deep_research_data_extraction",
                "limitations": "ä»…æ”¯æŒç®€å•è¡¨æ ¼ï¼ŒåµŒå¥—è¡¨æ ¼å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†"
            },
            "data": {
                "tables": tables,
                "summary": {
                    "total_tables": len(tables),
                    "pricing_tables": sum(1 for table in tables if table["purpose"] == "pricing"),
                    "spec_tables": sum(1 for table in tables if table["purpose"] == "specifications"),
                    "comparison_tables": sum(1 for table in tables if table["purpose"] == "comparison"),
                    "total_rows": sum(table["row_count"] for table in tables),
                    "total_columns": sum(table["col_count"] for table in tables)
                }
            }
        }
        
    except Exception as e:
        return {
            "type": "table_extraction_error",
            "title": "è¡¨æ ¼æå–å¤±è´¥",
            "error_message": str(e),
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "scenario": "deep_research_fallback",
                "recommendation": "å»ºè®®ä½¿ç”¨ç»“æ„åŒ–æ•°æ®æºæˆ–ç®€åŒ–è¡¨æ ¼ç»“æ„"
            }
        }

def research_html_analysis(html_content: str) -> dict:
    """
    æ·±åº¦ç ”ç©¶HTMLåˆ†æ - ä¸“ä¸ºæ·±åº¦ç ”ç©¶åœºæ™¯ä¼˜åŒ–
    ç»“åˆé“¾æ¥å’Œè¡¨æ ¼æå–ï¼Œæä¾›ç ”ç©¶æ´å¯Ÿ
    """
    # æ·±åº¦ç ”ç©¶ï¼šé™åˆ¶è¾“å…¥å¤§å°ï¼Œå…³æ³¨è´¨é‡è€Œéæ•°é‡
    if len(html_content) > 200000:
        html_content = html_content[:200000] + "\n[HTMLå†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ç”¨äºæ·±åº¦åˆ†æ]"
    
    # å¹¶è¡Œæå–ï¼ˆæ·±åº¦ç ”ç©¶éœ€è¦å¤šæ–¹é¢ä¿¡æ¯ï¼‰
    title_links = extract_html_title_and_links(html_content)
    tables = extract_simple_table_data(html_content)
    
    # æ·±åº¦ç ”ç©¶åˆ†æï¼šæå–å…³é”®æ´å¯Ÿ
    research_insights = []
    
    # åŸºäºé“¾æ¥çš„æ´å¯Ÿ
    if title_links.get("type") != "html_extraction_error":
        links_data = title_links.get("data", {})
        if links_data.get("statistics", {}).get("product_links", 0) > 0:
            research_insights.append("é¡µé¢åŒ…å«äº§å“ç›¸å…³é“¾æ¥ï¼Œå¯èƒ½æ˜¯ç”µå•†æˆ–äº§å“é¡µé¢")
        if links_data.get("statistics", {}).get("external_links", 0) > 5:
            research_insights.append("é¡µé¢åŒ…å«å¤šä¸ªå¤–éƒ¨é“¾æ¥ï¼Œå¯èƒ½æ˜¯èµ„æºèšåˆæˆ–å¼•ç”¨é¡µé¢")
    
    # åŸºäºè¡¨æ ¼çš„æ´å¯Ÿ
    if tables.get("type") != "table_extraction_error":
        tables_data = tables.get("data", {})
        if tables_data.get("summary", {}).get("pricing_tables", 0) > 0:
            research_insights.append("é¡µé¢åŒ…å«ä»·æ ¼è¡¨æ ¼ï¼Œé€‚åˆä»·æ ¼åˆ†æç ”ç©¶")
        if tables_data.get("summary", {}).get("comparison_tables", 0) > 0:
            research_insights.append("é¡µé¢åŒ…å«å¯¹æ¯”è¡¨æ ¼ï¼Œé€‚åˆäº§å“å¯¹æ¯”ç ”ç©¶")
    
    # æ„å»ºæ·±åº¦ç ”ç©¶æŠ¥å‘Š
    return {
        "type": "deep_research_html_analysis",
        "title": "æ·±åº¦ç ”ç©¶HTMLåˆ†ææŠ¥å‘Š",
        "timestamp": datetime.now().isoformat(),
        "metadata": {
            "original_length": len(html_content),
            "analysis_focus": "research_data_extraction",
            "version": "v3.1-research-optimized",
            "primary_use_cases": [
                "äº§å“è§„æ ¼å¯¹æ¯”ç ”ç©¶",
                "ä»·æ ¼ç­–ç•¥åˆ†æ", 
                "ç«å“åˆ†æ",
                "æŠ€æœ¯æ–‡æ¡£è§£æ"
            ]
        },
        "components": {
            "title_and_links": title_links,
            "tables": tables
        },
        "research_insights": research_insights if research_insights else ["éœ€è¦è¿›ä¸€æ­¥åˆ†æä»¥è·å¾—æ·±åº¦æ´å¯Ÿ"],
        "recommendations": [
            "å¯¹äºå¤æ‚é¡µé¢ï¼Œå»ºè®®ä½¿ç”¨crawl4aié¢„å¤„ç†",
            "å…³æ³¨é¡µé¢ä¸­çš„ç»“æ„åŒ–æ•°æ®ï¼ˆè¡¨æ ¼ã€åˆ—è¡¨ï¼‰",
            "ç»“åˆæ–‡æœ¬å†…å®¹è¿›è¡Œç»¼åˆåˆ†æ"
        ]
    }

# ===================== æ·±åº¦ç ”ç©¶ä½¿ç”¨ç¤ºä¾‹ =====================
if __name__ == "__main__":
    # ç¤ºä¾‹HTML - æ¨¡æ‹Ÿæ·±åº¦ç ”ç©¶åœºæ™¯
    html_content = """
    <html>
    <head>
        <title>æ·±åº¦ç ”ç©¶ç¤ºä¾‹ï¼šAIæ¨¡å‹å¯¹æ¯”åˆ†æ</title>
        <meta name="description" content="å¯¹æ¯”GPT-4, Claude 3, Gemini Proç­‰ä¸»æµAIæ¨¡å‹">
    </head>
    <body>
        <h1>ä¸»æµAIæ¨¡å‹å¯¹æ¯”åˆ†æ</h1>
        
        <nav>
            <a href="#pricing">ä»·æ ¼å¯¹æ¯”</a>
            <a href="#specs">æŠ€æœ¯è§„æ ¼</a>
            <a href="#performance">æ€§èƒ½æµ‹è¯•</a>
            <a href="https://openai.com">OpenAIå®˜ç½‘</a>
            <a href="https://anthropic.com">Anthropicå®˜ç½‘</a>
        </nav>
        
        <section id="pricing">
            <h2>ä»·æ ¼å¯¹æ¯”è¡¨</h2>
            <table border="1" class="pricing-table">
                <tr>
                    <th>æ¨¡å‹</th><th>è¾“å…¥ä»·æ ¼ ($/1M tokens)</th><th>è¾“å‡ºä»·æ ¼ ($/1M tokens)</th>
                </tr>
                <tr>
                    <td>GPT-4 Turbo</td><td>$10.00</td><td>$30.00</td>
                </tr>
                <tr>
                    <td>Claude 3 Opus</td><td>$15.00</td><td>$75.00</td>
                </tr>
                <tr>
                    <td>Gemini Pro</td><td>$0.50</td><td>$1.50</td>
                </tr>
            </table>
        </section>
        
        <section id="specs">
            <h2>æŠ€æœ¯è§„æ ¼å¯¹æ¯”</h2>
            <table class="spec-table">
                <tr><th>æ¨¡å‹</th><th>ä¸Šä¸‹æ–‡é•¿åº¦</th><th>å‚æ•°è§„æ¨¡</th><th>MMLUåˆ†æ•°</th></tr>
                <tr><td>GPT-4</td><td>128K</td><td>1.8ä¸‡äº¿</td><td>86.4</td></tr>
                <tr><td>Claude 3</td><td>200K</td><td>æœªçŸ¥</td><td>87.5</td></tr>
            </table>
        </section>
        
        <div class="resources">
            <h3>ç›¸å…³èµ„æº</h3>
            <a href="/whitepaper.pdf">æŠ€æœ¯ç™½çš®ä¹¦ä¸‹è½½</a>
            <a href="/api-docs">APIæ–‡æ¡£</a>
            <a href="mailto:research@example.com">è”ç³»ç ”ç©¶å›¢é˜Ÿ</a>
        </div>
    </body>
    </html>
    """
    
    # æ‰§è¡Œæ·±åº¦ç ”ç©¶åˆ†æ
    print("=== æ·±åº¦ç ”ç©¶HTMLåˆ†æ ===")
    result = research_html_analysis(html_content)
    print(json.dumps(result, ensure_ascii=False, indent=2))
```

---

## ğŸ¯ AIä½¿ç”¨æŒ‡å—ï¼ˆä¸ä¸­é—´ä»¶é…åˆï¼‰

### æ­¥éª¤ä¸€ï¼šè¯†åˆ«åˆ†æéœ€æ±‚ï¼ˆä¸­é—´ä»¶æ„ŸçŸ¥ï¼‰
å½“ç”¨æˆ·è¯·æ±‚åˆ†ææ–‡æœ¬æ—¶ï¼ŒAIåº”ï¼š
1. âœ… ç¡®è®¤æ–‡æœ¬å†…å®¹æ˜¯å¦å·²æä¾›ï¼ˆæ¥è‡ªdata_contextï¼‰
2. âœ… è¯†åˆ«åˆ†æç›®æ ‡ï¼ˆä»·æ ¼ã€è§„æ ¼ã€åˆ†ç±»ç­‰ï¼‰
3. âœ… é€‰æ‹©åˆé€‚çš„æå–å™¨ç»„åˆ
4. âœ… **é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹**
5. âœ… **ç¡®ä¿ä»£ç ä¸­æ²¡æœ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·**
6. âœ… **ç¡®ä¿è¾“å‡ºåŒ…å«typeå­—æ®µ**

### æ­¥éª¤äºŒï¼šç”Ÿæˆæ‰§è¡Œä»£ç ï¼ˆä¸­é—´ä»¶å…¼å®¹ï¼‰
```python
def generate_analysis_code_for_ai(user_text: str, analysis_type: str) -> str:
    """
    AIè°ƒç”¨æ­¤å‡½æ•°ç”Ÿæˆå¯æ‰§è¡Œçš„æ²™ç›’ä»£ç  - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ
    æ³¨æ„ï¼šè¿™æ˜¯ç»™AIçœ‹çš„æ¨¡æ¿ï¼Œä¸æ˜¯ç›´æ¥åœ¨æ²™ç›’ä¸­æ‰§è¡Œçš„ä»£ç 
    """
    # ç¤ºä¾‹ä»£ç æ¨¡æ¿ - æ³¨æ„ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
    code_template = f'''
import json
import re
from datetime import datetime

# ç”¨æˆ·æä¾›çš„åˆ†ææ–‡æœ¬
TEXT_TO_ANALYZE = """{user_text}"""

def analyze_content(text):
    """åˆ†æå‡½æ•° - å‡½æ•°å¼ç‰ˆæœ¬ï¼ˆä¸­é—´ä»¶å…¼å®¹ï¼‰"""
    result = {{
        "type": "analysis_report",  # ğŸš¨ å¿…é¡»å­—æ®µ
        "title": "{analysis_type} analysis result",
        "timestamp": datetime.now().isoformat(),
        "metadata": {{
            "analysis_method": "regex_extraction",
            "input_length": len(text)
        }},
        "data": {{}}
    }}
    
    # ä»·æ ¼æå– - ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
    price_match = re.search(r'\\$\\s*(\\d+[,\\d]*\\.?\\d*)', text)
    if price_match:
        result["data"]["price_usd"] = price_match.group(1)
    
    # è§„æ ¼æå– - ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
    height_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*é«˜', text, re.IGNORECASE)
    width_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*å®½', text, re.IGNORECASE)
    
    dimensions = {{}}
    if height_match:
        dimensions["height"] = height_match.group(1) + (height_match.group(2) or "")
    if width_match:
        dimensions["width"] = width_match.group(1) + (width_match.group(2) or "")
    
    if dimensions:
        result["data"]["dimensions"] = dimensions
    
    # ç¡®ä¿æ•°æ®ä¸ä¸ºç©º
    if not result["data"]:
        result["data"]["status"] = "no_data_extracted"
    
    return result

# æ‰§è¡Œåˆ†æ
try:
    analysis_result = analyze_content(TEXT_TO_ANALYZE)
    
    # ğŸš¨ å¿…é¡»ï¼šä»¥JSONæ ¼å¼è¾“å‡ºï¼Œensure_ascii=Falseæ”¯æŒä¸­æ–‡
    print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
    
except Exception as e:
    # é”™è¯¯å¤„ç† - ä¸­é—´ä»¶è¦æ±‚è¿”å›æ ‡å‡†æ ¼å¼
    error_result = {{
        "type": "analysis_error",
        "error_message": str(e),
        "timestamp": datetime.now().isoformat(),
        "input_sample": TEXT_TO_ANALYZE[:200]
    }}
    print(json.dumps(error_result, ensure_ascii=False, indent=2))
'''
    return code_template
```

### æ­¥éª¤ä¸‰ï¼šå¤„ç†è¿”å›ç»“æœï¼ˆä¸­é—´ä»¶é›†æˆï¼‰
AIæ”¶åˆ°æ²™ç›’æ‰§è¡Œç»“æœåï¼š
1. âœ… éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆtypeå­—æ®µå­˜åœ¨ï¼‰
2. âœ… æå–å…³é”®ä¿¡æ¯å‘ˆç°ç»™ç”¨æˆ·
3. âœ… æä¾›è¿›ä¸€æ­¥åˆ†æå»ºè®®
4. âœ… å¦‚æœå¤±è´¥ï¼Œåˆ©ç”¨ä¸­é—´ä»¶çš„å¤‡ç”¨æ–¹æ¡ˆ

---

## ğŸ”§ æ•…éšœæ’é™¤ä¸æœ€ä½³å®è·µï¼ˆä¸­é—´ä»¶ä¼˜åŒ–ç‰ˆï¼‰

### å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆï¼ˆé’ˆå¯¹ä¸­é—´ä»¶ä¼˜åŒ–ï¼‰

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| æ— è¾“å‡º | ä»£ç æœªæ‰§è¡Œprint | ç¡®ä¿æœ€åä¸€è¡Œæ˜¯print(json.dumps(...)) |
| æ ¼å¼é”™è¯¯ | éJSONè¾“å‡º | ä½¿ç”¨json.dumps()è€Œéstr() |
| æå–ä¸ºç©º | æ–‡æœ¬æ ¼å¼ä¸åŒ¹é… | æ·»åŠ æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ |
| ç¼–ç é—®é¢˜ | ä¸­æ–‡å­—ç¬¦ä¹±ç  | ä½¿ç”¨ensure_ascii=Falseå‚æ•° |
| ç±»å®šä¹‰é”™è¯¯ | ä¸­é—´ä»¶ä¸æ”¯æŒç±» | ä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹æ›¿ä»£ |
| **ä¸­æ–‡æ ‡ç‚¹é”™è¯¯** | **ä»£ç åŒ…å«ä¸­æ–‡æ ‡ç‚¹** | **å…¨éƒ¨æ›¿æ¢ä¸ºè‹±æ–‡æ ‡ç‚¹** |
| **ç¼ºå°‘typeå­—æ®µ** | **ä¸­é—´ä»¶æ— æ³•è¯†åˆ«è¾“å‡º** | **å¿…é¡»åŒ…å«typeå­—æ®µ** |
| **è¾“å‡ºè¿‡é•¿** | **ä¸­é—´ä»¶å¯èƒ½æˆªæ–­** | **é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œä½¿ç”¨data_buså­˜å‚¨** |

### ä¸­é—´ä»¶ç‰¹å®šä¼˜åŒ–å»ºè®®
1. **ç±»å‹å­—æ®µä¼˜å…ˆ**ï¼šæ‰€æœ‰è¾“å‡ºå¿…é¡»åŒ…å«typeå­—æ®µï¼Œè¿™æ˜¯ä¸­é—´ä»¶è¯†åˆ«çš„å…³é”®
2. **é”™è¯¯å¤„ç†æ ‡å‡†åŒ–**ï¼šä½¿ç”¨try-exceptåŒ…è£¹ï¼Œè¿”å›æ ‡å‡†é”™è¯¯æ ¼å¼
3. **é•¿åº¦é™åˆ¶**ï¼šé™åˆ¶æå–ç»“æœæ•°é‡ï¼Œé¿å…ä¸­é—´ä»¶å¤„ç†è¶…é•¿æ•°æ®
4. **æ—¶é—´æˆ³æ·»åŠ **ï¼šä¸ºæ¯æ¬¡åˆ†ææ·»åŠ æ—¶é—´æˆ³ï¼Œä¾¿äºä¸­é—´ä»¶è¿½è¸ª
5. **å…ƒæ•°æ®ä¸°å¯Œ**ï¼šæ·»åŠ metadataå­—æ®µï¼ŒåŒ…å«åˆ†ææ–¹æ³•ã€ç‰ˆæœ¬ç­‰ä¿¡æ¯
6. **æ•°æ®æ€»çº¿å…¼å®¹**ï¼šå¦‚æœæ•°æ®é‡å¤§ï¼Œè€ƒè™‘ä½¿ç”¨ä¸­é—´ä»¶çš„æ•°æ®æ€»çº¿å­˜å‚¨æœºåˆ¶

---

## ğŸ“‹ å®Œæ•´å·¥ä½œæµç¤ºä¾‹ï¼ˆä¸­é—´ä»¶å…¼å®¹ç‰ˆï¼‰

```python
# ===================== å®Œæ•´åˆ†æå·¥ä½œæµï¼ˆä¸­é—´ä»¶å…¼å®¹ç‰ˆï¼‰=====================
import json
import re
from datetime import datetime

def complete_analysis_workflow(data_context: str) -> str:
    """
    ç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ†æå·¥ä½œæµ - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ
    è¾“å…¥ï¼šçˆ¬è™«è·å–çš„æ–‡æœ¬æ•°æ®
    è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„åˆ†ææŠ¥å‘Š
    """
    
    try:
        # 1. å¹¶è¡Œæå–å„ç±»ä¿¡æ¯ï¼ˆä½¿ç”¨å‡½æ•°è€Œéç±»ï¼‰
        price_info = extract_price_info(data_context)
        dimensions = extract_dimensions(data_context)
        categories = categorize_with_confidence(data_context)
        
        # 2. æ„å»ºç»“æœ - ç¬¦åˆä¸­é—´ä»¶è¦æ±‚
        report = {
            "type": "comprehensive_analysis",  # ğŸš¨ å…³é”®å­—æ®µ
            "title": "ç»¼åˆæ–‡æœ¬åˆ†ææŠ¥å‘Š",
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "analysis_tools": "middleware_compatible_suite",
                "analysis_time": datetime.now().isoformat(),
                "confidence": calculate_confidence(price_info, dimensions),
                "version": "v3.1-middleware-optimized"
            },
            "data": {
                "price_information": price_info,
                "specifications": dimensions,
                "content_categorization": categories,
                "text_statistics": {
                    "total_length": len(data_context),
                    "line_count": data_context.count('\n'),
                    "key_sentences": extract_key_sentences(data_context, 3)
                }
            }
        }
        
        return json.dumps(report, ensure_ascii=False, indent=2)
        
    except Exception as e:
        # é”™è¯¯å¤„ç† - ä¸­é—´ä»¶å…¼å®¹æ ¼å¼
        error_report = {
            "type": "workflow_error",
            "error_message": str(e),
            "timestamp": datetime.now().isoformat(),
            "input_sample": data_context[:500] if len(data_context) > 500 else data_context
        }
        return json.dumps(error_report, ensure_ascii=False, indent=2)

# è¾…åŠ©å‡½æ•° - æ³¨æ„ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
def extract_key_sentences(text: str, max_sentences: int = 3) -> list:
    """æå–å…³é”®å¥å­ - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ"""
    sentences = []
    current = ""
    
    for char in text:
        current += char
        if char in '.!?ã€‚ï¼ï¼Ÿ':  # ä¸­è‹±æ–‡å¥æœ«æ ‡ç‚¹
            sentence = current.strip()
            if len(sentence) > 10:
                sentences.append(sentence)
            current = ""
        
        if len(sentences) >= max_sentences:
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿå¥å­ï¼ŒæŒ‰æ¢è¡Œåˆ†å‰²
    if len(sentences) < max_sentences:
        lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 10]
        sentences.extend(lines[:max_sentences - len(sentences)])
    
    return sentences[:max_sentences]

def calculate_confidence(price_info: dict, dimensions: dict) -> str:
    """è®¡ç®—åˆ†æç½®ä¿¡åº¦ - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ"""
    price_matches = price_info.get('price_matches', [])
    has_dimensions = bool(dimensions)
    
    if price_matches and has_dimensions:
        return "high"
    elif price_matches or has_dimensions:
        return "medium"
    else:
        return "low"

# ä¸»æ‰§è¡Œé€»è¾‘ - æ³¨æ„ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
if __name__ == "__main__":
    # ç¤ºä¾‹æ–‡æœ¬ - æ³¨æ„ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
    sample_text = """
    äº§å“: é«˜ç«¯æ™ºèƒ½æ‰‹è¡¨
    ä»·æ ¼: $299.99
    å°ºå¯¸: é«˜åº¦45mm, å®½åº¦38mm
    æè´¨: ä¸é”ˆé’¢è¡¨å£³, è“å®çŸ³ç»ç’ƒ
    åŠŸèƒ½: å¿ƒç‡ç›‘æµ‹, GPSå®šä½
    """
    
    result = complete_analysis_workflow(sample_text)
    print(result)
```

---

## âœ… éªŒè¯æµ‹è¯•ï¼ˆä¸­é—´ä»¶å…¼å®¹ï¼‰

è¿è¡Œä»¥ä¸‹ä»£ç éªŒè¯æ‚¨çš„åˆ†æå™¨ï¼š

```python
# æµ‹è¯•ç”¨ä¾‹ - ä¸­é—´ä»¶å…¼å®¹ç‰ˆ
import json

def run_middleware_compatible_tests():
    """è¿è¡Œä¸­é—´ä»¶å…¼å®¹æ€§æµ‹è¯•"""
    test_cases = [
        {
            "text": "Jimmy Choo DIDI 45 ä»·æ ¼ $299.99 æè´¨çš®é© é«˜åº¦45mm",
            "expected_type": "product_page_analysis",
            "has_price": True,
            "has_dimensions": True
        },
        {
            "text": "iPhone 15 Pro Max å”®ä»· Â¥9999 é‡é‡ 221g å®½åº¦78mm",
            "expected_type": "electronics_analysis",
            "has_price": True,
            "has_dimensions": True
        },
        {
            "text": "å®æœ¨é¤æ¡Œ å°ºå¯¸ 180x90cm ä»·æ ¼ â‚¬459 é«˜åº¦75cm",
            "expected_type": "home_goods_analysis",
            "has_price": True,
            "has_dimensions": True
        }
    ]
    
    test_results = []
    
    for i, test_case in enumerate(test_cases):
        # ä½¿ç”¨å‡½æ•°å¼åˆ†æå™¨
        dimensions = extract_dimensions(test_case["text"])
        categories = categorize_content(test_case["text"])
        
        result = {
            "type": "test_result",
            "test_id": i + 1,
            "test_case": test_case["expected_type"],
            "dimensions": dimensions,
            "categories": categories,
            "has_price": "$" in test_case["text"] or "Â¥" in test_case["text"] or "â‚¬" in test_case["text"],
            "passed_basic_checks": bool(dimensions) or bool(categories),
            "middleware_compatible": True  # æ ‡è®°ä¸ºä¸­é—´ä»¶å…¼å®¹
        }
        
        test_results.append(result)
    
    # è¾“å‡ºæ±‡æ€»æŠ¥å‘Š
    summary = {
        "type": "test_summary",
        "total_tests": len(test_results),
        "passed_tests": sum(1 for r in test_results if r["passed_basic_checks"]),
        "all_middleware_compatible": all(r["middleware_compatible"] for r in test_results),
        "test_results": test_results
    }
    
    return summary

# æ‰§è¡Œæµ‹è¯•
if __name__ == "__main__":
    summary = run_middleware_compatible_tests()
    print(json.dumps(summary, ensure_ascii=False, indent=2))
```

---

## ğŸ“Œ æ€»ç»“è¦ç‚¹ï¼ˆä¸­é—´ä»¶ä¼˜åŒ–ç‰ˆï¼‰

1. **ä¸ä¸­é—´ä»¶å®Œå…¨å…¼å®¹**ï¼šæ‰€æœ‰ä»£ç è®¾è®¡è€ƒè™‘äº†ToolExecutionMiddlewareçš„è¦æ±‚
2. **ç±»å‹å­—æ®µä¼˜å…ˆ**ï¼šè¾“å‡ºå¿…é¡»åŒ…å«typeå­—æ®µï¼Œè¿™æ˜¯ä¸­é—´ä»¶è¯†åˆ«çš„å…³é”®
3. **ä¸­æ–‡æ ‡ç‚¹è§„é¿**ï¼šä»£ç ä¸­ç¦æ­¢ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼Œåªä½¿ç”¨è‹±æ–‡æ ‡ç‚¹
4. **å‡½æ•°å¼ç¼–ç¨‹**ï¼šé¿å…ç±»å®šä¹‰ï¼Œä¸ä¸­é—´ä»¶ä¼˜åŒ–ä¿æŒä¸€è‡´
5. **é”™è¯¯å¤„ç†æ ‡å‡†åŒ–**ï¼šä½¿ç”¨try-exceptï¼Œè¿”å›ä¸­é—´ä»¶å¯è§£æçš„é”™è¯¯æ ¼å¼
6. **è¾“å‡ºæ ¼å¼ä¸¥æ ¼**ï¼šå¿…é¡»ä½¿ç”¨print(json.dumps(...))æ ¼å¼
7. **å…ƒæ•°æ®ä¸°å¯Œ**ï¼šæ·»åŠ æ—¶é—´æˆ³ã€ç‰ˆæœ¬å·ç­‰å…ƒæ•°æ®
8. **é•¿åº¦é™åˆ¶**ï¼šæ§åˆ¶è¾“å‡ºé•¿åº¦ï¼Œé¿å…ä¸­é—´ä»¶å¤„ç†é—®é¢˜

## ğŸ”„ ä»ç±»åˆ°å‡½æ•°çš„è½¬æ¢æŒ‡å—ï¼ˆä¸­é—´ä»¶è¦æ±‚ï¼‰

| åŸç±»å®šä¹‰ | è½¬æ¢åçš„å‡½æ•° | ä½¿ç”¨æ–¹å¼ | ä¸­é—´ä»¶å…¼å®¹æ€§ |
|---------|------------|---------|-------------|
| `class Extractor:`<br>`def extract(self, text):` | `def extract_data(text):` | `result = extract_data(text)` | âœ… |
| `obj = Extractor()`<br>`obj.extract(text)` | ç›´æ¥è°ƒç”¨å‡½æ•° | `extract_data(text)` | âœ… |
| ç±»å±æ€§ï¼ˆ`self.config`ï¼‰ | å‡½æ•°å‚æ•°æˆ–å…¨å±€å¸¸é‡ | `def func(text, config={})` | âœ… |
| å¤šä¸ªç›¸å…³æ–¹æ³• | å¤šä¸ªç‹¬ç«‹å‡½æ•°æˆ–ä¸»å‡½æ•°è°ƒç”¨å­å‡½æ•° | `def main_func():`<br>`data1 = func1()`<br>`data2 = func2()` | âœ… |
| **ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹** | **å…¨éƒ¨æ›¿æ¢ä¸ºè‹±æ–‡æ ‡ç‚¹** | **result = {"price": "100"}** | âœ… |
| **ç¼ºå°‘typeå­—æ®µ** | **å¿…é¡»æ·»åŠ typeå­—æ®µ** | **{"type": "analysis", "data": {}}** | âœ… |

## ğŸ¯ æœ€ç»ˆæ£€æŸ¥æ¸…å•ï¼ˆä¸­é—´ä»¶ä¼˜åŒ–ç‰ˆï¼‰

åœ¨ç”Ÿæˆæ²™ç›’ä»£ç å‰ï¼Œè¯·ç¡®è®¤ï¼š
- [ ] æ²¡æœ‰`class`å…³é”®å­—ï¼ˆå‡½æ•°å¼ç¼–ç¨‹ï¼‰
- [ ] æ‰€æœ‰åŠŸèƒ½éƒ½æ˜¯å‡½æ•°
- [ ] è¾“å‡ºåŒ…å«`type`å­—æ®µï¼ˆä¸­é—´ä»¶å¿…éœ€ï¼‰
- [ ] ä½¿ç”¨`json.dumps()`è¾“å‡º
- [ ] æ²¡æœ‰ç½‘ç»œè¯·æ±‚æˆ–æ–‡ä»¶ç³»ç»Ÿè®¿é—®
- [ ] æ­£åˆ™è¡¨è¾¾å¼æœ‰é™åˆ¶ï¼ˆé¿å…ReDoSï¼‰
- [ ] **ä»£ç ä¸­æ²¡æœ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·**
- [ ] **ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹ï¼ˆé€—å·, å¥å·. å†’å·:ï¼‰**
- [ ] **æ·»åŠ æ—¶é—´æˆ³å’Œå…ƒæ•°æ®**
- [ ] **åŒ…å«é”™è¯¯å¤„ç†æœºåˆ¶**
- [ ] **æ§åˆ¶è¾“å‡ºé•¿åº¦**
