{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-11T03:41:35.076Z",
    "slug": "bossjones-pytest",
    "source_url": "https://github.com/bossjones/logging-lab/tree/main/.claude/skills/pytest",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "7e9894850b9fa3b6f66d3084063af6135ddb645c74b612191fc6b34f10f574c5",
    "tree_hash": "83d26032b84b77730db3adc28d84aead9cb38009cceee32c76ada0922ebf7759"
  },
  "skill": {
    "name": "pytest",
    "description": "Python testing framework for writing simple, scalable, and powerful tests",
    "summary": "Python testing framework for writing simple, scalable, and powerful tests",
    "icon": "ðŸ§ª",
    "version": "1.0.0",
    "author": "bossjones",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "python",
      "testing",
      "tdd",
      "test-automation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "external_commands",
      "env_access"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate documentation file for the pytest testing framework. All 46 static findings are false positives caused by the static analyzer misinterpreting Python syntax patterns. The file contains only documentation and example code demonstrating pytest features. No executable code, credential handling, or malicious patterns exist.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 101,
            "line_end": 103
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 15,
            "line_end": 15
          }
        ]
      },
      {
        "factor": "env_access",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 200,
            "line_end": 202
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 329,
    "audit_model": "claude",
    "audited_at": "2026-01-11T03:41:35.076Z"
  },
  "content": {
    "user_title": "Write Python tests with pytest",
    "value_statement": "Create reliable test suites for Python code using pytest fixtures, parametrization, and async support. The framework handles test discovery, setup, teardown, and reporting automatically.",
    "seo_keywords": [
      "pytest",
      "python testing",
      "test automation",
      "pytest fixtures",
      "pytest parametrize",
      "claude code testing",
      "python unit tests",
      "claude testing",
      "codex testing",
      "test-driven development"
    ],
    "actual_capabilities": [
      "Create unit tests with assertions for Python functions and classes",
      "Use fixtures to share test setup and teardown code across tests",
      "Parametrize tests to run the same test logic with multiple inputs",
      "Write async tests for coroutines and async frameworks",
      "Organize tests with classes, markers, and conftest.py shared configuration",
      "Mock environment variables and external dependencies using monkeypatch"
    ],
    "limitations": [
      "Does not install pytest automatically; requires uv add --dev pytest",
      "Does not provide test database or mocking libraries; they must be added separately",
      "Does not generate tests automatically; tests must be written manually",
      "Does not run tests in parallel by default; requires pytest-xdist for parallel execution"
    ],
    "use_cases": [
      {
        "target_user": "Python developers",
        "title": "Unit test creation",
        "description": "Write and run unit tests to verify individual functions and classes work correctly"
      },
      {
        "target_user": "API developers",
        "title": "API integration testing",
        "description": "Test REST APIs using TestClient fixtures with request and response validation"
      },
      {
        "target_user": "Async developers",
        "title": "Async code testing",
        "description": "Write tests for async functions using pytest-asyncio mark and async fixtures"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic test",
        "scenario": "Write a unit test",
        "prompt": "Write a pytest test for the calculate_total function that verifies it correctly sums a list of prices"
      },
      {
        "title": "Parametrized test",
        "scenario": "Multiple test cases",
        "prompt": "Create a parametrized pytest test that verifies the parse_input function handles valid and invalid inputs correctly"
      },
      {
        "title": "Fixture-based test",
        "scenario": "Shared test setup",
        "prompt": "Write a pytest fixture that creates a test user and use it in tests for user-related API endpoints"
      },
      {
        "title": "Async test",
        "scenario": "Async function testing",
        "prompt": "Write an async pytest test using pytest-asyncio to verify the fetch_data async function returns expected results"
      }
    ],
    "output_examples": [
      {
        "input": "Write a pytest test for a login function that handles correct and incorrect passwords",
        "output": [
          "âœ“ Test created with parametrization for valid and invalid credentials",
          "âœ“ Fixture provides test user data with known password",
          "âœ“ Mocked external auth service call for isolation",
          "âœ“ Asserts successful login returns user object with correct attributes",
          "âœ“ Asserts failed login raises AuthenticationError"
        ]
      }
    ],
    "best_practices": [
      "Use fixtures to share setup code and avoid repetition across tests",
      "Parametrize tests instead of writing separate tests for each input combination",
      "Keep tests independent; avoid shared state between test functions",
      "Use meaningful test names that describe what is being verified"
    ],
    "anti_patterns": [
      "Avoid using eval() in production code; only for testing safe arithmetic expressions",
      "Do not hardcode API keys or secrets in tests; use environment variables or fixtures",
      "Avoid testing multiple unrelated things in a single test function"
    ],
    "faq": [
      {
        "question": "How do I run a specific test function?",
        "answer": "Use uv run pytest path/to/test.py::function_name to run a single test function."
      },
      {
        "question": "How do I skip slow tests during development?",
        "answer": "Mark slow tests with @pytest.mark.slow and run pytest -m 'not slow' to exclude them."
      },
      {
        "question": "How do I test code that reads environment variables?",
        "answer": "Use the monkeypatch fixture to setenv with test values before the test runs."
      },
      {
        "question": "How do I test async functions?",
        "answer": "Install pytest-asyncio, add @pytest.mark.asyncio decorator, and write async test functions."
      },
      {
        "question": "How do I share fixtures across test files?",
        "answer": "Place fixtures in conftest.py at the tests directory root; pytest discovers them automatically."
      },
      {
        "question": "How do I test that code raises exceptions?",
        "answer": "Use pytest.raises(ExceptionType, match='error pattern') as context manager to verify exceptions."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
