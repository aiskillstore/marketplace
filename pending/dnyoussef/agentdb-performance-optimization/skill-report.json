{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T13:17:38.113Z",
    "slug": "dnyoussef-agentdb-performance-optimization",
    "source_url": "https://github.com/DNYoussef/ai-chrome-extension/tree/main/.claude/skills/agentdb-optimization",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "25240b10a1a7d94b5e826489cb869e1cba1946a4414f07e87bc3b40e41ebc16a",
    "tree_hash": "eea530b284d246d89879ffea532e927646782c14d7b22c3dee991ad3d0783d75"
  },
  "skill": {
    "name": "AgentDB Performance Optimization",
    "description": "Optimize AgentDB performance with quantization (4-32x memory reduction), HNSW indexing (150x faster search), caching, and batch operations. Use when optimizing memory usage, improving search speed, or scaling to millions of vectors.",
    "summary": "Optimize AgentDB performance with quantization (4-32x memory reduction), HNSW indexing (150x faster ...",
    "icon": "⚡",
    "version": "1.0.0",
    "author": "DNYoussef",
    "license": "MIT",
    "category": "data",
    "tags": [
      "performance",
      "vector-database",
      "optimization",
      "agentdb",
      "machine-learning"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill is pure documentation describing AgentDB configuration patterns. Contains only markdown with TypeScript code examples for database adapter configuration. No executable scripts, network operations, or filesystem access.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 510,
    "audit_model": "claude",
    "audited_at": "2026-01-10T13:17:38.113Z"
  },
  "content": {
    "user_title": "Optimize AgentDB vector database performance",
    "value_statement": "Vector databases can consume massive memory and suffer slow searches at scale. This skill provides proven optimization techniques including quantization, HNSW indexing, and caching to achieve 150x-12,500x performance improvements while reducing memory usage by 4-32x.",
    "seo_keywords": [
      "AgentDB optimization",
      "vector database performance",
      "HNSW indexing",
      "quantization",
      "Claude Code",
      "Claude",
      "Codex",
      "vector search optimization",
      "memory reduction",
      "batch operations"
    ],
    "actual_capabilities": [
      "Apply quantization strategies (binary, scalar, product) for 4-32x memory reduction",
      "Configure HNSW indexing for O(log n) search complexity and 150x faster queries",
      "Implement in-memory LRU caching to achieve sub-millisecond pattern retrieval",
      "Use batch operations for 500x faster bulk inserts (2ms vs 1s for 100 vectors)",
      "Monitor performance metrics including cache hit rate and search latency",
      "Scale from small (<10K) to massive (>1M) vector deployments"
    ],
    "limitations": [
      "Requires AgentDB v1.0.7+ and Node.js 18+",
      "Quantization may cause 2-7% accuracy loss depending on strategy chosen",
      "HNSW parameters require tuning for optimal recall vs speed trade-offs",
      "Benefits apply only to AgentDB; not compatible with other vector databases"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Scale vector embeddings",
        "description": "Deploy memory-efficient vector search for production ML applications with millions of embeddings"
      },
      {
        "target_user": "AI Developers",
        "title": "Accelerate agent memory",
        "description": "Speed up reasoning bank queries for intelligent agents requiring fast context retrieval"
      },
      {
        "target_user": "Data Engineers",
        "title": "Optimize database resources",
        "description": "Reduce infrastructure costs by applying quantization and indexing to large-scale vector stores"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic optimization setup",
        "scenario": "Enable performance features",
        "prompt": "Configure AgentDB with quantization, HNSW indexing, and caching for optimal performance. Show the configuration options and explain the trade-offs between memory usage and accuracy."
      },
      {
        "title": "Memory optimization",
        "scenario": "Reduce memory footprint",
        "prompt": "What quantization strategy should I use to reduce memory usage by 32x for a large-scale deployment with 1M+ vectors? Show the configuration and expected accuracy impact."
      },
      {
        "title": "Speed optimization",
        "scenario": "Maximize search speed",
        "prompt": "Configure AgentDB for maximum search speed with sub-100µs queries. Include HNSW parameter tuning and cache configuration. What accuracy trade-offs should I expect?"
      },
      {
        "title": "Scaling strategy",
        "scenario": "Scale to millions",
        "prompt": "Create an optimization recipe for scaling AgentDB from 10K to over 1M vectors. Include quantization type selection, HNSW parameters, and cache sizing for each scale tier."
      }
    ],
    "output_examples": [
      {
        "input": "Configure AgentDB with binary quantization and HNSW indexing for a large-scale deployment",
        "output": [
          "Binary Quantization: 32x memory reduction (3GB → 96MB for 1M vectors)",
          "HNSW Index: Hierarchical Navigable Small World with M=32 connections",
          "Cache Size: 2000 patterns for frequent query acceleration",
          "Expected Performance: <100µs search latency, 10x faster queries",
          "Trade-off: 2-5% accuracy loss acceptable for scale"
        ]
      }
    ],
    "best_practices": [
      "Start with scalar quantization (4x reduction) for production unless memory is critical, as it preserves 98-99% accuracy",
      "Monitor cache hit rate and aim for >80% by adjusting cacheSize based on access patterns",
      "Tune HNSW parameters progressively: start with defaults (M=16, efSearch=100) then adjust based on recall requirements"
    ],
    "anti_patterns": [
      "Using no quantization on datasets over 100K vectors causes unnecessary memory pressure and slow searches",
      "Setting efSearch too high (above 200) degrades performance without significant recall improvement for most use cases",
      "Disabling HNSW indexing reverts to linear scan (O(n)) making large-scale queries impractical"
    ],
    "faq": [
      {
        "question": "Which quantization type should I choose?",
        "answer": "Binary for maximum memory savings (32x) with 2-5% accuracy loss. Scalar for balanced 4x reduction with 98-99% accuracy. Product for high-dimensional vectors requiring 8-16x compression."
      },
      {
        "question": "What is the minimum AgentDB version required?",
        "answer": "AgentDB v1.0.7 or later is required. Run npx agentdb@latest --version to check your installed version."
      },
      {
        "question": "How do I integrate this with Claude Code?",
        "answer": "Import createAgentDBAdapter from agentic-flow/reasoningbank and configure with optimization parameters. The skill provides complete TypeScript examples for all configuration options."
      },
      {
        "question": "Is my data safe with quantization?",
        "answer": "Quantization preserves semantic similarity while reducing precision. Binary quantization uses 1 bit per dimension. Accuracy loss is typically 2-7% depending on data characteristics and use case."
      },
      {
        "question": "Why is my search still slow after optimization?",
        "answer": "Check cache hit rate with adapter.getStats(). Increase cacheSize if below 80%. Verify HNSW indexing is enabled. Ensure efSearch is not set too low for your recall requirements."
      },
      {
        "question": "How does this compare to other vector databases?",
        "answer": "AgentDB with optimizations achieves comparable or better performance than Pinecone, Weaviate, and Qdrant for single-node deployments. HNSW indexing provides logarithmic search complexity similar to industry standards."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
