{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-13T23:18:15.487Z",
    "slug": "smallnest-crawl4ai",
    "source_url": "https://github.com/smallnest/crawl4ai-skill/tree/master/",
    "source_ref": "master",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "e932d043202cdef88ff04d7cabf4dfbbc3944549a1bb62b8784d9c55ccbe6722",
    "tree_hash": "5285ede44cf3475be10ca0268907daa306894fd743a8f160dbea3461d15ce3b6"
  },
  "skill": {
    "name": "crawl4ai",
    "description": "This skill should be used when users need to scrape websites, extract structured data, handle JavaScript-heavy pages, crawl multiple URLs, or build automated web data pipelines. Includes optimized extraction patterns with schema generation for efficient, LLM-free extraction.",
    "summary": "This skill should be used when users need to scrape websites, extract structured data, handle JavaSc...",
    "icon": "üï∑Ô∏è",
    "version": "1.0.0",
    "author": "smallnest",
    "license": "MIT",
    "category": "data",
    "tags": [
      "web-scraping",
      "data-extraction",
      "crawler",
      "markdown",
      "automation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate web crawling skill based on the Crawl4AI library. The static analyzer generated 2275 false positives primarily from markdown documentation files where code block backticks were misinterpreted as shell execution, and example URLs in documentation were flagged as hardcoded URLs. The actual Python scripts perform expected web crawling operations with file writes for saving crawled content locally. No malicious intent detected.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "scripts/google_search.py",
            "line_start": 35,
            "line_end": 35
          },
          {
            "file": "scripts/basic_crawler.py",
            "line_start": 41,
            "line_end": 44
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/basic_crawler.py",
            "line_start": 54,
            "line_end": 67
          },
          {
            "file": "scripts/batch_crawler.py",
            "line_start": 85,
            "line_end": 102
          },
          {
            "file": "scripts/google_search.py",
            "line_start": 299,
            "line_end": 301
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [
      {
        "type": "expected_behavior",
        "description": "Scripts write crawled content to local files. This is expected for a web crawler skill.",
        "file": "scripts/*.py",
        "recommendation": "Users should be aware that running this skill will create output files in the working directory."
      }
    ],
    "dangerous_patterns": [],
    "files_scanned": 16,
    "total_lines": 8776,
    "audit_model": "claude",
    "audited_at": "2026-01-13T23:18:15.487Z"
  },
  "content": {
    "user_title": "Crawl websites and extract structured data with AI",
    "value_statement": "Web scraping requires handling JavaScript rendering, authentication, and data extraction. Crawl4AI provides a complete solution with CLI and Python SDK for efficient web data collection.",
    "seo_keywords": [
      "web scraping",
      "web crawler",
      "data extraction",
      "Crawl4AI",
      "Claude",
      "Claude Code",
      "Codex",
      "JavaScript rendering",
      "markdown generation",
      "batch crawling"
    ],
    "actual_capabilities": [
      "Crawl JavaScript-heavy websites with full rendering support",
      "Extract structured data using CSS selectors without LLM costs",
      "Generate clean markdown from any webpage",
      "Process multiple URLs concurrently with batch operations",
      "Scrape Google search results to JSON format",
      "Handle authentication and session management"
    ],
    "limitations": [
      "Requires Crawl4AI library and Playwright browser installation",
      "LLM-based extraction requires separate API key configuration",
      "Some anti-bot protected sites may block automated access",
      "Rate limiting should be applied to avoid IP blocking"
    ],
    "use_cases": [
      {
        "target_user": "Data Analyst",
        "title": "Collect product data from e-commerce sites",
        "description": "Generate a CSS schema once and extract product listings at scale without ongoing LLM costs."
      },
      {
        "target_user": "Technical Writer",
        "title": "Convert documentation to markdown",
        "description": "Crawl technical documentation sites and export content as clean, formatted markdown files."
      },
      {
        "target_user": "Researcher",
        "title": "Gather search results for analysis",
        "description": "Scrape Google search results for specific queries and save structured data for research projects."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic page crawl",
        "scenario": "Get markdown from a single URL",
        "prompt": "Use crawl4ai to fetch https://example.com and save the content as markdown"
      },
      {
        "title": "Search results extraction",
        "scenario": "Scrape Google search results",
        "prompt": "Run the Google search script to get the top 20 results for 'machine learning tutorials' and save to JSON"
      },
      {
        "title": "Schema-based extraction",
        "scenario": "Extract product data efficiently",
        "prompt": "First generate a CSS schema for https://shop.example.com to extract products, then use that schema to crawl without LLM"
      },
      {
        "title": "Batch URL processing",
        "scenario": "Crawl multiple pages concurrently",
        "prompt": "Use batch_crawler.py to process urls.txt with 5 concurrent connections and save all results as markdown files"
      }
    ],
    "output_examples": [
      {
        "input": "Crawl example.com and get markdown",
        "output": [
          "Crawled: https://example.com",
          "Title: Example Domain",
          "Links found: 1 internal, 0 external",
          "Content saved to output.md (1,256 characters)"
        ]
      },
      {
        "input": "Search Google for AI news and extract results",
        "output": [
          "Searching: AI news 2026",
          "Successfully extracted 20 search results",
          "Results saved to: google_search_results.json",
          "Preview: TechCrunch - Latest AI breakthroughs announced..."
        ]
      },
      {
        "input": "Batch crawl 10 documentation pages",
        "output": [
          "Starting batch crawl of 10 URLs (max 5 concurrent)",
          "Success: 9 pages, Failed: 1 page",
          "Markdown files saved to: batch_markdown/",
          "Summary saved to: batch_results.json"
        ]
      }
    ],
    "best_practices": [
      "Use schema-based CSS extraction instead of LLM for repetitive patterns to reduce costs by 100x",
      "Enable caching during development and only bypass cache when you need fresh data",
      "Add delays between requests and respect robots.txt to avoid rate limiting"
    ],
    "anti_patterns": [
      "Do not use LLM extraction for every page when the data structure is consistent",
      "Do not set very short timeouts for JavaScript-heavy sites that need rendering time",
      "Do not run batch crawls without concurrency limits which can trigger anti-bot detection"
    ],
    "faq": [
      {
        "question": "How do I install Crawl4AI and its dependencies?",
        "answer": "Run 'pip install crawl4ai' followed by 'crawl4ai-setup' to install browser drivers. Verify with 'crawl4ai-doctor'."
      },
      {
        "question": "Why is my JavaScript content not loading?",
        "answer": "Use wait_for parameter with a CSS selector or increase page_timeout. Example: wait_for=css:.dynamic-content,page_timeout=60000"
      },
      {
        "question": "How can I avoid being blocked by anti-bot protection?",
        "answer": "Set headless=false, use random user agents, add delays between requests, and consider using proxy configuration."
      },
      {
        "question": "What is schema-based extraction and why should I use it?",
        "answer": "Generate a CSS schema once using LLM, then reuse it for extraction without LLM costs. This is 10-100x more efficient."
      },
      {
        "question": "Can I crawl pages that require login?",
        "answer": "Yes. Use session_id with js_code to automate login, then reuse the session for protected pages."
      },
      {
        "question": "Where are the crawled results saved?",
        "answer": "Results are saved in the current working directory as markdown files, JSON, or screenshots depending on the script used."
      }
    ]
  },
  "file_structure": [
    {
      "name": ".claude",
      "type": "dir",
      "path": ".claude",
      "children": [
        {
          "name": "settings.local.json",
          "type": "file",
          "path": ".claude/settings.local.json",
          "lines": 9
        }
      ]
    },
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "cli-guide.md",
          "type": "file",
          "path": "references/cli-guide.md",
          "lines": 359
        },
        {
          "name": "complete-sdk-reference.md",
          "type": "file",
          "path": "references/complete-sdk-reference.md",
          "lines": 5927
        },
        {
          "name": "sdk-guide.md",
          "type": "file",
          "path": "references/sdk-guide.md",
          "lines": 391
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "basic_crawler.py",
          "type": "file",
          "path": "scripts/basic_crawler.py",
          "lines": 81
        },
        {
          "name": "batch_crawler.py",
          "type": "file",
          "path": "scripts/batch_crawler.py",
          "lines": 237
        },
        {
          "name": "extraction_pipeline.py",
          "type": "file",
          "path": "scripts/extraction_pipeline.py",
          "lines": 363
        },
        {
          "name": "google_search.py",
          "type": "file",
          "path": "scripts/google_search.py",
          "lines": 321
        }
      ]
    },
    {
      "name": "tests",
      "type": "dir",
      "path": "tests",
      "children": [
        {
          "name": "README.md",
          "type": "file",
          "path": "tests/README.md",
          "lines": 49
        },
        {
          "name": "run_all_tests.py",
          "type": "file",
          "path": "tests/run_all_tests.py",
          "lines": 63
        },
        {
          "name": "test_advanced_patterns.py",
          "type": "file",
          "path": "tests/test_advanced_patterns.py",
          "lines": 73
        },
        {
          "name": "test_basic_crawling.py",
          "type": "file",
          "path": "tests/test_basic_crawling.py",
          "lines": 50
        },
        {
          "name": "test_data_extraction.py",
          "type": "file",
          "path": "tests/test_data_extraction.py",
          "lines": 63
        },
        {
          "name": "test_markdown_generation.py",
          "type": "file",
          "path": "tests/test_markdown_generation.py",
          "lines": 88
        }
      ]
    },
    {
      "name": "README.md",
      "type": "file",
      "path": "README.md",
      "lines": 298
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 404
    }
  ]
}
