{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-15T11:39:45.009Z",
    "slug": "alirezarezvani-senior-data-engineer",
    "source_url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/senior-data-engineer",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "47443a66dba421d32f6e8208e24305574eb4015f7ba85f34075bdc86d5c10bbd",
    "tree_hash": "3a15cdeed71d186493da576544ca6330fb6eced8e25935fe270b435077de96ee"
  },
  "skill": {
    "name": "senior-data-engineer",
    "description": "World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data infrastructure. Expertise in Python, SQL, Spark, Airflow, dbt, Kafka, and modern data stack. Includes data modeling, pipeline orchestration, data quality, and DataOps. Use when designing data architectures, building data pipelines, optimizing data workflows, or implementing data governance.",
    "summary": "World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data i...",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "alirezarezvani",
    "license": "MIT",
    "tags": [
      "senior-data-engineer",
      "ai",
      "skill",
      "claude"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "medium",
    "is_blocked": false,
    "safe_to_publish": false,
    "summary": "AI analysis failed after multiple attempts - MANUAL REVIEW REQUIRED before publishing. This skill cannot be auto-published until reviewed by a human.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 14,
            "line_end": 23
          },
          {
            "file": "SKILL.md",
            "line_start": 23,
            "line_end": 54
          },
          {
            "file": "SKILL.md",
            "line_start": 54,
            "line_end": 65
          },
          {
            "file": "SKILL.md",
            "line_start": 65,
            "line_end": 75
          },
          {
            "file": "SKILL.md",
            "line_start": 75,
            "line_end": 167
          },
          {
            "file": "SKILL.md",
            "line_start": 167,
            "line_end": 185
          },
          {
            "file": "SKILL.md",
            "line_start": 185,
            "line_end": 189
          },
          {
            "file": "SKILL.md",
            "line_start": 189,
            "line_end": 190
          },
          {
            "file": "SKILL.md",
            "line_start": 190,
            "line_end": 191
          },
          {
            "file": "SKILL.md",
            "line_start": 191,
            "line_end": 192
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [
      {
        "title": "Weak cryptographic algorithm",
        "description": "Weak cryptographic algorithm detected in code",
        "locations": [
          {
            "file": "references/data_modeling_patterns.md",
            "line_start": 9,
            "line_end": 9
          },
          {
            "file": "references/data_modeling_patterns.md",
            "line_start": 11,
            "line_end": 11
          },
          {
            "file": "references/data_modeling_patterns.md",
            "line_start": 17,
            "line_end": 17
          },
          {
            "file": "references/data_modeling_patterns.md",
            "line_start": 62,
            "line_end": 62
          },
          {
            "file": "references/data_pipeline_architecture.md",
            "line_start": 9,
            "line_end": 9
          },
          {
            "file": "references/data_pipeline_architecture.md",
            "line_start": 11,
            "line_end": 11
          },
          {
            "file": "references/data_pipeline_architecture.md",
            "line_start": 17,
            "line_end": 17
          },
          {
            "file": "references/data_pipeline_architecture.md",
            "line_start": 62,
            "line_end": 62
          },
          {
            "file": "references/dataops_best_practices.md",
            "line_start": 9,
            "line_end": 9
          },
          {
            "file": "references/dataops_best_practices.md",
            "line_start": 11,
            "line_end": 11
          },
          {
            "file": "references/dataops_best_practices.md",
            "line_start": 17,
            "line_end": 17
          },
          {
            "file": "references/dataops_best_practices.md",
            "line_start": 62,
            "line_end": 62
          },
          {
            "file": "scripts/data_quality_validator.py",
            "line_start": 71,
            "line_end": 71
          },
          {
            "file": "scripts/etl_performance_optimizer.py",
            "line_start": 71,
            "line_end": 71
          },
          {
            "file": "scripts/pipeline_orchestrator.py",
            "line_start": 71,
            "line_end": 71
          },
          {
            "file": "SKILL.md",
            "line_start": 3,
            "line_end": 3
          },
          {
            "file": "SKILL.md",
            "line_start": 3,
            "line_end": 3
          },
          {
            "file": "SKILL.md",
            "line_start": 3,
            "line_end": 3
          },
          {
            "file": "SKILL.md",
            "line_start": 30,
            "line_end": 30
          },
          {
            "file": "SKILL.md",
            "line_start": 68,
            "line_end": 68
          },
          {
            "file": "SKILL.md",
            "line_start": 69,
            "line_end": 69
          },
          {
            "file": "SKILL.md",
            "line_start": 77,
            "line_end": 77
          },
          {
            "file": "SKILL.md",
            "line_start": 90,
            "line_end": 90
          }
        ]
      },
      {
        "title": "Network reconnaissance",
        "description": "Network reconnaissance detected in code",
        "locations": [
          {
            "file": "references/data_modeling_patterns.md",
            "line_start": 21,
            "line_end": 22
          },
          {
            "file": "references/data_pipeline_architecture.md",
            "line_start": 21,
            "line_end": 22
          },
          {
            "file": "references/dataops_best_practices.md",
            "line_start": 21,
            "line_end": 22
          }
        ]
      },
      {
        "title": "Ruby/shell backtick execution",
        "description": "Ruby/shell backtick execution detected in code",
        "locations": [
          {
            "file": "SKILL.md",
            "line_start": 14,
            "line_end": 23
          },
          {
            "file": "SKILL.md",
            "line_start": 23,
            "line_end": 54
          },
          {
            "file": "SKILL.md",
            "line_start": 54,
            "line_end": 65
          },
          {
            "file": "SKILL.md",
            "line_start": 65,
            "line_end": 75
          },
          {
            "file": "SKILL.md",
            "line_start": 75,
            "line_end": 167
          },
          {
            "file": "SKILL.md",
            "line_start": 167,
            "line_end": 185
          },
          {
            "file": "SKILL.md",
            "line_start": 185,
            "line_end": 189
          },
          {
            "file": "SKILL.md",
            "line_start": 189,
            "line_end": 190
          },
          {
            "file": "SKILL.md",
            "line_start": 190,
            "line_end": 191
          },
          {
            "file": "SKILL.md",
            "line_start": 191,
            "line_end": 192
          }
        ]
      }
    ],
    "files_scanned": 7,
    "total_lines": 773,
    "audit_model": "claude",
    "audited_at": "2026-01-15T11:39:45.009Z"
  },
  "content": {
    "user_title": "Senior Data Engineer",
    "value_statement": "An AI skill for Claude, Codex, and Claude Code",
    "seo_keywords": [
      "senior-data-engineer",
      "ai",
      "skill",
      "claude"
    ],
    "actual_capabilities": [
      "Build and orchestrate production-grade data pipelines using Python, Spark, and Airflow",
      "Design scalable ETL/ELT workflows with data quality validation and monitoring",
      "Implement data modeling patterns including dimensional modeling and data vault architectures",
      "Optimize data pipeline performance at scale with distributed computing frameworks",
      "Develop DataOps best practices including CI/CD, automated testing, and deployment strategies",
      "Integrate modern data stack tools including dbt, Kafka, Databricks, and cloud data warehouses",
      "Build real-time and batch data processing systems with fault tolerance and high availability",
      "Implement data governance, security, and compliance controls for PII and sensitive data"
    ],
    "limitations": [
      "Requires knowledge of existing data infrastructure and schema designs to provide optimal recommendations",
      "Performance optimization advice may need to be adapted based on specific cloud provider capabilities",
      "Security and compliance implementations should be reviewed by security teams for production use",
      "Cost optimization strategies depend on specific cloud provider pricing and organizational requirements",
      "Real-time processing patterns may require additional infrastructure beyond scope of this skill"
    ],
    "use_cases": [
      {
        "title": "Build Scalable Data Pipeline",
        "description": "Design and implement an end-to-end data pipeline that ingests data from multiple sources, transforms it using Spark or dbt, and loads it into a data warehouse with proper orchestration, monitoring, and data quality checks.",
        "target_user": "Data engineers building production data infrastructure"
      },
      {
        "title": "Optimize ETL Performance",
        "description": "Analyze and optimize existing ETL workflows by identifying bottlenecks, implementing efficient data transformations, adding proper partitioning strategies, and leveraging distributed computing frameworks for better performance.",
        "target_user": "Senior engineers dealing with slow or failing data pipelines"
      },
      {
        "title": "Implement Data Quality Framework",
        "description": "Set up comprehensive data quality validation using the included data quality validator script, defining schemas, validation rules, and monitoring alerts to ensure data integrity across pipelines.",
        "target_user": "Data platform teams establishing data governance"
      },
      {
        "title": "Design Data Architecture",
        "description": "Create data modeling patterns and architectural designs for data lakes, data warehouses, or lakehouse architectures with proper separation of raw, curated, and presentation layers following best practices.",
        "target_user": "Architects designing greenfield data platforms"
      }
    ],
    "prompt_templates": [
      {
        "title": "Design Data Pipeline Architecture",
        "prompt": "I need to build a data pipeline that ingests data from [source systems], performs [transformation requirements], and loads it into [target data warehouse]. The pipeline should handle [volume/frequency] and needs to support [specific requirements like real-time, idempotency, etc.]. Can you help me design the architecture and recommend the best tools from the modern data stack?",
        "scenario": "Starting a new data pipeline project and need architectural guidance"
      },
      {
        "title": "Optimize Slow Data Pipeline",
        "prompt": "My data pipeline is taking [X hours/minutes] to process [Y volume] of data. The pipeline uses [current tools/tech]. The main bottlenecks appear to be [observed issues]. Can you analyze this and provide specific optimization recommendations?",
        "scenario": "Existing pipeline has performance issues that need troubleshooting"
      },
      {
        "title": "Implement Data Quality Checks",
        "prompt": "I need to add data quality validation to my pipeline for [dataset/table]. The key quality requirements are [list requirements like completeness, uniqueness, timeliness, etc.]. Can you help me implement this using the data quality validator script and set up appropriate alerts?",
        "scenario": "Need to ensure data quality and catch issues early in pipelines"
      },
      {
        "title": "Set Up DataOps Workflow",
        "prompt": "I want to implement DataOps best practices for my data team working with [tools/platforms]. We need CI/CD for our dbt models, automated testing, environment management, and deployment automation. Can you help me set up this workflow?",
        "scenario": "Moving from manual data workflows to automated DataOps practices"
      }
    ],
    "output_examples": [
      {
        "input": "Help me design a batch ETL pipeline to ingest sales data from PostgreSQL, transform it, and load into Snowflake nightly",
        "output": "Based on your requirements, I recommend this architecture:\n\n1. **Extraction Layer**: Use Airflow DAG to orchestrate extraction from PostgreSQL using incremental loads based on updated_at timestamp\n2. **Staging**: Land raw data in S3/cloud storage in Parquet format with date partitioning\n3. **Transformation**: Use dbt to transform data with these models:\n   - stg_sales: staging layer with light cleaning\n   - int_sales_enriched: intermediate layer joining with dimension tables\n   - fct_sales: final fact table with business metrics\n4. **Loading**: dbt incremental models to Snowflake with merge strategy\n5. **Orchestration**: Airflow DAG with sensors, proper task dependencies, and retry logic\n6. **Monitoring**: Add data quality checks using dbt tests and custom validators\n\nI can help you implement any of these components using the pipeline orchestrator script."
      },
      {
        "input": "My Spark job is running out of memory when processing 500GB of data. What should I check?",
        "output": "Let's diagnose and fix the memory issues:\n\n**Immediate Checks:**\n1. Executor memory allocation - increase executor memory and reduce executor cores\n2. Partition size - repartition to ~128MB per partition (500GB / 128MB = ~4000 partitions)\n3. Shuffle operations - check for expensive joins or groupBy operations\n4. Data skew - look for hot keys causing uneven partition sizes\n\n**Optimizations:**\n1. Use broadcast joins for small dimension tables (<2GB)\n2. Enable adaptive query execution (AQE) in Spark 3.x\n3. Persist intermediate results strategically with appropriate storage levels\n4. Use columnar formats (Parquet) with predicate pushdown\n5. Consider bucketing for frequently joined tables\n\nI can help you run the ETL performance optimizer script to analyze your job and generate specific recommendations."
      }
    ],
    "best_practices": [
      "Use incremental loading strategies with proper watermarking to avoid full table scans",
      "Implement idempotent pipelines that produce consistent results when re-run",
      "Separate raw, curated, and presentation data layers for better data governance",
      "Add comprehensive data quality validation at each pipeline stage with automated alerts",
      "Use infrastructure-as-code for data pipeline deployment and environment management",
      "Implement proper error handling, retry logic, and dead letter queues for failed records",
      "Monitor pipeline performance metrics including latency, throughput, and data freshness",
      "Document data lineage and maintain a data catalog for discoverability",
      "Use version control for all data transformation code and configuration",
      "Implement proper access controls and encryption for sensitive data"
    ],
    "anti_patterns": [
      "Avoid hard-coding credentials or connection strings in pipeline code",
      "Don't use SELECT * in production queries - explicitly specify required columns",
      "Never skip data quality validation assuming upstream data is always correct",
      "Avoid creating overly complex transformations in a single pipeline step",
      "Don't ignore failed pipeline runs or rely solely on manual monitoring",
      "Avoid tightly coupling data pipelines to specific infrastructure",
      "Don't process data without proper schema validation and type enforcement",
      "Never deploy pipeline changes directly to production without testing",
      "Avoid building custom solutions when proven open-source tools exist",
      "Don't neglect cost optimization and monitoring for cloud data processing"
    ],
    "faq": [
      {
        "question": "What's the difference between ETL and ELT, and when should I use each?",
        "answer": "ETL (Extract-Transform-Load) transforms data before loading into the warehouse, while ELT (Extract-Load-Transform) loads raw data first and transforms within the warehouse. Use ELT when you have a modern cloud data warehouse (Snowflake, BigQuery) with strong compute capabilities - this provides flexibility and leverages warehouse optimization. Use ETL when dealing with legacy systems, need to mask sensitive data before loading, or have limited warehouse compute resources. Modern data stacks like dbt favor the ELT approach."
      },
      {
        "question": "How do I handle data quality issues in production pipelines?",
        "answer": "Implement a multi-layered approach: (1) Add schema validation at ingestion using tools like Great Expectations or the included data quality validator, (2) Build data quality checks into your dbt models as tests, (3) Set up monitoring and alerting for critical metrics like row counts, null rates, and freshness, (4) Create a dead letter queue for failed records rather than failing entire pipeline runs, (5) Implement data contracts between teams defining expected schemas and SLAs. Always validate data at system boundaries and critical transformation points."
      },
      {
        "question": "What tools should I use for orchestrating data pipelines?",
        "answer": "Apache Airflow is the industry standard for complex data pipeline orchestration with extensive integrations and community support. Use Prefect or Dagster for more modern Python-native approaches with better developer experience. For simpler workflows, consider dbt Cloud's orchestration or cloud-native options like AWS Step Functions or GCP Cloud Composer (managed Airflow). The included pipeline orchestrator script provides a starting point that can integrate with any of these tools. Choose based on your team's Python proficiency, infrastructure preferences, and complexity requirements."
      },
      {
        "question": "How can I optimize costs for cloud data processing?",
        "answer": "Key strategies include: (1) Right-size compute resources and use autoscaling where available, (2) Partition data properly to minimize scanned data volumes, (3) Use appropriate compression formats like Parquet with Snappy, (4) Implement data lifecycle policies to archive or delete old data, (5) Schedule heavy batch jobs during off-peak hours if pricing varies, (6) Cache frequently accessed data and use materialized views, (7) Monitor query patterns and optimize expensive queries, (8) Use spot/preemptible instances for fault-tolerant workloads. The ETL performance optimizer script can help identify cost optimization opportunities."
      },
      {
        "question": "Should I use Spark for my data pipeline?",
        "answer": "Use Spark when you need to process large volumes (>100GB) of data that don't fit in memory, need distributed computing across clusters, or require complex transformations that benefit from parallel processing. For smaller datasets (<100GB) or simple transformations, tools like dbt with SQL or Pandas may be more appropriate and easier to maintain. Modern cloud data warehouses can handle many transformations that previously required Spark. Consider Spark when you need real-time streaming with Structured Streaming, ML workloads with MLlib, or need to process data across heterogeneous sources."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "data_modeling_patterns.md",
          "type": "file",
          "path": "references/data_modeling_patterns.md",
          "lines": 81
        },
        {
          "name": "data_pipeline_architecture.md",
          "type": "file",
          "path": "references/data_pipeline_architecture.md",
          "lines": 81
        },
        {
          "name": "dataops_best_practices.md",
          "type": "file",
          "path": "references/dataops_best_practices.md",
          "lines": 81
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "data_quality_validator.py",
          "type": "file",
          "path": "scripts/data_quality_validator.py",
          "lines": 101
        },
        {
          "name": "etl_performance_optimizer.py",
          "type": "file",
          "path": "scripts/etl_performance_optimizer.py",
          "lines": 101
        },
        {
          "name": "pipeline_orchestrator.py",
          "type": "file",
          "path": "scripts/pipeline_orchestrator.py",
          "lines": 101
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 227
    }
  ]
}
