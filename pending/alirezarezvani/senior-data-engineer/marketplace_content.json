{
  "skill": {
    "name": "Senior Data Engineer",
    "description": "World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data infrastructure. Expertise in Python, SQL, Spark, Airflow, dbt, Kafka, and modern data stack. Includes data modeling, pipeline orchestration, data quality, and DataOps.",
    "summary": "Build production data pipelines with Python, Spark, Airflow, and modern data stack tools",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "claude-skills",
    "license": "MIT",
    "category": "data",
    "tags": ["data-engineering", "pipelines", "etl", "dataops"],
    "supported_tools": ["claude", "codex", "claude-code"]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 36 static findings are FALSE POSITIVES. Scanner incorrectly flagged documentation about design patterns and algorithms as cryptographic code. No actual crypto implementations, network reconnaissance tools, or shell execution patterns exist in this skill. Files contain only standard documentation and template Python scripts for data engineering workflows.",
    "static_findings_evaluation": [
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_modeling_patterns.md:9",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 9 is part of documentation header '### Production-First Design'. This is a markdown heading, not cryptographic code. Scanner incorrectly flagged documentation about design patterns."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_modeling_patterns.md:11",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 11 contains bullet points about scalability and reliability - documentation text describing engineering best practices, not crypto implementations."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_modeling_patterns.md:17",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 17 is '### Performance by Design' heading - documentation about optimization principles, not cryptographic code."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_modeling_patterns.md:62",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 62 is '### Reliability' heading followed by bullet points about failure design. Documentation only."
      },
      {
        "finding": "[LOW] blocker: Network reconnaissance at references/data_modeling_patterns.md:21",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 21 contains 'Resource awareness' - a standard development best practice term. No network scanning tools present."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_pipeline_architecture.md:9",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Same as data_modeling_patterns.md - documentation header '### Production-First Design'. No crypto code."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_pipeline_architecture.md:11",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation bullet points about scalability targets. Pure documentation, no implementations."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_pipeline_architecture.md:17",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation heading '### Performance by Design' - conceptual text only."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/data_pipeline_architecture.md:62",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation section about reliability patterns. No cryptographic implementations."
      },
      {
        "finding": "[LOW] blocker: Network reconnaissance at references/data_pipeline_architecture.md:21",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 21 is 'Resource awareness' - development best practice, not network reconnaissance."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/dataops_best_practices.md:9",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation section header '### Production-First Design'. Pure documentation."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/dataops_best_practices.md:11",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation bullet points about production design principles. No code implementations."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/dataops_best_practices.md:17",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation heading '### Performance by Design'. Conceptual text only."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at references/dataops_best_practices.md:62",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation section about reliability. Pure reference material."
      },
      {
        "finding": "[LOW] blocker: Network reconnaissance at references/dataops_best_practices.md:21",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 21 is 'Resource awareness' - standard development terminology."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at scripts/data_quality_validator.py:71",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 71 is past the main code - _execute() returns at line 66. Lines 67-70 are comments and closing braces. No crypto code present."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at scripts/etl_performance_optimizer.py:71",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Same pattern - line 71 is past the code logic. No cryptographic implementations in this template script."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at scripts/pipeline_orchestrator.py:71",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Same pattern - line 71 is past the code logic. Template script with no crypto usage."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:14",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 14 is '```bash' code block delimiter - markdown documentation syntax, not backtick execution."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:23",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 23 is '```' closing code block delimiter. Documentation formatting, not execution."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:54",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 54 references 'references/data_pipeline_architecture.md' - file path reference, not execution."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:65",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 65 references 'references/data_modeling_patterns.md' - file path reference."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:75",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 75 references 'references/dataops_best_practices.md' - file path reference."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:167",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 167 is '```bash' opening code block for development commands. Documentation only."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:185",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 185 is '```' closing code block delimiter. Documentation formatting."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:189",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 189 references 'references/data_pipeline_architecture.md' - file path reference."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:190",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 190 references 'references/data_modeling_patterns.md' - file path reference."
      },
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution at SKILL.md:191",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 191 references 'references/dataops_best_practices.md' - file path reference."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at SKILL.md:3",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 3 is YAML frontmatter description field. Contains no code - metadata only."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at SKILL.md:30",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 30 is 'Scalable system design and implementation' - capability description. Documentation only."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at SKILL.md:68",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 68 is 'Architecture design patterns' - documentation section title. No crypto code."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at SKILL.md:69",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 69 is 'Tool integration guides' - documentation bullet point. Pure text."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at SKILL.md:77",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 77 is 'System design principles' - documentation heading. No implementations."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm at SKILL.md:90",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Line 90 is 'Advanced Patterns:' heading in Production Patterns section. Documentation only."
      }
    ],
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 7,
    "total_lines": 773
  },
  "content": {
    "user_title": "Build production data pipelines with Python",
    "value_statement": "Design and deploy scalable ETL workflows, data quality systems, and pipeline orchestration. This skill provides production-ready patterns for Spark, Airflow, dbt, and Kafka implementations.",
    "seo_keywords": ["claude data engineer skill", "claude code data pipelines", "spark etl automation", "airflow dags python", "data quality validation", "claude codex data engineering", "kafka streaming python", "dbt transformation", "dataops automation", "claude skill marketplace"],
    "actual_capabilities": [
      "Design and implement production ETL/ELT pipelines with Python and Spark",
      "Configure Airflow DAGs for workflow orchestration and scheduling",
      "Build data quality validation frameworks with schema enforcement",
      "Optimize pipeline performance through batching, caching, and partitioning",
      "Set up DataOps monitoring with logging, alerting, and drift detection",
      "Implement data governance with encryption, access controls, and audit trails"
    ],
    "limitations": [
      "Does not provide pre-built infrastructure or cloud resource provisioning",
      "Requires user to configure execution environment and dependencies",
      "Does not execute pipelines directly - provides patterns and code templates",
      "User must manage their own data sources, credentials, and integrations"
    ],
    "use_cases": [
      {"target_user": "Data Engineers", "title": "Build scalable pipelines", "description": "Create production data pipelines with Spark, Airflow, and modern data stack tools."},
      {"target_user": "ML Engineers", "title": "Deploy ML data infrastructure", "description": "Set up feature stores, model serving pipelines, and training data workflows."},
      {"target_user": "DevOps Teams", "title": "Automate data operations", "description": "Implement CI/CD for data pipelines, monitoring, and alerting systems."}
    ],
    "prompt_templates": [
      {"title": "Pipeline Design", "scenario": "Create ETL workflow", "prompt": "Design a production ETL pipeline using Python and Spark. Include data validation, error handling, and monitoring. Use Airflow for orchestration with daily scheduling."},
      {"title": "Quality Framework", "scenario": "Validate data quality", "prompt": "Build a data quality validation framework with schema enforcement, null checks, and anomaly detection. Include reporting and alerting for quality violations."},
      {"title": "Performance Tuning", "scenario": "Optimize pipeline speed", "prompt": "Analyze and optimize my data pipeline for better performance. Focus on Spark partitioning, caching strategies, and batch processing improvements."},
      {"title": "DataOps Setup", "scenario": "Implement monitoring", "prompt": "Set up a complete DataOps monitoring system for my data pipelines. Include logging, metrics, alerting, and drift detection for data quality."}
    ],
    "output_examples": [
      {"input": "Design a real-time data pipeline for streaming analytics with Kafka and Spark Streaming.", "output": ["Architecture: Kafka producers â†’ Spark Streaming â†’ Delta Lake â†’ BI dashboards", "Partition strategy: By event_time and device_id for parallel processing", "Checkpointing: Enable for fault tolerance and exactly-once semantics", "Monitoring: Track processing latency, lag, and error rates"]},
      {"input": "Create a data validation framework for a machine learning feature store.", "output": ["Schema validation: Enforce types, ranges, and freshness constraints", "Statistical checks: Detect drift in feature distributions over time", "Notification: Alert when features exceed acceptable thresholds", "Reporting: Generate daily quality scores and trends"]},
      {"input": "Optimize our dbt models for better build times and query performance.", "output": ["Materialization: Convert to incremental models where possible", "Partitioning: Add partition_by clauses on date columns", "Testing: Add generic and singular tests for data contracts", "Performance: Add indexes and optimize dbt source freshness"]}
    ],
    "best_practices": [
      "Design for failure with retries, circuit breakers, and checkpointing",
      "Implement comprehensive data quality checks at each pipeline stage",
      "Monitor everything including pipeline latency, data freshness, and schema changes"
    ],
    "anti_patterns": [
      "Running production workloads without monitoring or alerting",
      "Skipping data quality validation and schema enforcement",
      "Deploying pipelines without testing on realistic data volumes"
    ],
    "faq": [
      {"question": "What Python libraries does this skill use?", "answer": "PySpark, Pandas, Airflow, Great Expectations, Great Table, and SQLAlchemy."},
      {"question": "Can this skill create Airflow DAGs?", "answer": "Yes, it provides DAG templates and best practices for workflow orchestration."},
      {"question": "Does it handle real-time streaming?", "answer": "Yes, includes patterns for Kafka, Spark Streaming, and batch micro-services."},
      {"question": "What data quality tools are included?", "answer": "Framework for schema validation, null checks, drift detection, and anomaly alerts."},
      {"question": "Does it provision cloud resources?", "answer": "No, it provides patterns and code. Users configure their own infrastructure."},
      {"question": "Can I use this with dbt?", "answer": "Yes, includes dbt integration patterns and transformation best practices."}
    ]
  }
}
