{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T09:26:59.263Z",
    "slug": "89jobrien-cocoindex",
    "source_url": "https://github.com/89jobrien/steve/tree/main/steve/skills/cocoindex",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "301475b286f590f011a70a2acedf361ba2db22fed5cfbfdeb772880e86d2b04e",
    "tree_hash": "f03436529ade83fd1bc989e7cdc80e696624cf13d4dd4deab07a0156c09273cf"
  },
  "skill": {
    "name": "cocoindex",
    "description": "Comprehensive toolkit for developing with the CocoIndex library. Use when users need to create data transformation pipelines (flows), write custom functions, or operate flows via CLI or API. Covers building ETL workflows for AI data processing, including embedding documents into vector databases, building knowledge graphs, creating search indexes, or processing data streams with incremental updates.",
    "summary": "Comprehensive toolkit for developing with the CocoIndex library. Use when users need to create data ...",
    "icon": "ðŸ”„",
    "version": "1.0.1",
    "author": "Joseph OBrien",
    "license": "MIT",
    "category": "data",
    "tags": [
      "data-processing",
      "vector-search",
      "etl",
      "ai-data-pipeline"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill containing only markdown reference files for the CocoIndex library. No executable code, scripts, or runtime components. This skill only displays documentation and does not perform any file access, network operations, or code execution.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 1640,
    "audit_model": "claude",
    "audited_at": "2026-01-10T09:26:59.263Z"
  },
  "content": {
    "user_title": "Build AI data pipelines with CocoIndex",
    "value_statement": "Creating data transformation pipelines for AI applications requires understanding complex ETL patterns, embedding models, and vector databases. CocoIndex provides a unified framework for building real-time indexing flows that extract from multiple sources, transform with chunking and embeddings, and export to vector databases and knowledge graphs.",
    "seo_keywords": [
      "CocoIndex",
      "vector search index",
      "data transformation pipeline",
      "AI data processing",
      "document embedding",
      "knowledge graph",
      "Claude",
      "Codex",
      "Claude Code",
      "incremental ETL"
    ],
    "actual_capabilities": [
      "Create indexing flows for extracting data from files, S3, databases",
      "Apply transformations including text chunking and embeddings",
      "Export results to Postgres, Qdrant, LanceDB, or Neo4j",
      "Build custom transformation functions for specialized logic",
      "Run live updates that continuously sync source changes",
      "Operate flows via CLI or Python API"
    ],
    "limitations": [
      "Requires CocoIndex library installation and database setup",
      "Does not include runtime execution environment",
      "LLM API keys must be provided by the user",
      "Reference documentation only - users implement flows in their own projects"
    ],
    "use_cases": [
      {
        "target_user": "Data engineers building RAG systems",
        "title": "Build vector search indexes",
        "description": "Create pipelines that embed documents and store in vector databases for semantic search."
      },
      {
        "target_user": "ML engineers creating AI pipelines",
        "title": "Process data for AI applications",
        "description": "Transform raw data through chunking, embedding, and extraction for AI model consumption."
      },
      {
        "target_user": "Developers building knowledge systems",
        "title": "Construct knowledge graphs",
        "description": "Extract structured entities using LLMs and build graph databases for relationship-based queries."
      }
    ],
    "prompt_templates": [
      {
        "title": "Create vector index",
        "scenario": "Build embedding pipeline for documents",
        "prompt": "Help me create a CocoIndex flow that reads markdown files from a local directory, splits them into chunks of 2000 characters with 500 overlap, generates embeddings using OpenAI text-embedding-3-small, and exports to Postgres with pgvector for semantic search."
      },
      {
        "title": "Build knowledge graph",
        "scenario": "Extract entities and relationships",
        "prompt": "Show me how to use CocoIndex to read JSON product files, extract structured information using GPT-4, and export the results as nodes and relationships in a Neo4j knowledge graph."
      },
      {
        "title": "Implement live updates",
        "scenario": "Set up continuous sync",
        "prompt": "I want to create a CocoIndex flow with live updates. Help me configure a local file source with a refresh interval and set up automatic processing when files change."
      },
      {
        "title": "Write custom function",
        "scenario": "Create specialized transformation",
        "prompt": "I need to create a custom CocoIndex function that calls an external API to enrich my data. Show me how to use the spec+executor pattern with caching and API authentication."
      }
    ],
    "output_examples": [
      {
        "input": "Build a CocoIndex flow that embeds my documents",
        "output": [
          "Set up project with cocoindex package",
          "Create flow definition with LocalFile source",
          "Apply SplitRecursively for chunking",
          "Use SentenceTransformerEmbed or EmbedText for vectors",
          "Export to vector database target",
          "Run setup then update to build index"
        ]
      }
    ],
    "best_practices": [
      "Use evaluate command to test flows before running update",
      "Always assign transformed data to row fields, not local variables",
      "Increment behavior_version when modifying cached functions",
      "Add refresh_interval to sources for live update mode"
    ],
    "anti_patterns": [
      "Using local variables instead of row fields for transformation results",
      "Creating unnecessary dataclasses to mirror flow field schemas",
      "Omitting type annotations on custom function return values",
      "Running update without first running setup on new flows"
    ],
    "faq": [
      {
        "question": "Which AI tools is CocoIndex compatible with?",
        "answer": "CocoIndex works with OpenAI, Anthropic, Gemini, Voyage, and Ollama for embeddings and LLM extraction. Claude, Codex, and Claude Code can all use CocoIndex flows."
      },
      {
        "question": "What are the size limits for data processing?",
        "answer": "CocoIndex supports configurable concurrency limits. Set max_inflight_rows and max_inflight_bytes to control memory usage during processing."
      },
      {
        "question": "How do I integrate with my existing codebase?",
        "answer": "Install cocoindex package, define flows as Python functions with @cocoindex.flow_def decorator, then use CLI or Python API to operate flows."
      },
      {
        "question": "Is my data safe when using CocoIndex?",
        "answer": "CocoIndex runs locally with your data. API keys are read from environment variables. Source data stays on your machine except when explicitly exported to configured targets."
      },
      {
        "question": "Why does my flow fail with database connection error?",
        "answer": "Ensure COCOINDEX_DATABASE_URL is set in your .env file. The default is postgres://cocoindex:cocoindex@localhost/cocoindex for local development."
      },
      {
        "question": "How does CocoIndex compare to LangChain or LlamaIndex?",
        "answer": "CocoIndex focuses on real-time ETL pipelines with incremental processing. It complements orchestration libraries by handling the indexing and data transformation layer."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "api-operations.md",
          "type": "file",
          "path": "references/api-operations.md"
        },
        {
          "name": "cli-operations.md",
          "type": "file",
          "path": "references/cli-operations.md"
        },
        {
          "name": "custom-functions.md",
          "type": "file",
          "path": "references/custom-functions.md"
        },
        {
          "name": "flow-patterns.md",
          "type": "file",
          "path": "references/flow-patterns.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
