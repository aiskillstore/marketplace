{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T09:32:40.879Z",
    "slug": "89jobrien-meta-cognitive-reasoning",
    "source_url": "https://github.com/89jobrien/steve/tree/main/steve/skills/meta-cognitive-reasoning",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "b34b2eff30ea428c10c34be694ec257fc634ed8855edb94b499481327110a720",
    "tree_hash": "32bd500942ea052241e5795147afc7df64a44d79640e49b75616880d86b1aa57"
  },
  "skill": {
    "name": "meta-cognitive-reasoning",
    "description": "Meta-cognitive reasoning specialist for evidence-based analysis, hypothesis testing, and cognitive failure prevention. Use when conducting reviews, making assessments, debugging complex issues, or any task requiring rigorous analytical reasoning. Prevents premature conclusions, assumption-based errors, and pattern matching without verification.",
    "summary": "Meta-cognitive reasoning specialist for evidence-based analysis, hypothesis testing, and cognitive f...",
    "icon": "ðŸ§ ",
    "version": "1.0.1",
    "author": "Joseph OBrien",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "reasoning",
      "analysis",
      "review",
      "debugging",
      "assessment",
      "decision-making",
      "cognitive failure prevention",
      "meta-cognitive reasoning",
      "evidence-based reasoning"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a prompt-only skill consisting of Markdown documentation. No executable code, network calls, filesystem access beyond reading its own files, environment variable access, or external command execution. The skill provides reasoning frameworks for evidence-based analysis.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 1260,
    "audit_model": "claude",
    "audited_at": "2026-01-10T09:32:40.879Z"
  },
  "content": {
    "user_title": "Apply disciplined reasoning frameworks",
    "value_statement": "AI assistants often jump to conclusions based on pattern matching without verification. This skill enforces evidence-based reasoning, multiple hypothesis generation, and systematic verification before any conclusions are drawn. It prevents costly errors from assumption-based recommendations.",
    "seo_keywords": [
      "meta-cognitive reasoning",
      "evidence-based analysis",
      "cognitive failure prevention",
      "hypothesis testing",
      "Claude Code",
      "Claude",
      "Codex",
      "systematic verification",
      "analytical reasoning",
      "decision-making frameworks"
    ],
    "actual_capabilities": [
      "Enforces evidence-based conclusions with tool output before interpretation",
      "Generates multiple hypotheses to prevent premature commitment to single explanations",
      "Handles temporal knowledge limitations by verifying current information",
      "Provides self-correction protocols for transparent error handling",
      "Allocates cognitive effort appropriately using parsimony principles",
      "Distinguishes observation from mechanism before assessment"
    ],
    "limitations": [
      "Does not execute code or perform automated testing",
      "Does not access external systems or APIs",
      "Does not modify files or project structure",
      "Requires user to provide verification commands when needed"
    ],
    "use_cases": [
      {
        "target_user": "Code reviewers",
        "title": "Evidence-based code review",
        "description": "Review code with verified claims. Show file, line, and evidence for every issue identified before recommending changes."
      },
      {
        "target_user": "Debugging teams",
        "title": "Multiple hypothesis debugging",
        "description": "Debug complex issues by generating competing hypotheses. Gather discriminating evidence before concluding on root cause."
      },
      {
        "target_user": "Technical writers",
        "title": "Documentation quality analysis",
        "description": "Evaluate documentation value. Keep content that explains non-obvious context, constraints, and edge cases."
      }
    ],
    "prompt_templates": [
      {
        "title": "Verify version claim",
        "scenario": "Check if package version exists",
        "prompt": "Verify that package X version Y exists on the registry before recommending any changes. Check the lockfile or dependency file first."
      },
      {
        "title": "Generate hypotheses",
        "scenario": "Analyze unexpected observation",
        "prompt": "The tests are failing with timeout errors. What are the possible mechanisms that could cause this? Generate competing hypotheses with opposite implications."
      },
      {
        "title": "Evidence-based review",
        "scenario": "Conduct rigorous review",
        "prompt": "Review this code and show evidence for every claim. Quote specific file:line:code. Never conclude without proof from tool output."
      },
      {
        "title": "Self-correction protocol",
        "scenario": "Correct previous analysis error",
        "prompt": "If you discover an error in your previous output, apply the self-correction protocol: acknowledge explicitly, state previous claim, provide evidence, explain error cause, and state clear action."
      }
    ],
    "output_examples": [
      {
        "input": "Review this code and check if certifi version 2025.10.5 is valid",
        "output": [
          "Let me verify the certifi version before concluding...",
          "Evidence from lockfile: name = \"certifi\", version = \"2025.10.5\", source = { registry = \"https://pypi.org/simple\" }",
          "Assessment: Version EXISTS on PyPI with valid hash",
          "Action: NO CHANGE NEEDED - version is valid"
        ]
      }
    ],
    "best_practices": [
      "Always show tool output before interpretation. Quote specific evidence from files, commands, or tests.",
      "Generate competing hypotheses before investigating. Consider mechanisms with opposite implications.",
      "Verify temporal knowledge before claiming something does not exist. Check the actual current state.",
      "Reserve strong language like CRITICAL or BLOCKER for issues proven with evidence."
    ],
    "anti_patterns": [
      "Claiming a file or function does not exist without running grep, ls, or other verification commands",
      "Concluding based on pattern matching without understanding the actual mechanism or context",
      "Declaring success without verifying all requirements with tool output",
      "Using strong language without evidence from the actual codebase"
    ],
    "faq": [
      {
        "question": "Does this skill work with all Claude models?",
        "answer": "Yes. This skill is compatible with Claude, Codex, and Claude Code. It works with any model that follows system prompts."
      },
      {
        "question": "What is the verification workflow for version checks?",
        "answer": "Check the lockfile or dependency file first. If version exists there, verify on the registry. Never claim non-existence without checking."
      },
      {
        "question": "How does this integrate with existing workflows?",
        "answer": "Activate this skill when starting analysis tasks. The frameworks apply to any review, debugging, or assessment work."
      },
      {
        "question": "Is any data sent to external servers?",
        "answer": "No. This is a prompt-only skill. It does not make network calls, access external APIs, or transmit any data."
      },
      {
        "question": "What if verification commands fail or are unavailable?",
        "answer": "State uncertainty explicitly. Say 'I cannot verify' rather than assuming. Ask the user to provide verification."
      },
      {
        "question": "How is this different from standard reasoning prompts?",
        "answer": "This provides specific frameworks and protocols rather than general guidance. It mandates evidence sequence, hypothesis generation, and self-correction."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "detailed-frameworks.md",
          "type": "file",
          "path": "references/detailed-frameworks.md"
        },
        {
          "name": "domain-examples.md",
          "type": "file",
          "path": "references/domain-examples.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
