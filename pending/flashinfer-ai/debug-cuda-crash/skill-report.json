{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-06T07:41:12.851Z",
    "slug": "flashinfer-ai-debug-cuda-crash",
    "source_url": "https://github.com/flashinfer-ai/flashinfer/tree/main/.claude/skills/debug-cuda-crash",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "f091a152d7a3761ff0b6c6d04b44cbedcc7939b9ea59e59e628ead87e552a466",
    "tree_hash": "0e5cb024d296f0e0f0ca94f263a257166f965de0c81ccbf3985df6612d63926a"
  },
  "skill": {
    "name": "debug-cuda-crash",
    "description": "Tutorial for debugging CUDA crashes using API logging",
    "summary": "Tutorial for debugging CUDA crashes using API logging",
    "icon": "ðŸ”§",
    "version": "1.0.0",
    "author": "flashinfer-ai",
    "license": "Apache-2.0",
    "category": "documentation",
    "tags": [
      "cuda",
      "debugging",
      "flashinfer",
      "tutorial",
      "gpu"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill contains only markdown documentation. No executable code, network calls, filesystem access, or command execution. Pure educational content for debugging CUDA crashes.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 575,
    "audit_model": "claude",
    "audited_at": "2026-01-06T07:41:12.851Z"
  },
  "content": {
    "user_title": "Debug CUDA Crashes in FlashInfer",
    "value_statement": "CUDA crashes often leave no debugging information. This tutorial teaches you to use FlashInfer's API logging to capture tensor metadata before crashes, making it easy to identify shape mismatches, dtype errors, and numerical issues.",
    "seo_keywords": [
      "debug cuda crash",
      "flashinfer debugging",
      "cuda illegal memory access",
      "api logging",
      "cuda debugging tutorial",
      "gpu kernel debugging",
      "compute-sanitizer",
      "cuda-gdb",
      "claude code",
      "codex"
    ],
    "actual_capabilities": [
      "Debug CUDA crashes by capturing input tensors before failure",
      "Detect NaN and Inf values using tensor statistics logging",
      "Identify shape and dtype mismatches in kernel inputs",
      "Use compute-sanitizer for memory access validation",
      "Debug multi-GPU processes with separate log files",
      "Apply cuda-gdb for kernel-level debugging"
    ],
    "limitations": [
      "Requires FlashInfer library installed with @flashinfer_api decorator support",
      "Logging must be enabled before running the code that crashes",
      "Statistics logging (level 5) is skipped during CUDA graph capture",
      "Does not fix crashes automatically, only aids diagnosis"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Debug production ML crashes",
        "description": "Diagnose CUDA errors in production LLM serving workloads using API logging to capture failing inputs"
      },
      {
        "target_user": "GPU Kernel Developers",
        "title": "Debug kernel memory errors",
        "description": "Use compute-sanitizer combined with API logging to find illegal memory access and out-of-bounds errors"
      },
      {
        "target_user": "Research Scientists",
        "title": "Fix numerical instability issues",
        "description": "Detect NaN/Inf values in attention computations by examining tensor statistics from logged outputs"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic crash debugging",
        "scenario": "FlashInfer kernel crashed",
        "prompt": "My FlashInfer code crashed with 'illegal memory access'. Help me set up API logging to debug this."
      },
      {
        "title": "Numerical issues",
        "scenario": "NaN values in output",
        "prompt": "My attention kernel is producing NaN values. How do I use level 5 logging to find the source?"
      },
      {
        "title": "Multi-GPU debugging",
        "scenario": "Distributed training crash",
        "prompt": "My multi-GPU training job crashed. Show me how to log each rank separately for debugging."
      },
      {
        "title": "Memory errors",
        "scenario": "Using compute-sanitizer",
        "prompt": "Combine FlashInfer API logging with compute-sanitizer to debug memory access errors. Give me the exact command."
      }
    ],
    "output_examples": [
      {
        "input": "My batch_decode_with_padded_kv_cache crashed with 'illegal memory access'",
        "output": [
          "To debug this crash:",
          "1. Set environment variables: export FLASHINFER_LOGLEVEL=3, export FLASHINFER_LOGDEST=debug.log",
          "2. Run your code again - inputs will be logged BEFORE the crash",
          "3. Check the log for the last API call's tensor shapes and dtypes",
          "4. Common issues: shape mismatch, wrong dtype, CPU tensor passed to GPU",
          "5. For deeper analysis: compute-sanitizer --tool memcheck python your_script.py"
        ]
      }
    ],
    "best_practices": [
      "Always start with level 3 logging for tensor metadata without overwhelming output",
      "Log to a file instead of stdout to preserve logs after crashes",
      "Compare the last successful API call (inputs and outputs logged) with the first failed call (only inputs logged)"
    ],
    "anti_patterns": [
      "Using level 5 logging by default (adds statistics overhead for every call)",
      "Logging to stdout when program crashes (output may be lost)",
      "Forgetting to disable logging in production (adds overhead even at level 1)"
    ],
    "faq": [
      {
        "question": "What CUDA versions support API logging?",
        "answer": "API logging works with any CUDA version supported by FlashInfer (CUDA 11.8+). The logging is implemented in Python and doesn't require specific CUDA features."
      },
      {
        "question": "Does logging slow down my code?",
        "answer": "Level 0 (disabled) has zero overhead. Level 1-3 add minimal overhead for metadata extraction. Level 5 adds synchronization for statistics, so avoid it during CUDA graph capture."
      },
      {
        "question": "Can I use this with PyTorch torch.compile()?",
        "answer": "Yes. API logging works with compiled PyTorch code. Note that statistics (level 5) are automatically skipped during graph capture to avoid synchronization issues."
      },
      {
        "question": "Is my data safe in the log files?",
        "answer": "Log files contain tensor shapes, dtypes, and statistics (min/max/mean/nan/inf). They do NOT contain actual tensor values. Store logs in secure locations for sensitive workloads."
      },
      {
        "question": "What if no logs appear even with FLASHINFER_LOGLEVEL=3?",
        "answer": "Verify the environment variable is set: echo $FLASHINFER_LOGLEVEL. Check that the API you're using has the @flashinfer_api decorator (not all APIs have it yet)."
      },
      {
        "question": "How is this different from CUDA's built-in tools?",
        "answer": "API logging shows you WHAT data caused the crash (shapes, dtypes, statistics). Tools like compute-sanitizer show WHERE the crash occurred (kernel, thread, instruction). Use both together for complete diagnosis."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
