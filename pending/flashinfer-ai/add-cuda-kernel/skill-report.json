{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-06T07:37:22.815Z",
    "slug": "flashinfer-ai-add-cuda-kernel",
    "source_url": "https://github.com/flashinfer-ai/flashinfer/tree/main/.claude/skills/add-cuda-kernel",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "91d5b7cadc5a1140deeab2d95bd75535341915e82b66beed8dec16c9bb69dfb6",
    "tree_hash": "a567307f03379bc76f72f75fedbfdd89c0ffb1e427a15c7752cafca056147610"
  },
  "skill": {
    "name": "add-cuda-kernel",
    "description": "Step-by-step tutorial for adding new CUDA kernels to FlashInfer",
    "summary": "Step-by-step tutorial for adding new CUDA kernels to FlashInfer",
    "icon": "âš¡",
    "version": "1.0.0",
    "author": "flashinfer-ai",
    "license": "Apache-2.0",
    "category": "documentation",
    "tags": [
      "cuda",
      "gpu",
      "kernel-development",
      "tutorial",
      "flashinfer"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Documentation-only skill containing tutorial content for adding CUDA kernels to FlashInfer. No executable code, no network access, no file system modifications beyond its own static file.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 806,
    "audit_model": "claude",
    "audited_at": "2026-01-06T07:37:22.814Z"
  },
  "content": {
    "user_title": "Add CUDA Kernels to FlashInfer",
    "value_statement": "FlashInfer needs new GPU kernels but the process is unclear. This tutorial provides step-by-step instructions for adding element-wise CUDA operations, covering kernel definition, TVM-FFI bindings, Python APIs, testing, and benchmarking.",
    "seo_keywords": [
      "claude code cuda kernel tutorial",
      "flashinfer kernel development guide",
      "add cuda kernel flashinfer",
      "gpu kernel tutorial claude",
      "nvidia cuda development skill",
      "tvm ffi binding tutorial",
      "llm gpu kernel development",
      "python cuda kernel skill",
      "claude cuda programming",
      "flashinfer kernel integration"
    ],
    "actual_capabilities": [
      "Define framework-agnostic CUDA kernels using raw pointers and templates",
      "Create TVM-FFI launcher code for PyTorch tensor integration",
      "Generate JIT module specifications with architecture targeting",
      "Build Python APIs with validation decorators and caching",
      "Write unit tests with parametrization for multiple configurations",
      "Register kernels in AOT build system for pre-compiled packages"
    ],
    "limitations": [
      "Requires CUDA toolkit and compatible NVIDIA GPU to test examples",
      "Tutorial demonstrates simple element-wise operation; complex kernels need additional patterns",
      "Does not cover kernel optimization techniques for maximum performance"
    ],
    "use_cases": [
      {
        "target_user": "GPU Kernel Developers",
        "title": "Add New Operations",
        "description": "Learn the complete workflow to integrate custom CUDA operations into FlashInfer library"
      },
      {
        "target_user": "FlashInfer Contributors",
        "title": "Extend FlashInfer",
        "description": "Add new attention variants, GEMM operations, or custom kernels following project conventions"
      },
      {
        "target_user": "ML Infrastructure Engineers",
        "title": "Custom GPU Operations",
        "description": "Implement specialized tensor operations for LLM serving pipelines"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Scale Kernel",
        "scenario": "Adding simple element-wise operation",
        "prompt": "Show me how to add a simple scale operation to FlashInfer that multiplies each tensor element by a scalar factor"
      },
      {
        "title": "Attention Kernel",
        "scenario": "Adding complex attention operation",
        "prompt": "How do I add a custom attention kernel to FlashInfer with multiple backends like CUTLASS and cuDNN"
      },
      {
        "title": "Architecture Targeting",
        "scenario": "Specifying GPU architectures",
        "prompt": "How do I specify supported CUDA architectures for my kernel and use CompilationContext for SM90 and SM100 targeting"
      },
      {
        "title": "Testing Pattern",
        "scenario": "Writing kernel tests",
        "prompt": "Show me the pattern for writing unit tests in FlashInfer with pytest parametrization for different dtypes and sizes"
      }
    ],
    "output_examples": [
      {
        "input": "How do I add a new CUDA kernel to FlashInfer?",
        "output": [
          "Add a CUDA kernel in 10 steps:",
          "1. Define kernel in include/flashinfer/{name}.cuh using raw pointers",
          "2. Create launcher in csrc/{name}.cu with TVM-FFI tensor handling",
          "3. Export via TVM_FFI_DLL_EXPORT_TYPED_FUNC in csrc/{name}_jit_binding.cu",
          "4. Create JIT generator in flashinfer/jit/{name}.py",
          "5. Build Python API in flashinfer/{name}.py with @functools.cache",
          "6. Write tests in tests/ directory with pytest.mark.parametrize",
          "7. Register in flashinfer/aot.py for pre-compiled packages",
          "8. Export from flashinfer/__init__.py",
          "9. Add benchmark in benchmarks/ directory",
          "10. Run pytest to test; kernel auto-compiles on first use"
        ]
      }
    ],
    "best_practices": [
      "Keep kernel code framework-agnostic by using raw pointers instead of Torch tensors in include/ directory",
      "Use @functools.cache for module caching and @flashinfer_api decorator for logging",
      "Specify supported CUDA architectures via supported_compute_capability and CompilationContext for proper GPU targeting"
    ],
    "anti_patterns": [
      "Including Torch headers in include/ directory kernels - keep them framework-agnostic",
      "Skipping validation in @backend_requirement decorator - always validate inputs",
      "Hardcoding CUDA architecture versions instead of using CompilationContext for flexibility"
    ],
    "faq": [
      {
        "question": "What GPU architectures does FlashInfer support?",
        "answer": "FlashInfer supports SM75 through SM121, including Turing, Ampere, Hopper, and Blackwell architectures with automatic detection."
      },
      {
        "question": "What is the minimum problem size for FlashInfer kernels?",
        "answer": "Problem sizes vary by kernel type; check individual kernel documentation for specific constraints on tensor dimensions."
      },
      {
        "question": "How do I integrate my kernel with existing FlashInfer APIs?",
        "answer": "Export your function from flashinfer/__init__.py and follow the naming conventions used by existing operations."
      },
      {
        "question": "Does this skill execute any code on my machine?",
        "answer": "No, this is a read-only tutorial skill containing documentation and example code for learning purposes only."
      },
      {
        "question": "Why is my kernel not compiling on my GPU?",
        "answer": "Check that your GPU compute capability matches the supported versions and that CUDA toolkit is properly installed."
      },
      {
        "question": "How does this compare to raw CUTLASS development?",
        "answer": "FlashInfer provides TVM-FFI bindings and caching, while CUTLASS is lower-level templates requiring more boilerplate."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
