{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T09:55:01.254Z",
    "slug": "aceek-llm-doc-writer",
    "source_url": "https://github.com/Aceek/claude-config/tree/master/src/skills/llm-doc-writer",
    "source_ref": "master",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "e880b1cc5b778efdf31398742a7c58baa13e43298dacf547690e639362e422ee",
    "tree_hash": "8052486b95da9520c72f9320deaabb9a9e4e59e909728e47feb5823fe88a4d5b"
  },
  "skill": {
    "name": "llm-doc-writer",
    "description": "Write token-efficient documentation for LLM context. Use when creating CLAUDE.md, README, technical docs, agent instructions, or any documentation consumed by AI assistants.",
    "summary": "Write token-efficient documentation for LLM context. Use when creating CLAUDE.md, README, technical ...",
    "icon": "üìù",
    "version": "1.0.0",
    "author": "Aceek",
    "license": "MIT",
    "category": "documentation",
    "tags": [
      "documentation",
      "llm-optimization",
      "writing",
      "claude",
      "context-efficiency"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure prompt-based documentation skill with no code execution, filesystem access, network calls, or system modifications. Contains only instructional markdown content with writing guidelines and examples.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 282,
    "audit_model": "claude",
    "audited_at": "2026-01-10T09:55:01.254Z"
  },
  "content": {
    "user_title": "Write LLM-Optimized Documentation",
    "value_statement": "LLMs consume documentation differently than humans. This skill provides proven patterns for writing documentation that maximizes information density while minimizing token usage. Transform verbose docs into compact, AI-friendly format.",
    "seo_keywords": [
      "llm documentation",
      "claude md writing",
      "context optimization",
      "token efficiency",
      "ai documentation",
      "technical writing",
      "CLAUDE.md guide",
      "prompt engineering",
      "documentation patterns",
      "Claude Code documentation"
    ],
    "actual_capabilities": [
      "Transform verbose documentation into token-efficient formats",
      "Apply table-based formatting over prose explanations",
      "Convert paragraphs into scannable bullet points",
      "Use code examples instead of written explanations",
      "Structure documentation with progressive disclosure",
      "Eliminate filler words and redundant patterns"
    ],
    "limitations": [
      "Does not automatically edit existing files",
      "Does not generate documentation from codebase analysis",
      "Does not integrate with documentation tools or generators",
      "Provides guidelines only - requires manual application"
    ],
    "use_cases": [
      {
        "target_user": "Developers",
        "title": "Create Efficient CLAUDE.md",
        "description": "Write compact project documentation that gives Claude Code clear context without token waste."
      },
      {
        "target_user": "Technical Writers",
        "title": "Optimize AI Documentation",
        "description": "Transform existing documentation into formats optimized for LLM consumption and training."
      },
      {
        "target_user": "AI Engineers",
        "title": "Design Agent Instructions",
        "description": "Create precise, dense instructions for AI agents and autonomous systems."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Transformation",
        "scenario": "Convert verbose docs",
        "prompt": "Transform this documentation into LLM-optimized format using tables over prose, bullets over paragraphs, and code examples over explanations. Remove filler words and redundant statements."
      },
      {
        "title": "CLAUDE.md Creation",
        "scenario": "Create project documentation",
        "prompt": "Write a CLAUDE.md for this project using the llm-doc-writer patterns. Include Stack, Commands table, Architecture summary, and key conventions. Keep it under 200 lines with progressive disclosure."
      },
      {
        "title": "API Documentation",
        "scenario": "Document endpoints",
        "prompt": "Create LLM-optimized API documentation. Use the pattern: endpoint line, request/response JSON examples, and status codes in a compact table. No prose explanations."
      },
      {
        "title": "Architecture Review",
        "scenario": "Document system design",
        "prompt": "Document this system architecture using the llm-doc-writer patterns. Use ASCII diagrams for structure, tables for component mapping, and concise bullet points for behavior. Avoid narrative descriptions."
      }
    ],
    "output_examples": [
      {
        "input": "Convert this to LLM-optimized format:\n\nThe project is a web application for managing tasks. It uses React for the frontend and Node.js with Express for the backend. PostgreSQL is used as the database. Users can create, read, update, and delete tasks. They can also organize tasks into categories and set due dates.",
        "output": [
          "# Task Manager",
          "## Stack",
          "| Layer | Tech |",
          "|-------|------|",
          "| Frontend | React |",
          "| Backend | Node/Express |",
          "| DB | PostgreSQL |",
          "## Features",
          "- CRUD tasks",
          "- Categories",
          "- Due dates"
        ]
      }
    ],
    "best_practices": [
      "Use tables for structured data - they compress better than prose in LLM context",
      "Show code examples instead of explaining - code is more precise and token-efficient",
      "Apply progressive disclosure - keep main docs under 500 lines with links to detailed files"
    ],
    "anti_patterns": [
      "Writing narrative paragraphs that explain what code already shows clearly",
      "Including meta-commentary like 'In this section we will discuss...'",
      "Using filler words like 'simply', 'basically', or 'please note that'"
    ],
    "faq": [
      {
        "question": "How is this different from regular documentation?",
        "answer": "LLMs process information differently than humans. Tables, bullets, and code examples are more token-efficient than prose while conveying the same information clearly."
      },
      {
        "question": "What is the ideal length for LLM documentation?",
        "answer": "Keep main files under 500 lines. Use progressive disclosure with separate detail files for deep dives."
      },
      {
        "question": "Does this work with all AI assistants?",
        "answer": "Yes. The patterns work with Claude, Codex, GPT, and any LLM. Dense formatting reduces token usage across all models."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "This skill only provides guidelines. No files are read, written, or transmitted. All documentation stays on your machine."
      },
      {
        "question": "How do I apply these patterns to existing docs?",
        "answer": "Use the skill to transform sections one at a time. Replace verbose paragraphs with tables, bullets, or code examples following the patterns in patterns.md."
      },
      {
        "question": "How does this compare to using longer, more detailed documentation?",
        "answer": "Longer docs can work but waste tokens on filler. Compact docs let LLMs focus on actual content. The goal is clarity per token, not word count."
      }
    ]
  },
  "file_structure": [
    {
      "name": "patterns.md",
      "type": "file",
      "path": "patterns.md"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
