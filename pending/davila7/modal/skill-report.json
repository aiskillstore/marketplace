{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T00:45:11.816Z",
    "slug": "davila7-modal",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/modal",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "7c642d70a45ce0ee11735c5eba2c77072915afd8537072edb4d6e3496a7c7eb7",
    "tree_hash": "2f488b9cc86a9c46e8b21345418fe38e088aa449dfd362e8258cd9316104a1e8"
  },
  "skill": {
    "name": "modal",
    "description": "Run Python code in the cloud with serverless containers, GPUs, and autoscaling. Use when deploying ML models, running batch processing jobs, scheduling compute-intensive tasks, or serving APIs that require GPU acceleration or dynamic scaling.",
    "summary": "Run Python code in the cloud with serverless containers, GPUs, and autoscaling. Use when deploying M...",
    "icon": "☁️",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "scientific",
    "tags": [
      "cloud-computing",
      "gpu",
      "machine-learning",
      "serverless",
      "python"
    ],
    "supported_tools": [
      "claude",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Documentation-only skill containing no executable code. All content is Markdown documentation explaining Modal platform features with example code blocks. No scripts, network calls, file access, or command execution capabilities.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 13,
    "total_lines": 2731,
    "audit_model": "claude",
    "audited_at": "2026-01-07T00:45:11.816Z"
  },
  "content": {
    "user_title": "Deploy Python to cloud with GPUs and autoscaling",
    "value_statement": "Running ML models and batch processing locally is slow and expensive. Modal lets you deploy Python functions to the cloud with automatic GPU access, containers that scale from zero to thousands, and pay-only-for-compute billing.",
    "seo_keywords": [
      "Modal cloud computing",
      "Claude Code Modal skill",
      "serverless Python GPU",
      "cloud ML deployment",
      "batch processing cloud",
      "Modal autoscaling",
      "Python serverless platform",
      "GPU computing Claude",
      "Modal cron scheduling",
      "Modal secrets management"
    ],
    "actual_capabilities": [
      "Deploy Python functions to serverless cloud containers with GPU support",
      "Scale automatically from zero to thousands of parallel containers",
      "Run scheduled cron jobs for periodic data processing and model retraining",
      "Persist data across runs using distributed volumes for model weights and datasets",
      "Securely manage API keys and credentials through Modal Secrets",
      "Serve HTTP endpoints and APIs with automatic HTTPS and custom domains"
    ],
    "limitations": [
      "Requires Modal account and authentication token for cloud access",
      "Cloud resources incur costs based on compute usage",
      "Limited to Python workloads; other languages require Docker images",
      "Cold starts may cause latency on first request after idle periods"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Deploy ML model inference",
        "description": "Serve transformer models and custom ML models with GPU acceleration and automatic scaling based on request volume"
      },
      {
        "target_user": "Data Scientists",
        "title": "Run batch data processing",
        "description": "Process large datasets in parallel across hundreds of containers without managing infrastructure"
      },
      {
        "target_user": "Researchers",
        "title": "Schedule compute jobs",
        "description": "Automate periodic tasks like model retraining, data pipeline execution, and report generation"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic model deployment",
        "scenario": "Deploy a Python ML model to Modal",
        "prompt": "Help me deploy a Hugging Face transformer model to Modal for inference. I want to use an L40S GPU and serve predictions via a web endpoint."
      },
      {
        "title": "Batch processing",
        "scenario": "Process large dataset in parallel",
        "prompt": "Create a Modal script that processes 1000 CSV files in parallel across multiple containers using .map(). Each file needs CPU and memory resources."
      },
      {
        "title": "Scheduled training",
        "scenario": "Automate model retraining",
        "prompt": "Set up a weekly scheduled job on Modal that retrains my model on new data every Sunday at 2 AM UTC. Include GPU access and checkpoint saving to a Volume."
      },
      {
        "title": "Web API with secrets",
        "scenario": "Create authenticated API endpoint",
        "prompt": "Build a Modal web endpoint that uses FastAPI, requires bearer token authentication, and accesses external APIs using Modal Secrets for credentials."
      }
    ],
    "output_examples": [
      {
        "input": "Deploy a sentiment analysis model to Modal with GPU",
        "output": [
          "✓ Create Modal App with custom image (debian_slim with torch and transformers)",
          "✓ Define GPU configuration (L40S for cost-effective inference)",
          "✓ Build class-based service with @modal.enter() for model loading",
          "✓ Create @modal.method() for inference calls",
          "✓ Add @modal.fastapi_endpoint() for HTTP access",
          "✓ Deploy with: modal deploy script.py"
        ]
      }
    ],
    "best_practices": [
      "Pin package versions in image definitions for reproducible builds",
      "Use Volumes to store model weights and avoid re-downloading on each run",
      "Configure min_containers to reduce cold start latency for production APIs",
      "Attach secrets only to functions that actually need them"
    ],
    "anti_patterns": [
      "Hardcoding API keys in code or image definitions instead of using Secrets",
      "Processing items one-by-one instead of using .map() for parallel execution",
      "Using the wrong GPU type (H100 for inference when L40S is more cost-effective)",
      "Forgetting to call volume.commit() after writing data that needs to persist"
    ],
    "faq": [
      {
        "question": "Is Modal free to use?",
        "answer": "Modal offers $30/month in free credits for new accounts. Pay only for compute used beyond that. Check modal.com/pricing for current rates."
      },
      {
        "question": "What GPU types are available?",
        "answer": "T4, L4, A10, A100 (40GB/80GB), L40S, H100, H200, and B200. L40S offers best value for inference; H100/A100 for training."
      },
      {
        "question": "How does Modal integrate with Claude Code?",
        "prompt": "Install the Modal skill, then write Python functions decorated with @app.function() to run code remotely on Modal infrastructure."
      },
      {
        "question": "Is my data safe on Modal?",
        "answer": "Modal encrypts data at rest and in transit. Secrets are encrypted and not logged. You control what data leaves your containers."
      },
      {
        "question": "Why am I getting timeout errors?",
        "answer": "Default function timeout is 5 minutes. Increase with @app.function(timeout=3600) for longer operations. Check your code for infinite loops."
      },
      {
        "question": "How does Modal compare to AWS Lambda?",
        "answer": "Modal offers better GPU support, longer timeouts (up to 24 hours), persistent storage via Volumes, and Python-native configuration without YAML."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "api_reference.md",
          "type": "file",
          "path": "references/api_reference.md"
        },
        {
          "name": "examples.md",
          "type": "file",
          "path": "references/examples.md"
        },
        {
          "name": "functions.md",
          "type": "file",
          "path": "references/functions.md"
        },
        {
          "name": "getting-started.md",
          "type": "file",
          "path": "references/getting-started.md"
        },
        {
          "name": "gpu.md",
          "type": "file",
          "path": "references/gpu.md"
        },
        {
          "name": "images.md",
          "type": "file",
          "path": "references/images.md"
        },
        {
          "name": "resources.md",
          "type": "file",
          "path": "references/resources.md"
        },
        {
          "name": "scaling.md",
          "type": "file",
          "path": "references/scaling.md"
        },
        {
          "name": "scheduled-jobs.md",
          "type": "file",
          "path": "references/scheduled-jobs.md"
        },
        {
          "name": "secrets.md",
          "type": "file",
          "path": "references/secrets.md"
        },
        {
          "name": "volumes.md",
          "type": "file",
          "path": "references/volumes.md"
        },
        {
          "name": "web-endpoints.md",
          "type": "file",
          "path": "references/web-endpoints.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
