{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T00:42:24.015Z",
    "slug": "davila7-scholar-evaluation",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/scholar-evaluation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "16b0c085efeb81feef686070ee6bc9feaa44acd9bfd33e095dd6ab1862babc89",
    "tree_hash": "cb9507658527763dcb917cb2386714fab4c20d6fb63434ebc6d271335a8963dd"
  },
  "skill": {
    "name": "Scholar Evaluation",
    "description": "Apply the ScholarEval framework to systematically evaluate scholarly and research work. This skill provides structured evaluation methodology based on peer-reviewed research assessment criteria, enabling comprehensive analysis of academic papers, research proposals, literature reviews, and scholarly writing across multiple quality dimensions.",
    "summary": "Apply the ScholarEval framework to systematically evaluate scholarly and research work. This skill p...",
    "icon": "ðŸŽ“",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "research",
    "tags": [
      "academic",
      "research",
      "evaluation",
      "peer-review",
      "scholarly"
    ],
    "supported_tools": [
      "claude",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill consists of documentation and a Python scoring script. The script performs file-based calculations on user-specified JSON inputs. No network access, no credential access, and no persistence mechanisms detected. The code is straightforward and its behavior matches the stated purpose of academic evaluation.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 1,
            "line_end": 379
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 47,
            "line_end": 68
          },
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 234,
            "line_end": 240
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 1333,
    "audit_model": "claude",
    "audited_at": "2026-01-07T00:42:24.015Z"
  },
  "content": {
    "user_title": "Evaluate research papers with ScholarEval",
    "value_statement": "Assessing scholarly work requires consistent criteria and structured methodology. This skill applies the ScholarEval framework to evaluate research papers, proposals, and literature reviews across eight quality dimensions with quantifiable scores.",
    "seo_keywords": [
      "ScholarEval",
      "research evaluation",
      "academic peer review",
      "paper assessment",
      "Claude Code skill",
      "scholarly writing evaluation",
      "research quality metrics",
      "academic writing assessment",
      "literature review evaluation",
      "research methodology review"
    ],
    "actual_capabilities": [
      "Evaluate research papers across 8 quality dimensions",
      "Calculate weighted aggregate scores from dimension ratings",
      "Generate structured evaluation reports with visualizations",
      "Provide actionable feedback prioritized by impact",
      "Assess publication readiness for target venues"
    ],
    "limitations": [
      "Does not access external databases or verify citations",
      "Does not replace domain expert peer review",
      "Score calculations require manual dimension scoring first"
    ],
    "use_cases": [
      {
        "target_user": "Researchers",
        "title": "Pre-submission Review",
        "description": "Evaluate your own papers before journal submission to identify weaknesses and improve quality."
      },
      {
        "target_user": "Peer Reviewers",
        "title": "Structured Reviews",
        "description": "Apply consistent evaluation criteria when reviewing papers for conferences or journals."
      },
      {
        "target_user": "Graduate Students",
        "title": "Thesis Feedback",
        "description": "Assess thesis chapters and proposals against established academic standards."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick Paper Review",
        "scenario": "Basic research evaluation",
        "prompt": "Use ScholarEval to evaluate this research paper on [topic]. Provide dimension scores and overall assessment."
      },
      {
        "title": "Methodology Focus",
        "scenario": "Targeted methodology assessment",
        "prompt": "Focus the ScholarEval evaluation on the methodology and research design of this paper. Assess rigor and reproducibility."
      },
      {
        "title": "Publication Readiness",
        "scenario": "Submission preparation",
        "prompt": "Conduct a full ScholarEval assessment to determine if this paper is ready for submission to [journal/conference]. Identify gaps."
      },
      {
        "title": "Comparative Analysis",
        "scenario": "Benchmarking multiple papers",
        "prompt": "Compare these two research papers using ScholarEval. Score each across all dimensions and highlight relative strengths."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate this machine learning research paper for publication readiness",
        "output": [
          "Overall Score: 4.2/5.00 - Strong, publication-ready with minor revisions",
          "Top Strengths: Methodology (4.5), Writing (4.5), Problem Formulation (4.3)",
          "Areas for Improvement: Literature Review (3.7), Data Collection (3.8)",
          "Recommendation: Address citation diversity in literature review before submission"
        ]
      }
    ],
    "best_practices": [
      "Score dimensions independently before calculating overall assessment",
      "Document specific evidence from the work for each dimension score",
      "Adjust expectations based on publication venue and work stage"
    ],
    "anti_patterns": [
      "Skipping dimensions because they seem less important",
      "Allowing overall impression to bias individual dimension scores",
      "Providing feedback without specific examples or evidence"
    ],
    "faq": [
      {
        "question": "Which AI tools support this skill?",
        "answer": "Compatible with Claude and Claude Code. Not currently available for Codex."
      },
      {
        "question": "What is the scoring scale?",
        "answer": "Uses a 5-point scale: 5=Excellent, 4=Good, 3=Adequate, 2=Needs Improvement, 1=Poor."
      },
      {
        "question": "Can I customize dimension weights?",
        "answer": "Yes. Create a custom weights JSON file and pass it to the calculate_scores.py script."
      },
      {
        "question": "Is my data stored or shared?",
        "answer": "No. The skill processes data locally and does not transmit information externally."
      },
      {
        "question": "Why are my scores different each time?",
        "answer": "Ensure you score each dimension consistently. The scoring is subjective and depends on the evidence you identify."
      },
      {
        "question": "How does this compare to human peer review?",
        "answer": "ScholarEval provides structured, consistent assessment. It complements but does not replace expert human judgment."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "evaluation_framework.md",
          "type": "file",
          "path": "references/evaluation_framework.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "calculate_scores.py",
          "type": "file",
          "path": "scripts/calculate_scores.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
