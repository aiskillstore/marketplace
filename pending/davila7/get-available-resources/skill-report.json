{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:27:48.853Z",
    "slug": "davila7-get-available-resources",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/get-available-resources",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "e51562f2070cd450dd4a67cd2a55995f6596458714a9d9a01c7f008a21de6a7f",
    "tree_hash": "ad89478b48a70f0d9654f22045be65a22f81f532d969c74f75560e3f214fa646"
  },
  "skill": {
    "name": "get-available-resources",
    "description": "This skill should be used at the start of any computationally intensive scientific task to detect and report available system resources (CPU cores, GPUs, memory, disk space). It creates a JSON file with resource information and strategic recommendations that inform computational approach decisions such as whether to use parallel processing (joblib, multiprocessing), out-of-core computing (Dask, Zarr), GPU acceleration (PyTorch, JAX), or memory-efficient strategies. Use this skill before running analyses, training models, processing large datasets, or any task where resource constraints matter.",
    "summary": "This skill should be used at the start of any computationally intensive scientific task to detect an...",
    "icon": "ðŸ”¬",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "data",
    "tags": [
      "scientific-computing",
      "resource-detection",
      "gpu",
      "optimization"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate system resource detection script. The code behavior matches its stated purpose exactly. All subprocess calls are to standard system utilities (nvidia-smi, rocm-smi, system_profiler) for hardware detection. No network calls, no data exfiltration, no persistence mechanisms, no credential access.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 674,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:27:48.853Z"
  },
  "content": {
    "user_title": "Detect system resources for scientific computing",
    "value_statement": "Scientific computing tasks require appropriate hardware resources. This skill automatically detects CPU cores, GPU availability, memory, and disk space to recommend optimal computational strategies.",
    "seo_keywords": [
      "Claude Code skill",
      "scientific computing",
      "GPU detection",
      "resource monitoring",
      "parallel processing",
      "CUDA",
      "Metal",
      "ROCm",
      "system resources",
      "data science tools"
    ],
    "actual_capabilities": [
      "Detect CPU cores, architecture, and frequency",
      "Identify NVIDIA GPUs via nvidia-smi with VRAM and compute capability",
      "Detect AMD GPUs via rocm-smi",
      "Detect Apple Silicon GPUs (M1-M4) via system_profiler",
      "Measure RAM and swap memory availability",
      "Check disk space and usage for working directory",
      "Generate recommendations for parallel processing, memory strategy, and GPU acceleration"
    ],
    "limitations": [
      "Requires psutil Python package installation",
      "GPU detection depends on installed driver utilities (nvidia-smi, rocm-smi)",
      "Memory readings are snapshots that change over time",
      "Does not monitor resources continuously, only reports at execution time"
    ],
    "use_cases": [
      {
        "target_user": "Data scientists",
        "title": "Analyze large datasets",
        "description": "Determine if datasets fit in memory or require Dask, Zarr, or out-of-core processing"
      },
      {
        "target_user": "ML engineers",
        "title": "Train neural networks",
        "description": "Check GPU availability and select appropriate backend (CUDA, Metal, ROCm) for PyTorch or TensorFlow"
      },
      {
        "target_user": "Researchers",
        "title": "Run simulations",
        "description": "Identify optimal worker count for parallel processing with joblib or multiprocessing"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic resource check",
        "scenario": "Quick system scan",
        "prompt": "Use the get-available-resources skill to detect available CPU, GPU, memory, and disk resources"
      },
      {
        "title": "Dataset planning",
        "scenario": "Before large data work",
        "prompt": "Run get-available-resources and recommend whether I should use Dask, Zarr, or in-memory processing for a 20GB dataset"
      },
      {
        "title": "GPU optimization",
        "scenario": "ML workload setup",
        "prompt": "Use get-available-resources to check GPU availability and suggest optimal libraries for my hardware"
      },
      {
        "title": "Parallel scaling",
        "scenario": "Performance tuning",
        "prompt": "Run resource detection and determine the optimal number of parallel workers for processing 10,000 files"
      }
    ],
    "output_examples": [
      {
        "input": "Detect available system resources",
        "output": [
          "CPU: 8 cores (8 physical, arm64 architecture)",
          "Memory: 16 GB total, 8.5 GB available",
          "GPU: Apple M2 detected with Metal backend",
          "Recommendation: Use high parallelism with 6 workers",
          "Recommendation: GPU acceleration available via PyTorch-MPS"
        ]
      }
    ],
    "best_practices": [
      "Run resource detection at the start of each project session",
      "Re-run before scaling up parallel workers or data sizes",
      "Save the .claude_resources.json file in project directories for documentation"
    ],
    "anti_patterns": [
      "Running resource detection once and ignoring changing resource availability",
      "Assuming GPU availability without checking (nvidia-smi may not be installed)",
      "Using all available cores for parallel processing without leaving headroom for system operations"
    ],
    "faq": [
      {
        "question": "Which platforms are supported?",
        "answer": "Full support for macOS (including Apple Silicon), Linux (NVIDIA and AMD GPUs), and Windows (NVIDIA GPUs)."
      },
      {
        "question": "What Python packages are required?",
        "answer": "Only psutil is required. All other functionality uses Python standard library modules."
      },
      {
        "question": "How accurate are memory readings?",
        "answer": "Memory readings are snapshots at execution time. Available memory changes constantly as processes run."
      },
      {
        "question": "Can I use this skill in CI/CD pipelines?",
        "answer": "Yes, but GPU detection will fail without GPU utilities installed. The script handles missing tools gracefully."
      },
      {
        "question": "How do I detect multiple GPUs?",
        "answer": "The script queries nvidia-smi for all GPUs and reports VRAM, compute capability, and driver version for each."
      },
      {
        "question": "What is the difference between physical and logical cores?",
        "answer": "Physical cores are actual CPU cores. Logical cores (hyper-threading) allow multiple threads per core for improved throughput."
      }
    ]
  },
  "file_structure": [
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "detect_resources.py",
          "type": "file",
          "path": "scripts/detect_resources.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
