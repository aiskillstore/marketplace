{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T00:55:22.465Z",
    "slug": "davila7-shap",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/shap",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "734d6fc71b29f3fa44f7432131164c1f65965b274a09c6a6368f62646f1a1bcb",
    "tree_hash": "3036b529aae0f5affc6a0e4f49db95c31620c18b160770cfaae9d831bfc7fc3d"
  },
  "skill": {
    "name": "shap",
    "description": "Model interpretability and explainability using SHAP (SHapley Additive exPlanations). Use this skill when explaining machine learning model predictions, computing feature importance, generating SHAP plots (waterfall, beeswarm, bar, scatter, force, heatmap), debugging models, analyzing model bias or fairness, comparing models, or implementing explainable AI. Works with tree-based models (XGBoost, LightGBM, Random Forest), deep learning (TensorFlow, PyTorch), linear models, and any black-box model.",
    "summary": "Model interpretability and explainability using SHAP (SHapley Additive exPlanations). Use this skill...",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "data",
    "tags": [
      "machine-learning",
      "model-interpretation",
      "explainable-ai",
      "feature-importance"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill with no code execution, network calls, file access, or external dependencies. Contains only markdown guidance for using the SHAP library for model interpretability.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 2465,
    "audit_model": "claude",
    "audited_at": "2026-01-07T00:55:22.465Z"
  },
  "content": {
    "user_title": "Explain model predictions with SHAP",
    "value_statement": "Machine learning models often work as black boxes. SHAP provides a unified framework to understand which features drive predictions and how much each feature contributes. This skill helps you compute feature importance, generate visualizations, debug models, and implement explainable AI in your projects.",
    "seo_keywords": [
      "SHAP explainability",
      "SHAP machine learning",
      "model interpretability",
      "feature importance",
      "XGBoost explainer",
      "SHAP plots",
      "explainable AI",
      "Claude Code",
      "model debugging",
      "SHAP values"
    ],
    "actual_capabilities": [
      "Select appropriate SHAP explainer based on model type (TreeExplainer, DeepExplainer, KernelExplainer, LinearExplainer)",
      "Generate SHAP visualizations including waterfall, beeswarm, bar, scatter, force, and heatmap plots",
      "Debug machine learning models by identifying prediction errors and unexpected feature importance",
      "Analyze model fairness and bias across demographic groups using feature importance comparison",
      "Implement production-ready explanation services with cached explainers and API endpoints"
    ],
    "limitations": [
      "KernelExplainer can be slow for models with many features due to exponential complexity",
      "Does not include the SHAP library itself - requires installation via pip or uv",
      "SHAP measures association, not causation - explanations require domain knowledge for causal interpretation"
    ],
    "use_cases": [
      {
        "target_user": "Data Scientists",
        "title": "Debug model predictions",
        "description": "Identify why your model makes specific predictions by visualizing feature contributions for individual samples."
      },
      {
        "target_user": "ML Engineers",
        "title": "Deploy explainable AI",
        "description": "Integrate SHAP explanations into production APIs with cached explainers and efficient batch processing."
      },
      {
        "target_user": "Domain Experts",
        "title": "Analyze model fairness",
        "description": "Detect bias by comparing feature importance across demographic groups to ensure fair predictions."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic feature importance",
        "scenario": "Understanding model drivers",
        "prompt": "Generate SHAP values and a beeswarm plot to show which features are most important for my [XGBoost/LightGBM/Random Forest] model trained on [describe your dataset]."
      },
      {
        "title": "Individual prediction",
        "scenario": "Explaining specific decisions",
        "prompt": "Explain why my model predicted [specific outcome] for a single sample. Show me a waterfall plot with the feature contributions for sample index [number]."
      },
      {
        "title": "Model comparison",
        "scenario": "Comparing model behavior",
        "prompt": "Compare feature importance between my [Model A] and [Model B]. Generate bar plots showing which features each model relies on most and how their explanations differ."
      },
      {
        "title": "Fairness analysis",
        "scenario": "Detecting bias",
        "prompt": "Analyze my model for potential bias across [protected attribute like gender/age/race]. Compare SHAP values for [Group A] and [Group B] to see if certain groups are treated differently."
      }
    ],
    "output_examples": [
      {
        "input": "Explain the top 5 most important features for my XGBoost loan approval model and show me why one specific application was rejected.",
        "output": [
          "Top 5 features by mean |SHAP|: Credit Score, Income, Loan Amount, Employment Length, Debt-to-Income Ratio",
          "For this rejected application:",
          "  - Credit Score (580) reduced approval probability by 0.25 (below median)",
          "  - Income ($35,000) reduced probability by 0.18",
          "  - High Debt-to-Income (0.45) further reduced by 0.12",
          "  - Loan Amount ($25,000) had slight negative impact of 0.05",
          "  - Employment Length (2 years) marginally positive (+0.03)"
        ]
      }
    ],
    "best_practices": [
      "Use TreeExplainer for tree-based models (XGBoost, LightGBM, Random Forest) - it is exact and much faster than KernelExplainer",
      "Start with global visualizations (beeswarm) to understand overall patterns, then drill into individual predictions with waterfall plots",
      "Validate SHAP explanations against domain knowledge - SHAP shows model behavior, not causation"
    ],
    "anti_patterns": [
      "Using KernelExplainer for tree-based models when TreeExplainer is available (slower and unnecessary)",
      "Interpreting log-odds as probabilities for classifier models without checking model output type",
      "Assuming high feature importance indicates causation rather than model-learned associations"
    ],
    "faq": [
      {
        "question": "Which SHAP explainer should I use for my model?",
        "answer": "TreeExplainer for tree models (XGBoost, LightGBM, Random Forest), DeepExplainer for neural networks, LinearExplainer for linear models, KernelExplainer as fallback for any model type."
      },
      {
        "question": "How many background samples do I need for KernelExplainer?",
        "answer": "Use 50-1000 representative samples from your training data. More samples improve accuracy but increase computation time."
      },
      {
        "question": "Can I use SHAP with PyTorch or TensorFlow models?",
        "answer": "Yes, use DeepExplainer for TensorFlow/PyTorch models or GradientExplainer for expected gradients approximation."
      },
      {
        "question": "Is my data safe when using SHAP?",
        "answer": "SHAP computations happen locally on your machine. The skill contains only documentation - no data is sent anywhere."
      },
      {
        "question": "Why are my XGBoost SHAP values in log-odds instead of probabilities?",
        "answer": "XGBoost classifiers explain margin output by default. Use model_output='probability' in TreeExplainer to get probability-scale values."
      },
      {
        "question": "How does SHAP compare to LIME or permutation importance?",
        "answer": "SHAP provides both local and global explanations with theoretical guarantees. LIME approximates SHAP but without consistency. Permutation importance only provides global importance without instance-level explanations."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "explainers.md",
          "type": "file",
          "path": "references/explainers.md"
        },
        {
          "name": "plots.md",
          "type": "file",
          "path": "references/plots.md"
        },
        {
          "name": "theory.md",
          "type": "file",
          "path": "references/theory.md"
        },
        {
          "name": "workflows.md",
          "type": "file",
          "path": "references/workflows.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
