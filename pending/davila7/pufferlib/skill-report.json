{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:12:18.785Z",
    "slug": "davila7-pufferlib",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/pufferlib",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "8550a4a276dd5c9f007377bfaa2659eb853ca67cc5b68db783323b006d8fa490",
    "tree_hash": "a292334434619b1958411c185919e16ead5c75ce20de27627a04a84acf089fd8"
  },
  "skill": {
    "name": "pufferlib",
    "description": "This skill should be used when working with reinforcement learning tasks including high-performance RL training, custom environment development, vectorized parallel simulation, multi-agent systems, or integration with existing RL environments (Gymnasium, PettingZoo, Atari, Procgen, etc.). Use this skill for implementing PPO training, creating PufferEnv environments, optimizing RL performance, or developing policies with CNNs/LSTMs.",
    "summary": "This skill should be used when working with reinforcement learning tasks including high-performance ...",
    "icon": "ðŸ§ ",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "scientific",
    "tags": [
      "reinforcement-learning",
      "machine-learning",
      "pytorch",
      "parallel-simulation"
    ],
    "supported_tools": [
      "claude",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Legitimate reinforcement learning library documentation and training templates. Contains standard ML code with no malicious patterns. Risk factors are limited to standard filesystem operations for model checkpoints and external command references for distributed training setup.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/train_template.py",
            "line_start": 1,
            "line_end": 240
          },
          {
            "file": "scripts/env_template.py",
            "line_start": 1,
            "line_end": 341
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/train_template.py",
            "line_start": 231,
            "line_end": 232
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "references/training.md",
            "line_start": 108,
            "line_end": 134
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 2400,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:12:18.785Z"
  },
  "content": {
    "user_title": "Train reinforcement learning agents with PufferLib",
    "value_statement": "Training RL agents requires managing complex parallel simulations and implementing efficient PPO algorithms. PufferLib provides high-performance vectorized environments and optimized training to achieve millions of steps per second.",
    "seo_keywords": [
      "PufferLib",
      "reinforcement learning",
      "PPO training",
      "Claude Code",
      "PyTorch",
      "vectorized environments",
      "multi-agent RL",
      "Gymnasium",
      "PettingZoo"
    ],
    "actual_capabilities": [
      "Create vectorized parallel environments for millions of steps per second",
      "Implement PPO training with LSTM policies using PuffeRL algorithm",
      "Build custom PufferEnv environments with single-agent and multi-agent support",
      "Integrate with Gymnasium, PettingZoo, Atari, Procgen, and other RL frameworks",
      "Develop CNN, LSTM, and multi-input neural network policies",
      "Run distributed multi-GPU training with torchrun"
    ],
    "limitations": [
      "Requires PyTorch and CUDA-capable GPU for optimal performance",
      "Python-based environments limited to 100k-500k steps per second without C optimization",
      "Custom environments must follow PufferEnv API for vectorization compatibility"
    ],
    "use_cases": [
      {
        "target_user": "ML researchers",
        "title": "Fast RL experimentation",
        "description": "Prototype and test reinforcement learning algorithms with high-throughput parallel environments"
      },
      {
        "target_user": "Game AI developers",
        "title": "Train game-playing agents",
        "description": "Develop agents for Atari, Procgen, and custom game environments using PPO"
      },
      {
        "target_user": "Robotics engineers",
        "title": "Sim-to-real policy transfer",
        "description": "Train policies in vectorized simulators and deploy to physical systems"
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick environment test",
        "scenario": "Test an existing RL environment",
        "prompt": "Use PufferLib to create a vectorized environment for 'procgen-coinrun' with 256 parallel environments and run a quick evaluation to measure steps per second"
      },
      {
        "title": "Custom environment",
        "scenario": "Create a custom PufferEnv",
        "prompt": "Help me create a custom PufferEnv environment for a grid-world navigation task with discrete actions and vector observations"
      },
      {
        "title": "Train with PPO",
        "scenario": "Set up PPO training loop",
        "prompt": "Write a complete training script using PuffeRL with a CNN policy for image observations, including checkpoint saving and Weights & Biases logging"
      },
      {
        "title": "Multi-agent training",
        "scenario": "Train collaborative agents",
        "prompt": "Create a multi-agent training setup using PettingZoo environment with shared policy across agents, configured for 128 parallel environments"
      }
    ],
    "output_examples": [
      {
        "input": "Create a vectorized CartPole environment and train a policy",
        "output": [
          "âœ“ Created 256 parallel CartPole environments",
          "âœ“ Policy: MLP with 256 hidden units, ReLU activations",
          "âœ“ Training at 1.2M steps/second on GPU",
          "âœ“ Episode 100: Mean reward = 485.2 (Â±12.3)",
          "âœ“ Checkpoint saved: checkpoints/checkpoint_100.pt"
        ]
      }
    ],
    "best_practices": [
      "Start with smaller num_envs (64-128) for debugging, then scale to 256+ for production training",
      "Use torch.compile with mode='reduce-overhead' for 10-20% faster training on PyTorch 2.0+",
      "Profile environment performance before training to identify bottlenecks with shared memory buffers"
    ],
    "anti_patterns": [
      "Avoid excessive wrapper nesting as each wrapper adds overhead to environment step calls",
      "Do not allocate new arrays inside environment step methods - use pre-allocated buffers for zero-copy operations",
      "Avoid running training without checkpointing - resume capability is critical for long-running experiments"
    ],
    "faq": [
      {
        "question": "Which Python and PyTorch versions are supported?",
        "answer": "Python 3.8+ and PyTorch 1.13+ recommended. PyTorch 2.0+ enables torch.compile for faster training."
      },
      {
        "question": "What is the maximum throughput achievable?",
        "answer": "Pure Python environments: 100k-500k SPS. C-based environments: 100M+ SPS. Training with Python env: ~1-4M total SPS."
      },
      {
        "question": "How does PufferLib integrate with existing Gymnasium environments?",
        "answer": "Use pufferlib.emulate() to wrap any Gymnasium environment or pufferlib.make('gym-EnvName') for direct access."
      },
      {
        "question": "Is my training data safe?",
        "answer": "PufferLib runs entirely locally. Only wandb/neptune loggers optionally send metrics to external services when configured."
      },
      {
        "question": "Why is my training slow on CPU?",
        "answer": "RL training requires GPU for neural network inference. Use device='cuda' and ensure CUDA is available. Consider reducing num_envs if CPU-bound."
      },
      {
        "question": "How does PufferLib compare to Stable-Baselines3?",
        "answer": "PufferLib prioritizes throughput (1M+ SPS) over simplicity. SB3 is easier to use but slower. PufferLib suits research requiring rapid experimentation."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "environments.md",
          "type": "file",
          "path": "references/environments.md"
        },
        {
          "name": "integration.md",
          "type": "file",
          "path": "references/integration.md"
        },
        {
          "name": "policies.md",
          "type": "file",
          "path": "references/policies.md"
        },
        {
          "name": "training.md",
          "type": "file",
          "path": "references/training.md"
        },
        {
          "name": "vectorization.md",
          "type": "file",
          "path": "references/vectorization.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "env_template.py",
          "type": "file",
          "path": "scripts/env_template.py"
        },
        {
          "name": "train_template.py",
          "type": "file",
          "path": "scripts/train_template.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
