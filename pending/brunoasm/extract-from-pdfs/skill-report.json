{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-11T13:11:01.983Z",
    "slug": "brunoasm-extract-from-pdfs",
    "source_url": "https://github.com/brunoasm/my_claude_skills/tree/main/extract_from_pdfs",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "f74b9831acc3bf57beb71698b7fc1d6722be4f0a11a861471efcce9a43c5082a",
    "tree_hash": "8aa3eabe115ffecb0e0a4eb044dafa6f33114320e50868f623e1c9bfe957ea4d"
  },
  "skill": {
    "name": "extract-from-pdfs",
    "description": "This skill should be used when extracting structured data from scientific PDFs for systematic reviews, meta-analyses, or database creation. Use when working with collections of research papers that need to be converted into analyzable datasets with validation metrics.",
    "summary": "This skill should be used when extracting structured data from scientific PDFs for systematic review...",
    "icon": "ðŸ“„",
    "version": "1.0.0",
    "author": "brunoasm",
    "license": "MIT",
    "category": "research",
    "tags": [
      "data extraction",
      "pdf processing",
      "scientific research",
      "systematic review",
      " Claude"
    ],
    "supported_tools": [
      "claude",
      "claude-code"
    ],
    "risk_factors": [
      "filesystem",
      "network",
      "env_access"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate scientific research tool for PDF data extraction. All 481 static findings are false positives: JSON Schema syntax misidentified as cryptographic algorithms, shell command examples in documentation flagged as execution patterns, and standard API credential handling flagged as credential access. The skill combines file reading, network requests, and API calls as normal functionality for research data processing.",
    "risk_factor_evidence": [
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/03_extract_from_pdfs.py",
            "line_start": 106,
            "line_end": 106
          },
          {
            "file": "scripts/06_export_database.py",
            "line_start": 139,
            "line_end": 149
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "scripts/05_validate_with_apis.py",
            "line_start": 74,
            "line_end": 77
          }
        ]
      },
      {
        "factor": "env_access",
        "evidence": [
          {
            "file": "scripts/03_extract_from_pdfs.py",
            "line_start": 372,
            "line_end": 373
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 20,
    "total_lines": 5302,
    "audit_model": "claude",
    "audited_at": "2026-01-11T13:11:01.983Z"
  },
  "content": {
    "user_title": "Extract Structured Data from Scientific PDFs",
    "value_statement": "Convert scientific PDF collections into analyzable datasets. Automate extraction of structured data with Claude vision, validate against scientific databases, and export to analysis tools.",
    "seo_keywords": [
      "Claude PDF extraction",
      "scientific data extraction",
      "systematic review tool",
      "PDF to database",
      "Claude Code skill",
      "meta-analysis data collection",
      "research paper extraction",
      "Claude vision PDF"
    ],
    "actual_capabilities": [
      "Organize metadata from BibTeX, RIS, DOI lists, or PDF directories",
      "Filter papers by abstract using Claude Haiku, Sonnet, or local Ollama models",
      "Extract structured data from PDFs using Claude vision capabilities",
      "Validate and enrich data against scientific databases (GBIF, GeoNames, PubChem, NCBI)",
      "Calculate precision and recall metrics for quality assurance",
      "Export to Python, R, CSV, Excel, or SQLite formats"
    ],
    "limitations": [
      "Maximum PDF file size is 32MB per document",
      "Requires API keys for Claude and external validation services",
      "Does not perform full-text search or PDF editing"
    ],
    "use_cases": [
      {
        "target_user": "Academic Researchers",
        "title": "Systematic Literature Reviews",
        "description": "Extract structured data from hundreds of papers for meta-analyses with validated quality metrics."
      },
      {
        "target_user": "Data Scientists",
        "title": "Research Dataset Creation",
        "description": "Convert PDF literature collections into analysis-ready datasets for statistical processing in Python or R."
      },
      {
        "target_user": "Domain Experts",
        "title": "Field-Specific Data Mining",
        "description": "Design custom extraction schemas for biology, chemistry, medicine, or other scientific domains."
      }
    ],
    "prompt_templates": [
      {
        "title": "Start Extraction Project",
        "scenario": "Begin a new PDF extraction workflow",
        "prompt": "I want to extract structured data from scientific PDFs. Help me set up the extraction pipeline for my research on [TOPIC]. I have [NUMBER] PDFs organized in [LOCATION]."
      },
      {
        "title": "Create Custom Schema",
        "scenario": "Design extraction schema for specific data",
        "prompt": "Create a custom extraction schema for [DATA TYPE, e.g., plant species observations] from scientific papers. Include fields for [SPECIFIC FIELDS YOU NEED]."
      },
      {
        "title": "Run Quality Validation",
        "scenario": "Calculate extraction accuracy metrics",
        "prompt": "I have manually annotated [NUMBER] papers with ground truth. Help me run the validation script to calculate precision, recall, and F1 scores for my extraction results."
      },
      {
        "title": "Enrich with External Data",
        "scenario": "Validate against scientific databases",
        "prompt": "Configure API validation to cross-reference extracted [TAXONOMIC NAMES/LOCATIONS/COMPOUNDS] with [GBIF/GeoNames/PubChem] databases. Show me the configuration template."
      }
    ],
    "output_examples": [
      {
        "input": "Extract species observations from my ecology papers",
        "output": [
          "Extracted 47 records from 23 papers",
          "Field coverage: 94% for species names, 88% for locations, 76% for measurements",
          "Validation: 12 species matched in GBIF taxonomy database",
          "Next steps: Review low-coverage fields and consider schema adjustments"
        ]
      }
    ],
    "best_practices": [
      "Start with 5-10 example PDFs to design and refine your extraction schema before full processing",
      "Use abstract filtering to reduce costs when working with large paper collections",
      "Validate extraction quality on a sample of 20+ papers before processing the entire dataset"
    ],
    "anti_patterns": [
      "Skipping the validation step - always measure precision and recall to identify weak fields",
      "Using complex schemas initially - start simple and add fields iteratively",
      "Processing all papers without filtering - use abstract screening to reduce costs"
    ],
    "faq": [
      {
        "question": "What API keys do I need?",
        "answer": "ANTHROPIC_API_KEY is required. Optional keys: GEONAMES_USERNAME for geographic validation."
      },
      {
        "question": "Can I use this offline?",
        "answer": "Partially. Ollama supports local filtering, but PDF extraction requires Claude API."
      },
      {
        "question": "How many PDFs can I process?",
        "answer": "Unlimited. Typical cost is $6-9 for 100 papers with Sonnet model."
      },
      {
        "question": "What output formats are supported?",
        "answer": "Python (pandas), R (RDS), CSV, Excel, JSON, and SQLite database."
      },
      {
        "question": "How accurate is the extraction?",
        "answer": "Accuracy depends on schema design. Validation metrics help identify and improve weak fields."
      },
      {
        "question": "Can I customize what fields are extracted?",
        "answer": "Yes. Modify the JSON schema to define exactly what data fields you need from each paper."
      }
    ]
  },
  "file_structure": [
    {
      "name": "assets",
      "type": "dir",
      "path": "assets",
      "children": [
        {
          "name": "api_config_template.json",
          "type": "file",
          "path": "assets/api_config_template.json"
        },
        {
          "name": "example_api_config_ecology.json",
          "type": "file",
          "path": "assets/example_api_config_ecology.json"
        },
        {
          "name": "example_flower_visitors_schema.json",
          "type": "file",
          "path": "assets/example_flower_visitors_schema.json"
        },
        {
          "name": "schema_template.json",
          "type": "file",
          "path": "assets/schema_template.json"
        }
      ]
    },
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "api_reference.md",
          "type": "file",
          "path": "references/api_reference.md"
        },
        {
          "name": "setup_guide.md",
          "type": "file",
          "path": "references/setup_guide.md"
        },
        {
          "name": "validation_guide.md",
          "type": "file",
          "path": "references/validation_guide.md"
        },
        {
          "name": "workflow_guide.md",
          "type": "file",
          "path": "references/workflow_guide.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "01_organize_metadata.py",
          "type": "file",
          "path": "scripts/01_organize_metadata.py"
        },
        {
          "name": "02_filter_abstracts.py",
          "type": "file",
          "path": "scripts/02_filter_abstracts.py"
        },
        {
          "name": "03_extract_from_pdfs.py",
          "type": "file",
          "path": "scripts/03_extract_from_pdfs.py"
        },
        {
          "name": "04_repair_json.py",
          "type": "file",
          "path": "scripts/04_repair_json.py"
        },
        {
          "name": "05_validate_with_apis.py",
          "type": "file",
          "path": "scripts/05_validate_with_apis.py"
        },
        {
          "name": "06_export_database.py",
          "type": "file",
          "path": "scripts/06_export_database.py"
        },
        {
          "name": "07_prepare_validation_set.py",
          "type": "file",
          "path": "scripts/07_prepare_validation_set.py"
        },
        {
          "name": "08_calculate_validation_metrics.py",
          "type": "file",
          "path": "scripts/08_calculate_validation_metrics.py"
        }
      ]
    },
    {
      "name": "environment.yml",
      "type": "file",
      "path": "environment.yml"
    },
    {
      "name": "README.md",
      "type": "file",
      "path": "README.md"
    },
    {
      "name": "requirements.txt",
      "type": "file",
      "path": "requirements.txt"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
