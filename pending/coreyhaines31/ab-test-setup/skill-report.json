{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-19T22:25:02.633Z",
    "slug": "coreyhaines31-ab-test-setup",
    "source_url": "https://github.com/coreyhaines31/marketingskills/tree/main/skills/ab-test-setup",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "de20523eb1bbeb9e98e8e099b2516976fe7c798941f4893a7598b40d73abdf6d",
    "tree_hash": "a401f7135ff6b9385642ec766de107e070aee2501be0f14a1e4b5f6c459cfd53"
  },
  "skill": {
    "name": "ab-test-setup",
    "description": "When the user wants to plan, design, or implement an A/B test or experiment. Also use when the user mentions \"A/B test,\" \"split test,\" \"experiment,\" \"test this change,\" \"variant copy,\" \"multivariate test,\" or \"hypothesis.\" For tracking implementation, see analytics-tracking.",
    "summary": "Design statistically valid A/B tests with clear hypotheses, proper sample sizing, and actionable results.",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "coreyhaines31",
    "license": "MIT",
    "tags": [
      "experimentation",
      "conversion-optimization",
      "marketing",
      "data-analysis",
      "hypothesis-testing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All static findings are false positives. The scanner triggered on markdown code block delimiters (backticks) which are documentation templates, not shell commands. URLs to testing calculators are legitimate documentation references. Statistical formulas and marketing metrics were misidentified as cryptographic content and reconnaissance. This is a benign A/B testing documentation skill with no security concerns.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 59,
            "line_end": 65
          },
          {
            "file": "SKILL.md",
            "line_start": 134,
            "line_end": 138
          },
          {
            "file": "SKILL.md",
            "line_start": 222,
            "line_end": 231
          },
          {
            "file": "SKILL.md",
            "line_start": 393,
            "line_end": 417
          },
          {
            "file": "SKILL.md",
            "line_start": 432,
            "line_end": 460
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 129,
            "line_end": 130
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 509,
    "audit_model": "claude",
    "audited_at": "2026-01-19T22:25:02.633Z",
    "risk_factors": []
  },
  "content": {
    "user_title": "Design Effective A/B Tests",
    "value_statement": "A/B testing without proper planning leads to inconclusive results and wasted resources. This skill provides a structured framework for designing statistically valid experiments that produce actionable insights for conversion optimization.",
    "seo_keywords": [
      "A/B testing",
      "split testing",
      "experimentation",
      "conversion optimization",
      "hypothesis testing",
      "statistical significance",
      "sample size calculation",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Guide users through defining clear, testable hypotheses using a structured framework",
      "Calculate required sample sizes based on baseline conversion rates and minimum detectable effects",
      "Help select appropriate metrics including primary, secondary, and guardrail metrics",
      "Design effective test variants with single, meaningful changes",
      "Document test plans and results for organizational learning",
      "Analyze results and recommend data-driven decisions"
    ],
    "limitations": [
      "Does not execute tests or access external testing tools directly",
      "Cannot access real user data or current conversion rates without user input",
      "Does not perform statistical calculations or p-value computations",
      "Cannot integrate with specific A/B testing platforms like Optimizely or PostHog"
    ],
    "use_cases": [
      {
        "title": "Planning a New Conversion Experiment",
        "description": "A marketer wants to test a new headline on their landing page. The skill helps structure a hypothesis, calculate sample size requirements, define success metrics, and plan the test duration before implementation.",
        "target_user": "Marketing professionals, conversion rate optimizers"
      },
      {
        "title": "Validating Product Change Decisions",
        "description": "A product manager needs to determine if a proposed UI change will improve user engagement. The skill guides them through designing a proper A/B test with statistical rigor to ensure valid results.",
        "target_user": "Product managers, UX researchers"
      },
      {
        "title": "Analyzing Past Test Results",
        "description": "An analyst has completed an A/B test and needs to interpret the results. The skill provides a framework for evaluating statistical significance, practical impact, and making data-driven recommendations.",
        "target_user": "Data analysts, growth teams"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic A/B Test Request",
        "prompt": "Help me design an A/B test for [element] on [page]. We want to test [proposed change] because [observation or data]. Our current conversion rate is [rate] with [traffic] visitors per week.",
        "scenario": "Initial test planning with basic context provided"
      },
      {
        "title": "Full Test Planning",
        "prompt": "I need to design a complete A/B test. Here's what I know:\n- What we want to improve: [goal]\n- Proposed change: [description]\n- Current conversion rate: [rate]\n- Weekly traffic: [number]\n- Smallest improvement worth detecting: [MDE]\n- Available testing tool: [tool name]\n\nHelp me create a hypothesis, calculate sample size, define metrics, and plan the test duration.",
        "scenario": "Comprehensive test planning with detailed inputs"
      },
      {
        "title": "Hypothesis Review",
        "prompt": "Review my A/B test hypothesis and suggest improvements:\n\n[Hypothesis draft]\n\nIs this hypothesis specific and testable? What could make it stronger? What metrics should we use to measure success?",
        "scenario": "Refining and improving hypothesis statements"
      },
      {
        "title": "Results Interpretation",
        "prompt": "We ran an A/B test with the following results:\n- Sample size: [number] per variant\n- Control conversion rate: [rate]\n- Variant conversion rate: [rate]\n- Confidence level: [percentage]\n- Duration: [time period]\n\nHelp me interpret these results. Is the difference statistically significant? Is it practically meaningful? What should we decide?",
        "scenario": "Analyzing and interpreting test results"
      }
    ],
    "output_examples": [
      {
        "input": "Help me design an A/B test for the signup button on our pricing page. We want to test making the button larger and changing the color to orange. Our current signup rate is 5% with 10,000 visitors per week.",
        "output": "## A/B Test Plan\n\n### Hypothesis\nBecause users may not notice the current signup button (per [observation needed]), we believe making the button larger with a contrasting orange color will increase signup rate by 10%+ for pricing page visitors. We'll measure this by tracking click-through rate from pricing page to signup completion.\n\n### Test Design\n- Type: A/B Test\n- Duration: ~3 weeks\n- Sample size: ~7,000 visitors per variant\n- Traffic allocation: 50/50\n\n### Variants\n- Control: Current button (size, color, copy)\n- Variant: Larger button with orange color\n\n### Metrics\n- Primary: Signup completion rate\n- Secondary: Button click-through rate, time to click\n- Guardrail: Bounce rate, page load time\n\n### Next Steps\n1. Calculate exact sample size using Evan Miller's calculator\n2. Create mockups for both variants\n3. Implement in your testing tool\n4. QA both variants before launch"
      },
      {
        "input": "We ran a headline test for 2 weeks. Control (original headline) got 3.2% conversion on 5,000 visitors. Variant B (new headline) got 3.8% conversion on 5,000 visitors. Is this significant?",
        "output": "## Results Analysis\n\n### Summary\n- Control: 3.2% conversion (160 conversions)\n- Variant: 3.8% conversion (190 conversions)\n- Relative lift: +18.8%\n\n### Statistical Assessment\nWith 5,000 visitors per variant and a 0.6 percentage point difference, this result is **likely statistically significant** at the 95% confidence level.\n\n### Recommendation\n**Implement the variant** with the new headline. The 18.8% relative improvement is both statistically significant and practically meaningful for your business.\n\n### Considerations\n- Consider running a full business cycle (2+ weeks) to account for day-of-week effects\n- Check if the lift is consistent across mobile and desktop\n- Monitor downstream metrics like activation rate"
      }
    ],
    "best_practices": [
      "Always start with a clear, specific hypothesis that includes the expected outcome and how you will measure it",
      "Pre-calculate sample size before starting and commit to running the full test duration",
      "Test one variable at a time to isolate what actually caused any observed effect"
    ],
    "anti_patterns": [
      "Avoid making changes to variants mid-test based on early results",
      "Do not stop tests early just because results look promising",
      "Avoid testing too many changes at once as it becomes impossible to identify what worked"
    ],
    "faq": [
      {
        "question": "What is the minimum sample size for an A/B test?",
        "answer": "Sample size depends on your baseline conversion rate and the minimum effect you want to detect. For a 5% baseline with 20% lift, you need about 7,000 visitors per variant. Smaller baselines or smaller effects require much more traffic. Use online calculators like Evan Miller's to determine your specific needs."
      },
      {
        "question": "How long should I run an A/B test?",
        "answer": "Run tests for at least 1-2 full business cycles (1-2 weeks minimum) to account for day-of-week variation. Calculate duration based on your sample size needs. Avoid running tests too long as novelty effects and external factors can confound results."
      },
      {
        "question": "What makes a good A/B test hypothesis?",
        "answer": "A good hypothesis follows the format: Because [observation], we believe [change] will cause [outcome] for [audience]. We will know this is true when [metric]. It should be specific, testable, and include a clear success metric."
      },
      {
        "question": "When should I use multivariate testing instead of A/B testing?",
        "answer": "Use multivariate testing only when you have significant traffic (typically 100,000+ visitors per month) and want to test interactions between multiple variables. A/B testing is simpler, requires less traffic, and provides clearer results for most optimization efforts."
      },
      {
        "question": "What is statistical significance in A/B testing?",
        "answer": "Statistical significance indicates the probability that the difference between variants is not due to random chance. 95% significance means there is less than 5% chance the result is random. It does not guarantee the result is practically meaningful for your business."
      },
      {
        "question": "How do I choose metrics for my A/B test?",
        "answer": "Select one primary metric directly tied to your hypothesis. Add secondary metrics to understand how the change affects behavior. Include guardrail metrics to ensure you do not harm important outcomes like revenue or user satisfaction."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 509
    }
  ]
}
