{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T07:01:57.172Z",
    "slug": "muratcankoylan-project-development",
    "source_url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering/tree/main/skills/project-development",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "f6625c6d5f973f44eea8fbc948e8f007a2196094da28173c89b6ac36158a31cb",
    "tree_hash": "2b87c35f91a098b22e1ef0597ff58486ca2a4b7146197a897eb82c5c8da03271"
  },
  "skill": {
    "name": "project-development",
    "description": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"evaluate task-model fit\", \"structure agent project\", or mentions pipeline architecture, agent-assisted development, cost estimation, or choosing between LLM and traditional approaches.",
    "summary": "This skill should be used when the user asks to \"start an LLM project\", \"design batch pipeline\", \"ev...",
    "icon": "ðŸš€",
    "version": "1.0.0",
    "author": "muratcankoylan",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "llm",
      "pipeline",
      "architecture",
      "batch-processing",
      "agent-development"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "external_commands",
      "network",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a documentation and template skill for LLM project development methodology. All static findings are false positives: markdown files contain code examples for educational purposes, the Python script is a template requiring user customization for legitimate batch processing, and no actual credentials or malicious patterns exist. A previous AI audit also concluded 'safe' with no findings.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "references/case-studies.md",
            "line_start": 53,
            "line_end": 55
          },
          {
            "file": "references/case-studies.md",
            "line_start": 55,
            "line_end": 61
          },
          {
            "file": "references/case-studies.md",
            "line_start": 61,
            "line_end": 61
          },
          {
            "file": "references/case-studies.md",
            "line_start": 61,
            "line_end": 61
          },
          {
            "file": "references/case-studies.md",
            "line_start": 61,
            "line_end": 67
          },
          {
            "file": "references/case-studies.md",
            "line_start": 67,
            "line_end": 72
          },
          {
            "file": "references/case-studies.md",
            "line_start": 72,
            "line_end": 78
          },
          {
            "file": "references/case-studies.md",
            "line_start": 78,
            "line_end": 78
          },
          {
            "file": "references/case-studies.md",
            "line_start": 78,
            "line_end": 84
          },
          {
            "file": "references/case-studies.md",
            "line_start": 84,
            "line_end": 84
          },
          {
            "file": "references/case-studies.md",
            "line_start": 84,
            "line_end": 90
          },
          {
            "file": "references/case-studies.md",
            "line_start": 90,
            "line_end": 104
          },
          {
            "file": "references/case-studies.md",
            "line_start": 104,
            "line_end": 116
          },
          {
            "file": "references/case-studies.md",
            "line_start": 116,
            "line_end": 124
          },
          {
            "file": "references/case-studies.md",
            "line_start": 124,
            "line_end": 183
          },
          {
            "file": "references/case-studies.md",
            "line_start": 183,
            "line_end": 191
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 7,
            "line_end": 9
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 9,
            "line_end": 27
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 27,
            "line_end": 42
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 42,
            "line_end": 46
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 46,
            "line_end": 60
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 60,
            "line_end": 64
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 64,
            "line_end": 81
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 81,
            "line_end": 87
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 87,
            "line_end": 106
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 106,
            "line_end": 116
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 116,
            "line_end": 136
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 136,
            "line_end": 142
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 142,
            "line_end": 172
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 172,
            "line_end": 178
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 178,
            "line_end": 187
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 187,
            "line_end": 191
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 191,
            "line_end": 198
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 198,
            "line_end": 202
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 202,
            "line_end": 212
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 212,
            "line_end": 216
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 216,
            "line_end": 230
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 230,
            "line_end": 234
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 234,
            "line_end": 263
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 263,
            "line_end": 269
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 269,
            "line_end": 290
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 290,
            "line_end": 294
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 294,
            "line_end": 311
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 311,
            "line_end": 315
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 315,
            "line_end": 333
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 333,
            "line_end": 339
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 339,
            "line_end": 361
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 361,
            "line_end": 365
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 365,
            "line_end": 399
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 399,
            "line_end": 405
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 405,
            "line_end": 463
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 463,
            "line_end": 469
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 469,
            "line_end": 487
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 487,
            "line_end": 491
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 491,
            "line_end": 507
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 507,
            "line_end": 513
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 513,
            "line_end": 544
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 544,
            "line_end": 550
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 550,
            "line_end": 581
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 581,
            "line_end": 585
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 585,
            "line_end": 609
          },
          {
            "file": "SKILL.md",
            "line_start": 72,
            "line_end": 74
          },
          {
            "file": "SKILL.md",
            "line_start": 74,
            "line_end": 88
          },
          {
            "file": "SKILL.md",
            "line_start": 88,
            "line_end": 94
          },
          {
            "file": "SKILL.md",
            "line_start": 94,
            "line_end": 116
          },
          {
            "file": "SKILL.md",
            "line_start": 116,
            "line_end": 130
          },
          {
            "file": "SKILL.md",
            "line_start": 130,
            "line_end": 158
          },
          {
            "file": "SKILL.md",
            "line_start": 158,
            "line_end": 160
          },
          {
            "file": "SKILL.md",
            "line_start": 160,
            "line_end": 190
          },
          {
            "file": "SKILL.md",
            "line_start": 190,
            "line_end": 210
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "references/case-studies.md",
            "line_start": 7,
            "line_end": 7
          },
          {
            "file": "references/case-studies.md",
            "line_start": 142,
            "line_end": 142
          },
          {
            "file": "skill-report.json",
            "line_start": 150,
            "line_end": 150
          },
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          },
          {
            "file": "SKILL.md",
            "line_start": 329,
            "line_end": 329
          },
          {
            "file": "SKILL.md",
            "line_start": 330,
            "line_end": 330
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 309,
            "line_end": 309
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 485,
            "line_end": 485
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 500,
            "line_end": 500
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 505,
            "line_end": 505
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 529,
            "line_end": 529
          },
          {
            "file": "references/pipeline-patterns.md",
            "line_start": 608,
            "line_end": 608
          },
          {
            "file": "scripts/pipeline_template.py",
            "line_start": 137,
            "line_end": 137
          },
          {
            "file": "scripts/pipeline_template.py",
            "line_start": 195,
            "line_end": 195
          },
          {
            "file": "scripts/pipeline_template.py",
            "line_start": 250,
            "line_end": 250
          },
          {
            "file": "scripts/pipeline_template.py",
            "line_start": 335,
            "line_end": 335
          },
          {
            "file": "scripts/pipeline_template.py",
            "line_start": 348,
            "line_end": 348
          },
          {
            "file": "scripts/pipeline_template.py",
            "line_start": 456,
            "line_end": 456
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 2228,
    "audit_model": "claude",
    "audited_at": "2026-01-17T07:01:57.172Z"
  },
  "content": {
    "user_title": "Design LLM Projects with Proven Architecture Patterns",
    "value_statement": "Building LLM applications without proper architecture leads to wasted time and resources. This skill provides battle-tested patterns from production systems like Karpathy's HN Time Capsule and Vercel's d0 agent.",
    "seo_keywords": [
      "LLM project architecture",
      "batch processing pipeline",
      "Claude Code",
      "agent development",
      "task-model fit",
      "cost estimation",
      "multi-agent patterns",
      "structured outputs",
      "Codex",
      "Claude"
    ],
    "actual_capabilities": [
      "Evaluates if tasks are suitable for LLM processing vs traditional code",
      "Designs 5-stage pipelines: acquire â†’ prepare â†’ process â†’ parse â†’ render",
      "Estimates token costs and processing time before development",
      "Provides templates for structured LLM outputs with parsing",
      "Includes case studies from Karpathy, Vercel, Manus, and Anthropic"
    ],
    "limitations": [
      "Template requires customization for specific use cases",
      "No built-in LLM API integrations - users implement their own",
      "Focuses on batch processing, not real-time applications"
    ],
    "use_cases": [
      {
        "target_user": "Data scientists",
        "title": "Batch Content Analysis",
        "description": "Build systems to analyze thousands of documents, reviews, or social media posts with consistent quality and cost tracking."
      },
      {
        "target_user": "AI engineers",
        "title": "Multi-Agent Research Systems",
        "description": "Design architectures where multiple agents explore different aspects of complex problems in parallel."
      },
      {
        "target_user": "Product managers",
        "title": "LLM Project Planning",
        "description": "Evaluate whether AI features will deliver value before investing in development resources."
      }
    ],
    "prompt_templates": [
      {
        "title": "Task-Model Fit Evaluation",
        "scenario": "Determine if a task benefits from LLM processing",
        "prompt": "I want to [describe your task]. Evaluate if this is well-suited for LLM processing using the characteristics table. Consider: synthesis needs, subjective judgment requirements, error tolerance, and batch processing potential."
      },
      {
        "title": "Pipeline Architecture Design",
        "scenario": "Structure a batch processing system",
        "prompt": "Design a 5-stage pipeline for [describe your data source and goal]. Specify what happens in acquire, prepare, process, parse, and render stages. Include file naming conventions and parallelization approach."
      },
      {
        "title": "Cost Estimation",
        "scenario": "Budget LLM processing expenses",
        "prompt": "Estimate costs for processing [X] items through [model name]. Each item has approximately [Y] tokens of input and expects [Z] tokens output. Include 30% buffer for retries and API overhead."
      },
      {
        "title": "Structured Output Design",
        "scenario": "Create parseable LLM outputs",
        "prompt": "Design a prompt template that produces structured output for [your task]. Include section markers, format examples, and parsing instructions. Specify exact format because output will be parsed programmatically."
      }
    ],
    "output_examples": [
      {
        "input": "Help me build a system to analyze 1000 customer reviews for sentiment and key themes",
        "output": [
          "Task-model fit confirmed: Reviews need synthesis, subjective judgment, and tolerate errors",
          "Pipeline design: acquire (fetch reviews) â†’ prepare (format prompts) â†’ process (sentiment analysis) â†’ parse (extract scores/themes) â†’ render (dashboard)",
          "Cost estimate: ~$15 for GPT-4 processing (1000 reviews Ã— 500 tokens avg Ã— $0.03/1K tokens)",
          "Architecture: File-based state management with parallel workers for LLM calls",
          "Next steps: Manual prototype with 5 reviews, then build pipeline template"
        ]
      }
    ],
    "best_practices": [
      "Always validate task-model fit with manual prototyping before building automation",
      "Use file system for state management - each stage completion marked by file existence",
      "Design prompts for structured outputs with explicit format examples and parsing rationale",
      "Start with minimal architecture and add complexity only when proven necessary"
    ],
    "anti_patterns": [
      "Skipping manual validation and building automation for tasks models cannot perform",
      "Creating monolithic pipelines without discrete, debuggable stages",
      "Expecting perfect LLM output formatting without building robust parsers",
      "Ignoring cost estimation until production when token expenses compound quickly"
    ],
    "faq": [
      {
        "question": "Which LLM models work best with this approach?",
        "answer": "The methodology works with any model. GPT-4, Claude, and Opus excel at following structured output formats. Smaller models may need more constrained prompts."
      },
      {
        "question": "How do I handle API rate limits?",
        "answer": "Use the ThreadPoolExecutor pattern with 5-15 workers depending on your API tier. Implement exponential backoff for retries and consider batching requests."
      },
      {
        "question": "Can this handle real-time processing?",
        "answer": "This skill focuses on batch processing. For real-time needs, consider caching pre-computed results or using streaming APIs with different architecture patterns."
      },
      {
        "question": "What if my data requires preprocessing?",
        "answer": "Add preprocessing substages within the prepare stage. Clean data, extract relevant sections, or convert formats before generating prompts."
      },
      {
        "question": "How do I debug parsing failures?",
        "answer": "Log both successful and failed parses with the raw LLM output. Build unit tests for your parsers using real examples, handling format variations gracefully."
      },
      {
        "question": "When should I use multi-agent vs single pipeline?",
        "answer": "Use multi-agent when tasks exceed context windows, need parallel exploration, or benefit from specialized sub-agents. Single pipelines work for independent batch items."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "case-studies.md",
          "type": "file",
          "path": "references/case-studies.md",
          "lines": 389
        },
        {
          "name": "pipeline-patterns.md",
          "type": "file",
          "path": "references/pipeline-patterns.md",
          "lines": 611
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "pipeline_template.py",
          "type": "file",
          "path": "scripts/pipeline_template.py",
          "lines": 678
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 343
    }
  ]
}
