{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T18:40:37.599Z",
    "slug": "pytorch-at-dispatch-v2",
    "source_url": "https://github.com/pytorch/pytorch/tree/main/.claude/skills/at-dispatch-v2",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "441e54058d3446fdc95d8b015f03b028e09ecd619f5431a38079177b50f14528",
    "tree_hash": "1f6b928fe10467f765afe5e6bb3c8e9c750354856330918f33a3871b0900ca47"
  },
  "skill": {
    "name": "at-dispatch-v2",
    "description": "Convert PyTorch AT_DISPATCH macros to AT_DISPATCH_V2 format in ATen C++ code. Use when porting AT_DISPATCH_ALL_TYPES_AND*, AT_DISPATCH_FLOATING_TYPES*, or other dispatch macros to the new v2 API. For ATen kernel files, CUDA kernels, and native operator implementations.",
    "summary": "Convert PyTorch AT_DISPATCH macros to AT_DISPATCH_V2 format in ATen C++ code.",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "pytorch",
    "license": "MIT",
    "tags": [
      "PyTorch",
      "C++",
      "ATen",
      "kernel",
      "CUDA"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a documentation-only skill that provides conversion rules for PyTorch C++ dispatch macros. The static analyzer flagged markdown documentation syntax (backticks) and metadata fields as potential security risks, but all findings are false positives. No executable code, no network access, no external commands are present. The skill consists of a single SKILL.md file containing documentation examples.",
    "risk_factor_evidence": [
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 1,
            "line_end": 306
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 1195,
    "audit_model": "claude",
    "audited_at": "2026-01-21T18:40:37.599Z"
  },
  "content": {
    "user_title": "Convert AT_DISPATCH macros to v2",
    "value_statement": "Porting legacy PyTorch AT_DISPATCH macros to the new v2 API requires careful argument reordering and type group handling. This skill provides step-by-step transformation rules and examples for converting ATen dispatch macros to AT_DISPATCH_V2 format.",
    "seo_keywords": [
      "PyTorch",
      "AT_DISPATCH",
      "AT_DISPATCH_V2",
      "ATen",
      "kernel development",
      "CUDA",
      "dispatch macro",
      "type dispatch",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Identify legacy AT_DISPATCH_* macros in C++ code",
      "Convert AT_DISPATCH_* macros to AT_DISPATCH_V2 format",
      "Reorder macro arguments correctly (scalar_type, name, lambda, types)",
      "Apply AT_WRAP and AT_EXPAND macros appropriately",
      "Map type group macros to their v2 equivalents",
      "Handle complex cases including multi-line lambdas and multiple types"
    ],
    "limitations": [
      "Does not compile or test converted code",
      "Does not modify files without explicit user request",
      "Does not handle custom dispatch patterns outside documented examples",
      "Does not provide runtime type checking or validation"
    ],
    "use_cases": [
      {
        "title": "ATen Kernel Porting",
        "description": "Port existing ATen kernel files from legacy AT_DISPATCH macros to the new AT_DISPATCH_V2 API for consistency with modern PyTorch dispatch patterns.",
        "target_user": "PyTorch kernel developers maintaining ATen native operators"
      },
      {
        "title": "CUDA Kernel Conversion",
        "description": "Convert dispatch macros in CUDA kernel implementations to use the v2 format, enabling better composable type sets and cleaner argument ordering.",
        "target_user": "GPU kernel engineers working on CUDA backends"
      },
      {
        "title": "Legacy Code Modernization",
        "description": "Modernize older PyTorch operator implementations by updating dispatch macro usage to the v2 API for improved maintainability.",
        "target_user": "Framework maintainers updating legacy codebases"
      }
    ],
    "prompt_templates": [
      {
        "title": "Simple Macro Conversion",
        "prompt": "Convert this AT_DISPATCH macro to AT_DISPATCH_V2 format:\n\n```cpp\nAT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, \"op\", [&]() {\n  kernel<scalar_t>(data);\n});\n```",
        "scenario": "Converting a basic AT_DISPATCH_ALL_TYPES_AND2 macro"
      },
      {
        "title": "Floating Types with Multiple Additional Types",
        "prompt": "Convert this AT_DISPATCH_FLOATING_TYPES_AND3 macro to AT_DISPATCH_V2:\n\n```cpp\nAT_DISPATCH_FLOATING_TYPES_AND3(kHalf, kBFloat16, kFloat8_e4m3fn,\n    tensor.scalar_type(), \"float_op\", [&] {\n  process<scalar_t>(tensor);\n});\n```",
        "scenario": "Converting floating types with three additional type arguments"
      },
      {
        "title": "Complex Types with Multiple Groups",
        "prompt": "Convert this complex dispatch macro to AT_DISPATCH_V2:\n\n```cpp\nAT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(\n    kComplexHalf, kHalf,\n    self.scalar_type(),\n    \"complex_op\",\n    [&] {\n      result = compute<scalar_t>(self);\n    }\n);\n```",
        "scenario": "Converting dispatch with both ALL_TYPES and COMPLEX_TYPES groups"
      },
      {
        "title": "Full File Porting",
        "prompt": "Read the file at [FILE_PATH] and convert all AT_DISPATCH_* macros to AT_DISPATCH_V2 format. Add the #include <ATen/Dispatch_v2.h> header if not present. Show me each change made.",
        "scenario": "Porting an entire file containing multiple dispatch macros"
      }
    ],
    "output_examples": [
      {
        "input": "Convert this macro:\n```cpp\nAT_DISPATCH_ALL_TYPES(dtype, \"op\", [&]() { kernel<scalar_t>(); });\n```",
        "output": "```cpp\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES));\n```\n\nChanges made:\n1. Added AT_DISPATCH_V2 wrapper with reordered arguments\n2. Wrapped lambda in AT_WRAP() for comma handling\n3. Added AT_EXPAND(AT_ALL_TYPES) for type group"
      },
      {
        "input": "Convert this macro with multiple additional types:\n```cpp\nAT_DISPATCH_FLOATING_TYPES_AND4(kHalf, kBFloat16, kFloat8_e4m3fn, kFloat8_e5m2,\n    dtype, \"float8_op\", [&]() { kernel<scalar_t>(); });\n```",
        "output": "```cpp\nAT_DISPATCH_V2(dtype, \"float8_op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_FLOATING_TYPES), kHalf, kBFloat16, kFloat8_e4m3fn, kFloat8_e5m2);\n```\n\nChanges made:\n1. Reordered: scalar_type first, then name\n2. Wrapped lambda in AT_WRAP()\n3. Added AT_EXPAND(AT_FLOATING_TYPES) for type group\n4. Listed individual types at the end"
      }
    ],
    "best_practices": [
      "Always wrap lambdas in AT_WRAP() to handle internal commas correctly",
      "Use AT_EXPAND() for type groups (AT_ALL_TYPES, AT_FLOATING_TYPES) but not for individual types",
      "Verify the conversion by checking that scalar_type, name, lambda, and types are in the correct order",
      "Add #include <ATen/Dispatch_v2.h> when converting files that do not already include it"
    ],
    "anti_patterns": [
      "Forgetting to wrap the lambda in AT_WRAP() - this causes parsing errors with multi-expression lambdas",
      "Using AT_EXPAND() on individual types like kHalf or kBFloat16 instead of just listing them",
      "Keeping the old argument order instead of reordering to scalar_type, name, lambda, types",
      "Not adding the Dispatch_v2.h include when converting files"
    ],
    "faq": [
      {
        "question": "What is the difference between AT_DISPATCH and AT_DISPATCH_V2?",
        "answer": "AT_DISPATCH_V2 is the new dispatch API in PyTorch that removes arity from macro names. Instead of AT_DISPATCH_ALL_TYPES_AND2, AND3, etc., you use AT_DISPATCH_V2 with AT_EXPAND() for type groups and individual types as trailing arguments."
      },
      {
        "question": "Why do I need AT_WRAP() around the lambda?",
        "answer": "AT_WRAP() is required to handle commas inside the lambda body. Without it, the preprocessor cannot correctly parse the lambda as a single argument to the macro."
      },
      {
        "question": "What are AT_EXPAND and AT_WRAP macros?",
        "answer": "AT_EXPAND() expands type group macros like AT_ALL_TYPES into their constituent types. AT_WRAP() wraps the lambda to protect it from comma parsing issues. Both are defined in Dispatch_v2.h."
      },
      {
        "question": "Do I need to remove the old Dispatch.h include?",
        "answer": "No. Keep #include <ATen/Dispatch.h> as other code in the file may still need it. Only add #include <ATen/Dispatch_v2.h> for the new v2 macros."
      },
      {
        "question": "How do I handle lambdas with multiple statements?",
        "answer": "Place the entire lambda body inside AT_WRAP(). The lambda can contain multiple statements, commas, and complex expressions as long as it is fully wrapped."
      },
      {
        "question": "What happens if I have more than 4 additional types (AND5, AND6)?",
        "answer": "AT_DISPATCH_V2 handles any number of additional types. Simply list them all as trailing arguments after the type groups. There is no hard limit like with the old AND2, AND3 naming scheme."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 306
    }
  ]
}
