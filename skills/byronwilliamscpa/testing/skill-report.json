{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T20:49:17.291Z",
    "slug": "byronwilliamscpa-testing",
    "source_url": "https://github.com/ByronWilliamsCPA/fragrance-rater/tree/main/.claude/skills/testing",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "48e619062eabc86c65d9fe2b209ea6d187423e98dc1a2e7941b8bb8e1436aa76",
    "tree_hash": "08decd621d5ec21c1d3921d6d948969b4fa751e2c391d874547bf47ac11c884f"
  },
  "skill": {
    "name": "testing",
    "description": "Automated test generation, review, and execution for pytest-based projects.",
    "summary": "Automated test generation, review, and execution for pytest-based projects.",
    "icon": "ðŸ§ª",
    "version": "1.0.0",
    "author": "ByronWilliamsCPA",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "testing",
      "pytest",
      "coverage",
      "test-automation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation skill containing only markdown files with testing guidance and example commands. All static findings are false positives triggered by: (1) SHA256 hash strings being misinterpreted as C2 indicators, (2) standard pytest command examples in documentation being flagged as shell execution, and (3) code examples in markdown being scanned as if they were executable. The skill has no executable code, no network capabilities, no filesystem access, and no external command execution - it only provides testing guidance through prompts.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 34,
            "line_end": 54
          },
          {
            "file": "SKILL.md",
            "line_start": 54,
            "line_end": 64
          },
          {
            "file": "SKILL.md",
            "line_start": 64,
            "line_end": 72
          },
          {
            "file": "SKILL.md",
            "line_start": 72,
            "line_end": 77
          },
          {
            "file": "SKILL.md",
            "line_start": 77,
            "line_end": 87
          },
          {
            "file": "SKILL.md",
            "line_start": 87,
            "line_end": 90
          },
          {
            "file": "SKILL.md",
            "line_start": 90,
            "line_end": 97
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 176,
    "audit_model": "claude",
    "audited_at": "2026-01-16T20:49:17.291Z"
  },
  "content": {
    "user_title": "Generate and Review pytest Tests",
    "value_statement": "Writing tests manually is time-consuming and error-prone. This skill automates test generation, review, and execution for pytest projects with built-in coverage standards.",
    "seo_keywords": [
      "testing skill",
      "pytest",
      "test generation",
      "code coverage",
      "Claude Code",
      "test automation",
      "unit testing",
      "integration testing",
      "e2e testing",
      "test review"
    ],
    "actual_capabilities": [
      "Generate unit and integration tests for Python code",
      "Review existing tests for quality and coverage gaps",
      "Execute pytest with coverage reporting",
      "Run mutation testing to verify test effectiveness",
      "Apply AAA pattern and fixture-based testing",
      "Organize tests by category (unit, integration, e2e, security)"
    ],
    "limitations": [
      "Limited to pytest-based Python projects",
      "Cannot execute tests outside the project directory",
      "Does not modify production code directly",
      "Coverage enforcement is informational only"
    ],
    "use_cases": [
      {
        "target_user": "Python Developers",
        "title": "Automate Test Creation",
        "description": "Generate comprehensive test suites from existing code automatically"
      },
      {
        "target_user": "QA Engineers",
        "title": "Review Test Coverage",
        "description": "Analyze test quality and identify gaps in existing test suites"
      },
      {
        "target_user": "DevOps Teams",
        "title": "Run CI Test Suites",
        "description": "Execute tests with coverage reports for continuous integration"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Test Request",
        "scenario": "Generate unit tests",
        "prompt": "Generate unit tests for the function [function_name] in [file_path] using the AAA pattern"
      },
      {
        "title": "Coverage Analysis",
        "scenario": "Check test coverage",
        "prompt": "Run coverage analysis on [module_name] and identify untested branches"
      },
      {
        "title": "Integration Tests",
        "scenario": "Create integration tests",
        "prompt": "Create integration tests for [feature_name] including edge cases and error handling"
      },
      {
        "title": "Test Review",
        "scenario": "Review test quality",
        "prompt": "Review tests in [test_file] for coverage, mocking quality, and assertion completeness"
      }
    ],
    "output_examples": [
      {
        "input": "Generate unit tests for the calculate_total function in src/utils/financial.py",
        "output": [
          "Generated 5 test cases covering normal inputs, edge cases, and error conditions",
          "Applied AAA pattern with proper fixture setup",
          "Coverage: 100% on calculate_total function",
          "Included property-based tests using Hypothesis"
        ]
      },
      {
        "input": "Run coverage analysis on the authentication module",
        "output": [
          "Overall coverage: 78% (below 80% threshold)",
          "Missing branch coverage in login function at lines 45-52",
          "Recommend adding tests for invalid token scenarios",
          "Report generated: htmlcov/index.html"
        ]
      },
      {
        "input": "Review the existing test suite in tests/api/",
        "output": [
          "Found 45 tests with 92% coverage",
          "Good use of fixtures for test data setup",
          "Consider adding mocking for external API calls",
          "Some tests depend on execution order - recommend fixes"
        ]
      }
    ],
    "best_practices": [
      "Write tests before code (TDD) for better design and confidence",
      "Aim for meaningful coverage, not just high percentages",
      "Use fixtures to share setup code across multiple tests"
    ],
    "anti_patterns": [
      "Avoid testing implementation details instead of observable behavior",
      "Do not skip tests without documenting the reason",
      "Avoid hardcoded values in assertions - use variables instead"
    ],
    "faq": [
      {
        "question": "What test frameworks does this skill support?",
        "answer": "This skill focuses on pytest but concepts apply to unittest and other Python testing frameworks."
      },
      {
        "question": "What is the minimum coverage requirement?",
        "answer": "The skill enforces 80% minimum coverage with branch coverage enabled for all projects."
      },
      {
        "question": "Can this skill integrate with CI/CD pipelines?",
        "answer": "Yes. The skill generates pytest commands compatible with GitHub Actions, GitLab CI, and other CI systems."
      },
      {
        "question": "Is my test data safe?",
        "answer": "Yes. All testing happens locally in your project. No test data is sent to external services."
      },
      {
        "question": "Why are my tests failing after generation?",
        "answer": "Generated tests may need adjustment for complex dependencies. Review the test and add any missing mocks or fixtures."
      },
      {
        "question": "How is this different from other testing tools?",
        "answer": "This skill uses Claude Code's context awareness to generate contextually appropriate tests for your specific codebase."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 98
    }
  ]
}
