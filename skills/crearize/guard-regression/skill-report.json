{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T16:32:19.088Z",
    "slug": "crearize-guard-regression",
    "source_url": "https://github.com/Crearize/ProjectTemplate/tree/main/.claude/skills/guard-regression",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "96460d3a1cc73c05cd6ba989546acd5e356c48a596d7f044a863047171f6f44b",
    "tree_hash": "ee666e20b0b3659897c538db74759fe8b500baa88f2769b21ecfa64b715b2cfa"
  },
  "skill": {
    "name": "guard-regression",
    "description": "Monitors code quality during refactoring by comparing test results, build outcomes, and performance metrics before and after changes. Triggers rollback when regressions are detected.",
    "summary": "Quality regression monitoring for safe refactoring",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "Crearize",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "refactoring",
      "quality-assurance",
      "regression-testing",
      "code-review",
      "testing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 24 static findings are false positives. This skill contains only documentation in Japanese describing a regression monitoring workflow. The static analyzer misidentified markdown code blocks as executable code, line numbers as cryptographic algorithms, and Japanese UTF-8 text as high entropy binary data. No actual executable code, network calls, or security risks exist.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 663,
    "audit_model": "claude",
    "audited_at": "2026-01-21T16:32:19.088Z",
    "risk_factors": []
  },
  "content": {
    "user_title": "Monitor Quality Regression During Refactoring",
    "value_statement": "Refactoring can introduce subtle bugs that break existing functionality. This skill guides you through recording quality baselines before changes and systematically compares test results, build outcomes, and code coverage afterwards to detect regressions early.",
    "seo_keywords": [
      "Claude",
      "Claude Code",
      "Codex",
      "regression testing",
      "refactoring safety",
      "code quality monitoring",
      "test comparison",
      "build verification",
      "rollback detection",
      "quality assurance"
    ],
    "actual_capabilities": [
      "Guides recording of baseline metrics including test counts, success rates, lint errors, build status, and code coverage before refactoring",
      "Provides structured comparison workflow to detect regressions by comparing current state against baseline metrics",
      "Evaluates regression severity using predefined thresholds to recommend rollback or continuation",
      "Generates detailed comparison reports with before and after metrics in both English and Japanese",
      "Supports both backend and frontend projects with framework-agnostic command examples",
      "Defines clear rollback criteria distinguishing critical regressions from minor warnings"
    ],
    "limitations": [
      "Provides documentation and workflow guidance only; does not execute tests or builds automatically",
      "Requires manual command execution for baseline recording and verification phases",
      "Effectiveness depends on existing test coverage; cannot detect regressions in untested code paths",
      "Assumes stable environment between baseline and verification to avoid false positives"
    ],
    "use_cases": [
      {
        "title": "Safe Database Schema Refactoring",
        "description": "Record baseline test results before modifying database models or migrations. After refactoring, compare test success rates to ensure data integrity and query logic remain intact.",
        "target_user": "Backend developers"
      },
      {
        "title": "API Version Upgrade Validation",
        "description": "Establish baseline for API tests before upgrading framework or library versions. Verify that response formats and contract compatibility are maintained after the upgrade.",
        "target_user": "Full-stack developers"
      },
      {
        "title": "Legacy Code Modernization",
        "description": "Create comprehensive baseline metrics before refactoring legacy code. Systematically verify that behavior remains unchanged while improving code structure and maintainability.",
        "target_user": "Senior developers and tech leads"
      }
    ],
    "prompt_templates": [
      {
        "title": "Record Baseline Before Refactoring",
        "prompt": "Record the current baseline metrics for our project before we begin refactoring the authentication module. Include test success rates, build results, lint errors, and code coverage.",
        "scenario": "First step before starting any refactoring work to establish a quality baseline"
      },
      {
        "title": "Verify Quality After Changes",
        "prompt": "Compare the current project state against the baseline we recorded earlier. Report any regressions in tests, builds, lint, or coverage and recommend whether to proceed or rollback.",
        "scenario": "Final verification step after completing refactoring to detect any quality degradation"
      },
      {
        "title": "Quick Critical Regression Check",
        "prompt": "Focus on test success rate and build results only. Compare these critical metrics against our baseline and tell me if we need to rollback immediately.",
        "scenario": "Fast safety check during iterative refactoring when you need quick feedback"
      },
      {
        "title": "Generate Detailed Regression Report",
        "prompt": "Create a comprehensive regression report for the recent API refactoring. Include all metrics with before and after comparisons, list any new failures, and provide rollback recommendations.",
        "scenario": "Preparing documentation for code review or stakeholder presentation"
      }
    ],
    "output_examples": [
      {
        "input": "Record baseline before refactoring the authentication module",
        "output": "Baseline recorded successfully. Backend: 150 of 150 tests passing (100% success rate), 0 lint errors with 3 warnings, build successful, code coverage at 85%. Proceed with refactoring and run verification after changes are complete."
      },
      {
        "input": "Verify quality after refactoring the authentication module",
        "output": "Verification complete. No regressions detected. All metrics maintained or improved: test success rate 100% to 100%, lint errors 0 to 0, build still successful, coverage improved from 85% to 87%. Refactoring is safe to proceed."
      },
      {
        "input": "Compare current state to baseline and check for regressions",
        "output": "Regression detected. Test success rate dropped from 100% to 98% with 3 new test failures in UserServiceTest. Build failed with compilation error. Immediate rollback recommended. Failed tests: testDuplicateEmail, testPasswordValidation, testUserRegistration."
      }
    ],
    "best_practices": [
      "Always record the baseline immediately before starting refactoring work to ensure accurate comparison points",
      "Execute baseline and verification commands in identical environments to prevent environment-related false positives",
      "Define clear severity thresholds before refactoring so you know when to rollback versus when to investigate further"
    ],
    "anti_patterns": [
      "Skipping baseline recording because the refactoring seems small or the codebase appears stable",
      "Dismissing minor regressions as acceptable when they accumulate across multiple refactoring sessions",
      "Running verification in a different environment or with different dependency versions than the baseline"
    ],
    "faq": [
      {
        "question": "What metrics does this skill monitor?",
        "answer": "The skill tracks test success rates and counts, build success or failure status, lint errors and warnings, code coverage percentages, and optionally build time and test execution time."
      },
      {
        "question": "How does it decide when to recommend rollback?",
        "answer": "It uses predefined severity thresholds. Critical regressions like any new test failures, build failures, or coverage drops over 5% trigger immediate rollback. Minor issues like small warning increases generate warnings but allow continuation."
      },
      {
        "question": "Can I customize the regression thresholds?",
        "answer": "Yes. The thresholds are documented in the skill file and you can adjust them to match your project requirements, such as changing what percentage of coverage drop constitutes a critical issue."
      },
      {
        "question": "Does this skill execute tests and builds automatically?",
        "answer": "No. This is a workflow documentation skill that guides you through the process. You must manually run the test and build commands and compare the results."
      },
      {
        "question": "What if my baseline already has failing tests?",
        "answer": "The workflow recommends excluding pre-existing failures from regression detection to avoid false positives. Only newly introduced failures trigger rollback recommendations."
      },
      {
        "question": "Is this skill compatible with my test framework?",
        "answer": "Yes. The skill provides example commands for common frameworks like Gradle and npm, but you can substitute your own project-specific commands for any testing, linting, or build system."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 329
    }
  ]
}
