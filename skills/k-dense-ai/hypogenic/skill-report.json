{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-12T16:30:26.069Z",
    "slug": "k-dense-ai-hypogenic",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/hypogenic",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "7187603d06cbca40a56f42ac8f34d8f690e70687c67bf542adba36762de25988",
    "tree_hash": "54f1b35891170b262399ba16723a52a3c1fd4688c131d3e630f1d68938fbb464"
  },
  "skill": {
    "name": "hypogenic",
    "description": "Automated LLM-driven hypothesis generation and testing on tabular datasets. Use when you want to systematically explore hypotheses about patterns in empirical data (e.g., deception detection, content analysis). Combines literature insights with data-driven hypothesis testing. For manual hypothesis formulation use hypothesis-generation; for creative ideation use scientific-brainstorming.",
    "summary": "Automated LLM-driven hypothesis generation and testing on tabular datasets. Use when you want to sys...",
    "icon": "ðŸ”¬",
    "version": "1.0.0",
    "author": "K-Dense-AI",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "hypothesis-generation",
      "scientific-discovery",
      "llm",
      "data-analysis",
      "research-tool"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "env_access",
      "external_commands",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "medium",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill contains legitimate research tooling with some security considerations. External command execution is used for academic tools like GROBID PDF processing, not malicious purposes. API key access is standard for LLM integration. The 'C2 keywords' finding appears to be a false positive - the context is academic citations, not command & control infrastructure.",
    "risk_factor_evidence": [
      {
        "factor": "env_access",
        "evidence": [
          {
            "file": "references/config_template.yaml",
            "line_start": 17,
            "line_end": 17
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 233,
            "line_end": 233
          },
          {
            "file": "SKILL.md",
            "line_start": 243,
            "line_end": 243
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 24,
            "line_end": 24
          },
          {
            "file": "SKILL.md",
            "line_start": 135,
            "line_end": 135
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [
      {
        "title": "External command execution for GROBID PDF processing",
        "description": "External commands are used to set up and run GROBID for PDF literature processing. GROBID is a legitimate open-source academic tool for extracting structured information from scientific PDFs. The bash scripts in modules/ directory are used for academic research purposes, not for malicious activities.",
        "locations": [
          {
            "file": "SKILL.md",
            "line_start": 233,
            "line_end": 233
          },
          {
            "file": "SKILL.md",
            "line_start": 243,
            "line_end": 243
          }
        ]
      }
    ],
    "low_findings": [
      {
        "title": "Hardcoded URLs to academic resources",
        "description": "The skill references hardcoded URLs pointing to legitimate academic resources including arXiv papers, GitHub repositories for the ChicagoHAI research group, and PyPI package distribution. These URLs are for accessing open-source research tools and datasets essential to the skill's functionality.",
        "locations": [
          {
            "file": "SKILL.md",
            "line_start": 24,
            "line_end": 24
          },
          {
            "file": "SKILL.md",
            "line_start": 135,
            "line_end": 135
          }
        ]
      }
    ],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 806,
    "audit_model": "claude",
    "audited_at": "2026-01-12T16:30:26.069Z"
  },
  "content": {
    "user_title": "Generate and test scientific hypotheses with AI",
    "value_statement": "Hypogenic automates hypothesis generation and testing using large language models, accelerating scientific discovery. It combines literature insights with data-driven pattern recognition to systematically explore research questions.",
    "seo_keywords": [
      "hypothesis generation",
      "scientific discovery",
      "LLM research",
      "data analysis",
      "Claude Code",
      "research automation",
      "empirical testing",
      "literature review",
      "pattern recognition",
      "academic research"
    ],
    "actual_capabilities": [
      "Generate 10-20 testable hypotheses from tabular datasets",
      "Integrate insights from research papers with empirical data",
      "Test hypotheses using multi-model inference and voting",
      "Cache LLM responses to reduce API costs",
      "Process PDF literature using GROBID academic tool",
      "Support for OpenAI, Anthropic, and local LLMs"
    ],
    "limitations": [
      "Requires structured datasets in HuggingFace format",
      "Needs API keys for cloud LLM providers",
      "Literature processing requires GROBID setup",
      "Performance depends on prompt template quality"
    ],
    "use_cases": [
      {
        "target_user": "Academic researchers",
        "title": "Accelerate hypothesis discovery",
        "description": "Generate novel hypotheses from observational data in minutes rather than weeks, combining literature insights with empirical patterns."
      },
      {
        "target_user": "Data scientists",
        "title": "Systematic pattern exploration",
        "description": "Test multiple competing hypotheses on classification tasks like deception detection, AI-content identification, or mental health indicators."
      },
      {
        "target_user": "Graduate students",
        "title": "Literature-driven research",
        "description": "Build on existing theoretical foundations by integrating research papers with your dataset for theory-grounded hypothesis generation."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic hypothesis generation",
        "scenario": "Generate hypotheses from your dataset",
        "prompt": "Use hypogenic to generate 20 testable hypotheses from my deception detection dataset. Create a config.yaml and run hypothesis generation with the HypoGeniC method."
      },
      {
        "title": "Literature integration",
        "scenario": "Combine papers with data analysis",
        "prompt": "Use hypogenic with the HypoRefine method to generate 15 theory-grounded hypotheses from my hotel review dataset. Include 10 relevant research papers in the literature/ directory. This should integrate both literature insights and empirical patterns from the data."
      }
    ],
    "output_examples": [
      {
        "input": "Generate 10 testable hypotheses from a hotel review dataset with genuine vs. deceptive labels",
        "output": [
          "Deceptive reviews contain fewer first-person pronouns",
          "Genuine reviews have more specific sensory details",
          "Deceptive reviews use more absolute terms (always, never)",
          "Genuine reviews have more temporal references",
          "Deceptive reviews have shorter average sentence lengths"
        ]
      },
      {
        "input": "Use HypoRefine with 5 papers on linguistic cues to generate 15 hypotheses for AI-content detection",
        "output": [
          "AI-generated text has higher formality scores",
          "Human text contains more personal anecdotes",
          "AI-generated content has more balanced sentence structure",
          "Human text shows inconsistent grammatical patterns",
          "AI text uses more hedge words (perhaps, might, possibly)"
        ]
      },
      {
        "input": "Run inference on test data with 20 generated hypotheses about mental health indicators",
        "output": [
          "Multi-hypothesis voting achieves 73% accuracy vs 62% single hypothesis baseline",
          "Hypotheses about emotional vocabulary show highest precision",
          "Pronoun usage hypotheses generalize across demographics"
        ]
      }
    ],
    "best_practices": [
      "Use clear, descriptive feature names (e.g., review_text, headline) in your dataset",
      "Prepare 10-15 relevant research papers for HypoRefine to maximize literature integration",
      "Enable Redis caching when running iterative experiments to reduce API costs",
      "Customize the extract_label() function for your domain-specific LLM output format",
      "Refine prompt templates to request specific, testable hypotheses rather than general patterns",
      "Start with HypoGeniC for exploratory research without existing literature",
      "Use Union methods when you need comprehensive hypothesis coverage combining literature and data-driven approaches",
      "Include sufficient training examples (100+) for reliable hypothesis generation",
      "Test multiple hypotheses simultaneously to identify the most predictive patterns"
    ],
    "anti_patterns": [
      "Using unstructured text data instead of tabular format with labeled features",
      "Generating hypotheses without customizing prompt templates for your domain",
      "Skipping the extract_label() configuration, causing inference accuracy issues",
      "Using HypoRefine without relevant research papers (use HypoGeniC instead)",
      "Expecting good results with very small datasets (fewer than 50 examples)",
      "Not validating extracted labels match your dataset format before running inference",
      "Running hypothesis generation on tasks without clear classification labels"
    ],
    "faq": [
      {
        "question": "What dataset formats does Hypogenic support?",
        "answer": "Hypogenic requires datasets in HuggingFace format with JSON files containing text_features (lists of strings) and label fields. Each task needs train, validation, and test splits in separate files."
      },
      {
        "question": "How many hypotheses should I generate per task?",
        "answer": "The framework works best with 10-20 hypotheses. Too few may miss important patterns; too many reduces quality through redundancy. Start with 15 and adjust based on results."
      },
      {
        "question": "Do I need GROBID for all use cases?",
        "answer": "No. GROBID is only required for HypoRefine and Union methods that integrate literature. For data-only hypothesis generation (HypoGeniC), GROBID is not needed."
      },
      {
        "question": "Can I use local LLMs instead of API providers?",
        "answer": "Yes. Hypogenic supports OpenAI, Anthropic, and local LLMs through its flexible configuration system. Local models require setting up your own inference endpoint."
      },
      {
        "question": "What is the difference between HypoGeniC and HypoRefine?",
        "answer": "HypoGeniC generates hypotheses purely from data patterns. HypoRefine synergistically combines existing literature with empirical data for theory-grounded hypothesis generation."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "config_template.yaml",
          "type": "file",
          "path": "references/config_template.yaml"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
