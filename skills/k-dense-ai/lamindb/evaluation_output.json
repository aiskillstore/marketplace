{
  "skill": {
    "name": "lamindb",
    "description": "This skill should be used when working with LaminDB, an open-source data framework for biology that makes data queryable, traceable, reproducible, and FAIR. Use when managing biological datasets (scRNA-seq, spatial, flow cytometry, etc.), tracking computational workflows, curating and validating data with biological ontologies, building data lakehouses, or ensuring data lineage and reproducibility in biological research. Covers data management, annotation, ontologies (genes, cell types, diseases, tissues), schema validation, integrations with workflow managers (Nextflow, Snakemake) and MLOps platforms (W&B, MLflow), and deployment strategies.",
    "summary": "Biological data management framework for queryable, traceable, reproducible, and FAIR datasets",
    "icon": "ðŸ§¬",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "Apache-2.0",
    "category": "data",
    "tags": ["biological-data", "data-management", "ontologies", "reproducibility", "workflow-tracking"],
    "supported_tools": ["claude", "codex", "claude-code"]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation skill for LaminDB biological data management framework. All 570 static findings are false positives - the analyzer incorrectly flagged markdown documentation code examples as executable security issues. No actual code execution, credential harvesting, or malicious patterns exist. Files contain legitimate documentation for an open-source scientific data management library.",
    "static_findings_evaluation": [
      {
        "finding": "external_commands: Ruby/shell backtick execution (414 locations across all markdown files)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Analyzer incorrectly detected markdown inline code formatting (backticks like `import lamindb`) and code blocks (```python, ```bash) as shell command execution. All files are pure documentation containing Python code examples demonstrating legitimate LaminDB library usage. No actual backtick command execution exists."
      },
      {
        "finding": "filesystem: Hard link creation (40 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation examples showing ln.Artifact class usage are misinterpreted as hard link operations. 'ln' is the standard import alias for the lamindb module. Code like 'ln.Artifact(key=...).save()' calls a class constructor method, not filesystem hard link creation. References to hidden files (.lamin/, ~/.lamin/) are legitimate configuration paths, not malicious file operations."
      },
      {
        "finding": "blocker: Weak cryptographic algorithm (HIGH, 29 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "No actual cryptographic algorithms or implementations exist in these documentation files. The analyzer likely flagged text patterns containing 'hash', 'encrypt', or similar terms in documentation context. All references are documentation about data hashing concepts, not weak crypto code."
      },
      {
        "finding": "blocker: System reconnaissance (LOW, 14 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation of standard LaminDB commands like 'lamin info', 'lamin ls', 'lamin cache get' are legitimate configuration queries. Database connectivity tests like 'pg_dump' and 'psql' are standard PostgreSQL commands for backup and troubleshooting, not reconnaissance behavior."
      },
      {
        "finding": "blocker: Network scanning tools (HIGH, 1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "No network scanning tools exist in this documentation. References to network-related concepts are standard cloud storage configuration documentation (S3, GCS connectivity)."
      },
      {
        "finding": "network: Hardcoded URL (LOW, 17 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation contains legitimate URLs for LaminDB official resources: docs.lamin.ai, GitHub repository, tutorials, and API documentation. These are standard documentation references, not suspicious hardcoded endpoints for malicious purposes."
      },
      {
        "finding": "network: Python HTTP libraries (LOW, 1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows standard requests.get() for fetching data from public APIs as part of integration examples. This is legitimate HTTP usage for data retrieval, not malicious network activity."
      },
      {
        "finding": "env_access: AWS credential environment variables (14 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Standard configuration documentation showing environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION for legitimate S3 storage setup. These are documented best practices for cloud authentication, not credential harvesting patterns."
      },
      {
        "finding": "env_access: GCP credential environment variables (3 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows GOOGLE_APPLICATION_CREDENTIALS for legitimate Google Cloud Storage authentication. Standard configuration documentation for cloud storage access."
      },
      {
        "finding": "env_access: Generic API/secret keys (2 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation examples use placeholder values like 'your_key_id', 'your_secret_key' to demonstrate proper configuration patterns. Standard documentation practice for credential setup."
      },
      {
        "finding": "sensitive: Certificate/key files (HIGH, 2 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "References to credentials.json are standard GCP service account authentication documentation. Documentation shows proper paths for credential files, not exposure of actual credentials."
      },
      {
        "finding": "sensitive: Credential JSON file (HIGH, 2 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation mentions credentials.json for GCP authentication as standard practice. Path references like /path/to/credentials.json are placeholders, not actual credential exposure."
      },
      {
        "finding": "sensitive: Browser storage access (HIGH, 4 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "References to browser storage APIs are part of Vitessce visualization integration documentation. These are legitimate web application integration patterns, not malicious storage access."
      },
      {
        "finding": "sensitive: Environment file access (HIGH, 2 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows system.env file creation for multi-user cache configuration. Standard system administration documentation for shared cache setup."
      },
      {
        "finding": "sensitive: SQLite database file (MEDIUM, 1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Reference to SQLite database location at ./data/.lamindb/ is legitimate LaminDB configuration documentation. Standard database path documentation, not sensitive file exposure."
      },
      {
        "finding": "external_commands: sudo privilege escalation (HIGH, 4 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows sudo commands for creating shared system cache directories in multi-user environments. Standard system administration documentation for permission management, not privilege escalation exploitation."
      },
      {
        "finding": "external_commands: Shell command substitution (MEDIUM, 1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows $() command substitution in pg_dump backup examples. Standard shell scripting for timestamped backup files, not malicious command injection."
      },
      {
        "finding": "external_commands: Template literal with command substitution (MEDIUM, 1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Backtick command substitution in pg_dump backup example uses standard shell scripting patterns. Documentation for backup procedures, not malicious code."
      },
      {
        "finding": "filesystem: Hidden file in home directory (HIGH, 1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Reference to ~/.lamin/ is LaminDB's standard configuration directory. Documentation of legitimate application configuration path, not malicious hidden file access."
      },
      {
        "finding": "filesystem: Hidden file access (MEDIUM, 2 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows access to ~/.lamin/ directory for API key storage. Standard configuration documentation for LaminDB authentication setup."
      },
      {
        "finding": "filesystem: Temp directory access (MEDIUM, 2 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows /tmp/export/ for data export functionality. Standard temporary file usage for data migration, not malicious temp file operations."
      },
      {
        "finding": "filesystem: Python shutil operations (MEDIUM, 2 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows shutil.rmtree() for cache clearing. Standard library usage for cache management, not malicious file deletion."
      },
      {
        "finding": "filesystem: Python file write/append (MEDIUM, 1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Documentation shows file write operations for Vitessce configuration JSON. Standard file I/O for visualization configuration, not malicious file operations."
      },
      {
        "finding": "obfuscation: DANGEROUS COMBINATION - Code execution + Network + Credential access (CRITICAL)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Analyzer flagged documentation covering Python code examples, HTTP URLs for documentation links, and cloud credential configuration as suspicious combination. This is simply comprehensive documentation for a data management library covering all aspects of setup (code, network access for data, cloud credentials for storage). No obfuscation or malicious intent exists - these are standard documentation patterns for complex scientific software."
      },
      {
        "finding": "obfuscation: SUSPICIOUS COMBINATION - Filesystem + Credentials + Network (HIGH)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Same as above - documentation shows standard patterns for configuring cloud storage with credentials, filesystem paths, and network endpoints. Standard configuration documentation, not suspicious behavior."
      }
    ],
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 7,
    "total_lines": 3594
  },
  "content": {
    "user_title": "Manage biological data with LaminDB",
    "value_statement": "LaminDB makes biological data queryable, traceable, reproducible, and FAIR. It provides unified data management for single-cell genomics, spatial transcriptomics, and other biological datasets with automatic lineage tracking and ontology-based annotation.",
    "seo_keywords": [
      "LaminDB",
      "biological data management",
      "single-cell genomics",
      "FAIR data",
      "data lineage tracking",
      "biological ontologies",
      "scRNA-seq data",
      "Claude Code",
      "Claude",
      "Codex"
    ],
    "actual_capabilities": [
      "Track computational workflows with automatic lineage from raw data to results",
      "Query and filter biological datasets by metadata, features, and ontology terms",
      "Validate and annotate datasets using biological ontologies (genes, cell types, diseases)",
      "Integrate with workflow managers (Nextflow, Snakemake) and MLOps platforms (W&B, MLflow)",
      "Manage local, S3, and Google Cloud storage with unified API",
      "Version data artifacts and visualize data provenance graphs"
    ],
    "limitations": [
      "Requires understanding of biological data formats (AnnData, Parquet, Zarr)",
      "Ontology validation depends on Bionty package coverage",
      "Multi-user deployments require PostgreSQL, not suitable for SQLite in production"
    ],
    "use_cases": [
      {
        "target_user": "Bioinformatics researchers",
        "title": "Single-cell analysis",
        "description": "Track scRNA-seq workflows, validate cell type annotations against ontologies, and ensure reproducibility"
      },
      {
        "target_user": "Data engineers",
        "title": "Data lakehouse building",
        "description": "Build queryable data lakehouses across multiple biological datasets with unified metadata search"
      },
      {
        "target_user": "ML engineers",
        "title": "ML pipeline integration",
        "description": "Connect ML workflows with experiment tracking platforms for model lineage and reproducibility"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic data tracking",
        "scenario": "Start tracking a new analysis",
        "prompt": "Help me set up LaminDB tracking for my scRNA-seq analysis pipeline. Show me how to use ln.track() and ln.finish() to capture lineage."
      },
      {
        "title": "Ontology validation",
        "scenario": "Validate cell type annotations",
        "prompt": "I have cell type annotations in my AnnData object. Show me how to validate and standardize them using the CellType ontology via Bionty."
      },
      {
        "title": "Query datasets",
        "scenario": "Search biological data",
        "prompt": "Show me how to query LaminDB artifacts by tissue type, creation date, and features. I need to filter for PBMC datasets from treated samples."
      },
      {
        "title": "Cloud integration",
        "scenario": "Configure cloud storage",
        "prompt": "Help me configure LaminDB with AWS S3 storage. Show me how to set up credentials, initialize the instance, and access artifacts from the cloud."
      }
    ],
    "output_examples": [
      {
        "input": "How do I track my single-cell RNA-seq analysis workflow with LaminDB?",
        "output": [
          "Start by initializing tracking at the beginning of your analysis with ln.track()",
          "This automatically captures your code, environment, and parameters",
          "Load your AnnData object and process as normal",
          "Save processed data as versioned artifacts with descriptive keys",
          "Call ln.finish() when done to complete the lineage record",
          "View the lineage graph with artifact.view_lineage() to see data provenance"
        ]
      }
    ],
    "best_practices": [
      "Use ln.track() at the start of every analysis for automatic lineage capture",
      "Structure artifact keys hierarchically (project/experiment/batch/file.h5ad)",
      "Define typed features for queryable metadata annotations",
      "Validate data against ontologies before analysis to ensure standardized terminology"
    ],
    "anti_patterns": [
      "Don't skip ln.track() - lineage information is lost and cannot be recovered",
      "Avoid duplicating data with new keys - use versioning instead",
      "Don't load large files without filtering - query metadata first to identify relevant datasets"
    ],
    "faq": [
      {
        "question": "What is LaminDB used for?",
        "answer": "LaminDB manages biological data for queryability, traceability, reproducibility, and FAIR compliance. It tracks computational workflows and validates annotations."
      },
      {
        "question": "How does LaminDB track data lineage?",
        "answer": "Use ln.track() at workflow start and ln.finish() at end. This captures code, environment, and parameters with automatic versioning of input and output artifacts."
      },
      {
        "question": "What biological ontologies are supported?",
        "answer": "Genes, proteins, cell types, tissues, diseases, phenotypes, pathways, developmental stages, organisms, and drugs via the Bionty package."
      },
      {
        "question": "Can LaminDB integrate with cloud storage?",
        "answer": "Yes, LaminDB supports AWS S3, Google Cloud Storage, and S3-compatible services like MinIO and Cloudflare R2 with a unified API."
      },
      {
        "question": "What databases does LaminDB support?",
        "answer": "SQLite for local development and PostgreSQL for production multi-user deployments. PostgreSQL is required for concurrent access."
      },
      {
        "question": "How do I validate cell type annotations?",
        "answer": "Use Bionty's CellType.standardize() to convert terms to validated names, and CellType.validate() to check which terms are valid ontology terms."
      }
    ]
  }
}
