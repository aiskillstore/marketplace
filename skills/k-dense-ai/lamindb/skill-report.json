{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T06:09:37.932Z",
    "slug": "k-dense-ai-lamindb",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/lamindb",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "e32b270afb4cae6be85fa8df0bcfea7cc70113bf0f2a3b0818f066ccec489c0a",
    "tree_hash": "fbd69c819ecf333426b24c39fc932b2cb80e91a8c44c8761789d64a2049143a1"
  },
  "skill": {
    "name": "lamindb",
    "description": "This skill should be used when working with LaminDB, an open-source data framework for biology that makes data queryable, traceable, reproducible, and FAIR. Use when managing biological datasets (scRNA-seq, spatial, flow cytometry, etc.), tracking computational workflows, curating and validating data with biological ontologies, building data lakehouses, or ensuring data lineage and reproducibility in biological research. Covers data management, annotation, ontologies (genes, cell types, diseases, tissues), schema validation, integrations with workflow managers (Nextflow, Snakemake) and MLOps platforms (W&B, MLflow), and deployment strategies.",
    "summary": "This skill should be used when working with LaminDB, an open-source data framework for biology that ...",
    "icon": "ðŸ§¬",
    "version": "1.0.0",
    "author": "K-Dense-AI",
    "license": "Apache-2.0 license",
    "category": "data",
    "tags": [
      "biological-data",
      "data-management",
      "ontologies",
      "reproducibility",
      "workflow-tracking"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "external_commands",
      "filesystem",
      "network",
      "env_access"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation skill containing only markdown files with code examples for LaminDB biological data management. All 607 static findings are false positives. The analyzer incorrectly flagged markdown code formatting (backticks, code blocks), documentation about cloud storage configuration (AWS, GCP credentials), and library usage patterns (ln.Artifact) as security issues. No executable code, scripts, credential harvesting, or malicious patterns exist.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "references/annotation-validation.md",
            "line_start": 21,
            "line_end": 32
          },
          {
            "file": "SKILL.md",
            "line_start": 49,
            "line_end": 61
          },
          {
            "file": "references/setup-deployment.md",
            "line_start": 9,
            "line_end": 21
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "references/core-concepts.md",
            "line_start": 12,
            "line_end": 24
          },
          {
            "file": "SKILL.md",
            "line_start": 195,
            "line_end": 226
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "references/integrations.md",
            "line_start": 46,
            "line_end": 72
          },
          {
            "file": "SKILL.md",
            "line_start": 383,
            "line_end": 387
          }
        ]
      },
      {
        "factor": "env_access",
        "evidence": [
          {
            "file": "references/integrations.md",
            "line_start": 49,
            "line_end": 50
          },
          {
            "file": "references/setup-deployment.md",
            "line_start": 205,
            "line_end": 206
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 9,
    "total_lines": 6559,
    "audit_model": "claude",
    "audited_at": "2026-01-17T06:09:37.932Z"
  },
  "content": {
    "user_title": "Manage biological data with LaminDB",
    "value_statement": "Biological research generates complex datasets that are difficult to track, query, and reproduce. LaminDB provides a unified framework for managing biological data with automatic lineage tracking, ontology-based annotations, and seamless integration with workflow managers.",
    "seo_keywords": [
      "lamindb",
      "biological data management",
      "single-cell analysis",
      "FAIR data",
      "data lineage",
      "ontologies",
      "Claude",
      "Codex",
      "claude-code",
      "workflow tracking"
    ],
    "actual_capabilities": [
      "Create and version datasets (DataFrames, AnnData, Parquet, Zarr formats)",
      "Track computational lineage from raw data through analysis to results",
      "Validate and standardize data using biological ontologies (genes, cell types, diseases, tissues)",
      "Query datasets by metadata, features, and ontology terms",
      "Integrate with Nextflow, Snakemake, W&B, and MLflow",
      "Deploy local or cloud-based data management systems (AWS S3, GCS)"
    ],
    "limitations": [
      "Requires understanding of Python and biological data formats",
      "Cloud storage costs depend on provider and usage",
      "Ontology imports require internet access for initial download",
      "Does not replace workflow execution engines (coordinates with them)"
    ],
    "use_cases": [
      {
        "target_user": "Single-cell researchers",
        "title": "Annotate scRNA-seq data",
        "description": "Validate and standardize cell type annotations using controlled vocabularies from Cell Ontology"
      },
      {
        "target_user": "Data engineers",
        "title": "Build data lakehouses",
        "description": "Create unified query interfaces across multiple biological datasets with automatic versioning"
      },
      {
        "target_user": "ML engineers",
        "title": "Track model lineage",
        "description": "Link training data artifacts to MLflow or W&B experiments for full reproducibility"
      }
    ],
    "prompt_templates": [
      {
        "title": "Get started",
        "scenario": "New LaminDB user",
        "prompt": "Help me set up LaminDB locally. I want to install it, authenticate, and initialize a local instance for managing my single-cell datasets."
      },
      {
        "title": "Annotate data",
        "scenario": "Adding metadata",
        "prompt": "I have scRNA-seq data with cell type labels. Show me how to validate and standardize these labels using the Cell Ontology via Bionty."
      },
      {
        "title": "Track lineage",
        "scenario": "Pipeline integration",
        "prompt": "I run Nextflow pipelines for bulk RNA-seq analysis. Show me how to integrate LaminDB to track which code produced which output files."
      },
      {
        "title": "Query data",
        "scenario": "Find datasets",
        "prompt": "I have hundreds of Parquet files organized by experiment and batch. Show me how to query all artifacts from project X with tissue=PBMC and condition=treated without loading all files."
      }
    ],
    "output_examples": [
      {
        "input": "How do I track my notebook analysis with LaminDB?",
        "output": [
          "Use ln.track() at the start of your notebook to begin lineage capture",
          "Import your data and perform analysis as normal",
          "Call ln.finish() to complete tracking when done",
          "View lineage with artifact.view_lineage() to see data provenance"
        ]
      },
      {
        "input": "Can you help me validate my experimental metadata?",
        "output": [
          "Define a schema with required columns and data types",
          "Create a DataFrameCurator or AnnDataCurator with your schema",
          "Use curator.validate() to check data integrity",
          "Use .cat.standardize() to fix typos and map synonyms"
        ]
      },
      {
        "input": "How do I connect LaminDB to my cloud storage?",
        "output": [
          "Install extras: pip install 'lamindb[aws]' or 'lamindb[gcp]'",
          "Configure storage: lamin init --storage s3://your-bucket",
          "Set credentials via environment variables or config files",
          "LaminDB handles caching and sync automatically"
        ]
      }
    ],
    "best_practices": [
      "Start every analysis notebook with ln.track() and end with ln.finish() for automatic lineage capture",
      "Define schemas and validate data early to catch issues before extensive analysis",
      "Use hierarchical artifact keys like 'project/experiment/batch/file.h5ad' for organization"
    ],
    "anti_patterns": [
      "Creating new artifact keys for modified versions instead of using built-in versioning",
      "Loading large datasets without filtering first - query metadata first to reduce I/O",
      "Skipping ontology standardization which leads to inconsistent queries across similar terms"
    ],
    "faq": [
      {
        "question": "What data formats does LaminDB support?",
        "answer": "LaminDB supports DataFrames (Parquet, CSV), AnnData (single-cell), MuData (multi-modal), SpatialData, and TileDB-SOMA arrays."
      },
      {
        "question": "Do I need a server to use LaminDB?",
        "answer": "No. LaminDB works locally with SQLite for development. Scale to cloud storage with PostgreSQL for production teams."
      },
      {
        "question": "How does LaminDB integrate with Nextflow?",
        "answer": "Use ln.track() in process scripts to record inputs and outputs. LaminDB captures provenance automatically for each step."
      },
      {
        "question": "What biological ontologies are available?",
        "answer": "Genes (Ensembl), Proteins (UniProt), Cell types (CL), Tissues (Uberon), Diseases (Mondo), Phenotypes (HPO), and Pathways (GO)."
      },
      {
        "question": "Can I use LaminDB without internet?",
        "answer": "Yes for local operations. Initial ontology downloads and cloud storage access require internet. Cache ontologies locally for offline use."
      },
      {
        "question": "How is LaminDB different from a database?",
        "answer": "LaminDB combines database features (querying, filtering) with versioned file storage and lineage tracking specialized for scientific data workflows."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "annotation-validation.md",
          "type": "file",
          "path": "references/annotation-validation.md",
          "lines": 514
        },
        {
          "name": "core-concepts.md",
          "type": "file",
          "path": "references/core-concepts.md",
          "lines": 381
        },
        {
          "name": "data-management.md",
          "type": "file",
          "path": "references/data-management.md",
          "lines": 434
        },
        {
          "name": "integrations.md",
          "type": "file",
          "path": "references/integrations.md",
          "lines": 643
        },
        {
          "name": "ontologies.md",
          "type": "file",
          "path": "references/ontologies.md",
          "lines": 498
        },
        {
          "name": "setup-deployment.md",
          "type": "file",
          "path": "references/setup-deployment.md",
          "lines": 734
        }
      ]
    },
    {
      "name": "evaluation_output.json",
      "type": "file",
      "path": "evaluation_output.json",
      "lines": 299
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 390
    }
  ]
}
