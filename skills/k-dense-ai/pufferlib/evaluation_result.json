{
  "skill": {
    "name": "pufferlib",
    "description": "High-performance reinforcement learning framework optimized for speed and scale. Use when you need fast parallel training, vectorized environments, multi-agent systems, or integration with game environments (Atari, Procgen, NetHack). Achieves 2-10x speedups over standard implementations. For quick prototyping or standard algorithm implementations with extensive documentation, use stable-baselines3 instead.",
    "summary": "High-performance RL framework for fast parallel training, vectorized environments, and multi-agent systems",
    "icon": "ðŸŽ®",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "data",
    "tags": ["reinforcement-learning", "machine-learning", "pytorch", "training", "parallel-computing"],
    "supported_tools": ["claude", "codex", "claude-code"]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 316 static findings are FALSE POSITIVES. This is a legitimate open-source reinforcement learning library. The analyzer incorrectly flagged shell command examples in documentation (markdown code blocks) as dangerous backtick execution. No actual command injection, credential exfiltration, or malicious patterns exist in the codebase.",
    "static_findings_evaluation": [
      {
        "finding": "[MEDIUM] external_commands: Ruby/shell backtick execution (251 locations across SKILL.md, references/*.md)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "These are documentation examples of CLI commands in markdown code blocks (puffer train, torchrun, uv pip install). No actual backtick execution exists in Python code. Arguments are hardcoded documentation strings, not user input."
      },
      {
        "finding": "[HIGH] blocker: Weak cryptographic algorithm (env_template.py:5,11; train_template.py:5,167; references/*.md)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "No cryptographic functions (md5, hashlib, encrypt, decrypt) found in codebase. Static analyzer incorrectly flagged docstring text and Python imports. Verified via grep - no actual weak crypto implementation."
      },
      {
        "finding": "[HIGH] sensitive: Environment file access (references/environments.md:350; references/integration.md:124,278,282,307,338,346,376,555; env_template.py:326)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "These are documentation examples showing CLI argument parsing for training configuration (--neptune-token). No actual environment file parsing exists. Standard argparse pattern for ML training workflows."
      },
      {
        "finding": "[LOW] blocker: System reconnaissance (multiple locations in SKILL.md, templates, references)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "These are legitimate system queries for ML training: torch.cuda.is_available(), torch.cuda.get_device_name() for GPU device selection. Standard practice for PyTorch training scripts."
      },
      {
        "finding": "[MEDIUM] filesystem: Python os file operations (scripts/train_template.py:232)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "Standard checkpoint saving (os.makedirs for checkpoints directory). Legitimate use case for model persistence in training workflows."
      },
      {
        "finding": "[LOW] network: Hardcoded URL (SKILL.md:431,432)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "These are documentation links to official docs (puffer.ai, github.com). No dynamic URL fetching or network calls in code."
      },
      {
        "finding": "[CRITICAL] obfuscation: DANGEROUS COMBINATION (multiple:1)",
        "verdict": "false_positive",
        "confidence": "medium",
        "reasoning": "Heuristic triggered by false positive patterns. No actual code execution + network + credential access combination exists. All findings are documentation examples and standard ML training code."
      },
      {
        "finding": "[HIGH] obfuscation: SUSPICIOUS COMBINATION (multiple:1)",
        "verdict": "false_positive",
        "confidence": "medium",
        "reasoning": "Heuristic triggered by false positive patterns. No malicious combination of filesystem + credentials + network found."
      }
    ],
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [{"file": "SKILL.md", "line_start": 33, "line_end": 269}]
      },
      {
        "factor": "scripts",
        "evidence": [{"file": "scripts/env_template.py", "line_start": 1, "line_end": 341}]
      },
      {
        "factor": "scripts",
        "evidence": [{"file": "scripts/train_template.py", "line_start": 1, "line_end": 240}]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 3721
  },
  "content": {
    "user_title": "Train reinforcement learning agents fast",
    "value_statement": "Training RL agents requires high-performance parallel environments and efficient algorithms. PufferLib provides optimized PPO+LSTM training with 2-10x speedups through vectorization, shared memory buffers, and multi-agent support.",
    "seo_keywords": [
      "pufferlib",
      "reinforcement learning",
      "PPO training",
      "parallel environments",
      "multi-agent RL",
      "Gymnasium integration",
      "PettingZoo",
      "vectorized simulation",
      "Claude Code",
      "Claude"
    ],
    "actual_capabilities": [
      "High-performance PPO+LSTM training at 1M-4M steps per second",
      "Vectorized parallel environment simulation with zero-copy data passing",
      "Seamless integration with Gymnasium, PettingZoo, Atari, Procgen, NetHack",
      "Built-in Ocean suite of 20+ pre-built environments",
      "Multi-agent environment support with shared or independent policies",
      "CLI and Python training scripts with checkpointing and logging"
    ],
    "limitations": [
      "Requires PyTorch and compatible CUDA version for GPU training",
      "Custom environments must follow PufferEnv API for best performance",
      "Distributed training requires NCCL-compatible cluster for multi-node"
    ],
    "use_cases": [
      {
        "target_user": "RL researchers",
        "title": "Fast benchmarking",
        "description": "Quickly benchmark new algorithms on Ocean environments with millions of steps per second throughput"
      },
      {
        "target_user": "Game AI developers",
        "title": "Game environment training",
        "description": "Train agents on Atari, Procgen, or NetHack with optimized vectorization and efficient PPO"
      },
      {
        "target_user": "Multi-agent system builders",
        "title": "Cooperative agent teams",
        "description": "Build and train multi-agent systems with PettingZoo integration and shared policy options"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic environment training",
        "scenario": "Train on standard benchmark",
        "prompt": "Use pufferlib to train a PPO agent on the procgen-coinrun environment with 256 parallel envs. Show the training loop and how to save checkpoints."
      },
      {
        "title": "Custom environment",
        "scenario": "Create custom PufferEnv",
        "prompt": "Help me create a custom PufferEnv for a grid world task with 4 discrete actions. Show the reset, step, and observation space definitions."
      },
      {
        "title": "Multi-agent training",
        "scenario": "Train cooperative agents",
        "prompt": "Use pufferlib to train multiple agents on a PettingZoo environment. Show how to handle dict observations and shared policies."
      },
      {
        "title": "Performance tuning",
        "scenario": "Optimize training speed",
        "prompt": "Optimize my pufferlib training setup for maximum throughput. What vectorization settings and hyperparameters should I use for 4 GPUs?"
      }
    ],
    "output_examples": [
      {
        "input": "Train PPO on CartPole with pufferlib",
        "output": [
          "âœ“ Environment: gym-CartPole-v1 with 256 parallel envs",
          "âœ“ Policy: 2-layer MLP (256 hidden units) with layer_init",
          "âœ“ Training: 10,000 iterations, batch size 32768",
          "âœ“ Checkpoint: Saved to checkpoints/checkpoint_1000.pt",
          "âœ“ Final throughput: 1.2M steps/second on GPU"
        ]
      }
    ],
    "best_practices": [
      "Start with Ocean environments or Gymnasium integration before building custom environments",
      "Profile steps per second early to identify bottlenecks before scaling",
      "Use torch.compile and CUDA for maximum training throughput"
    ],
    "anti_patterns": [
      "Avoid using CPU for large-scale training - use GPU with sufficient VRAM",
      "Do not skip environment validation before scaling to many parallel envs",
      "Avoid hardcoding hyperparameters - use CLI arguments for reproducibility"
    ],
    "faq": [
      {
        "question": "What environments does pufferlib support?",
        "answer": "Gymnasium, PettingZoo, Atari, Procgen, NetHack, Minigrid, Neural MMO, Crafter, and 20+ Ocean suite environments."
      },
      {
        "question": "How fast is pufferlib compared to standard implementations?",
        "answer": "Achieves 2-10x speedups through optimized vectorization, shared memory, and efficient PPO+LSTM implementation."
      },
      {
        "question": "Can I use pufferlib with custom environments?",
        "answer": "Yes, implement the PufferEnv API with reset, step methods, and observation/action spaces for best performance."
      },
      {
        "question": "Does pufferlib support multi-GPU training?",
        "answer": "Yes, use torchrun with --nproc_per_node for multi-GPU and NCCL for multi-node distributed training."
      },
      {
        "question": "What logging frameworks integrate with pufferlib?",
        "answer": "Weights & Biases (wandb) and Neptune loggers are built-in with simple configuration."
      },
      {
        "question": "How do I save and resume training?",
        "answer": "Use trainer.save_checkpoint() and trainer.load_checkpoint() with periodic save frequency for resume capability."
      }
    ]
  }
}