{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T21:55:57.412Z",
    "slug": "codingcossack-verification-before-completion",
    "source_url": "https://github.com/CodingCossack/agent-skills-library/tree/main/skills/verification-before-completion",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "8ea193ede8942068948d20f659057c85e02ff53541aeadd7175ad7c24ec73f5b",
    "tree_hash": "e99f8607b3dad794da8e47132020f1d8cb766a0a328c787b465b19c7d043e10b"
  },
  "skill": {
    "name": "verification-before-completion",
    "description": "Verification discipline for completion claims. Use when about to assert success, claim a fix is complete, report tests passing, or before commits and PRs. Enforces evidence-first workflow.",
    "summary": "Verification discipline for completion claims. Use when about to assert success, claim a fix is comp...",
    "icon": "✅",
    "version": "1.0.0",
    "author": "CodingCossack",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "verification",
      "workflow",
      "quality-assurance",
      "discipline",
      "testing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure markdown prompt skill containing only instructional guidelines for AI workflow discipline. All 25 static findings are false positives triggered by pattern-matching on common software development terms in documentation context. No executable code, network calls, file system access, environment variable access, or external command execution capabilities exist in this skill.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 14,
            "line_end": 23
          },
          {
            "file": "SKILL.md",
            "line_start": 23,
            "line_end": 34
          },
          {
            "file": "SKILL.md",
            "line_start": 34,
            "line_end": 41
          },
          {
            "file": "SKILL.md",
            "line_start": 41,
            "line_end": 78
          },
          {
            "file": "SKILL.md",
            "line_start": 78,
            "line_end": 81
          },
          {
            "file": "SKILL.md",
            "line_start": 81,
            "line_end": 84
          },
          {
            "file": "SKILL.md",
            "line_start": 84,
            "line_end": 87
          },
          {
            "file": "SKILL.md",
            "line_start": 87,
            "line_end": 90
          },
          {
            "file": "SKILL.md",
            "line_start": 90,
            "line_end": 93
          },
          {
            "file": "SKILL.md",
            "line_start": 93,
            "line_end": 96
          },
          {
            "file": "SKILL.md",
            "line_start": 96,
            "line_end": 99
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 277,
    "audit_model": "claude",
    "audited_at": "2026-01-16T21:55:57.412Z"
  },
  "content": {
    "user_title": "Verify work before claiming completion",
    "value_statement": "AI agents often claim task completion without proper verification. This skill enforces an evidence-first workflow requiring fresh verification commands and actual output evidence before any completion claims are made.",
    "seo_keywords": [
      "verification",
      "completion claims",
      "evidence-first workflow",
      "AI testing",
      "code quality",
      "Claude",
      "Codex",
      "Claude Code",
      "test verification",
      "workflow discipline"
    ],
    "actual_capabilities": [
      "Enforces verification before any completion claim",
      "Requires running actual verification commands with output",
      "Validates test output, exit codes, and failure counts",
      "Prevents trusting agent success reports without evidence",
      "Applies to tests, builds, linting, bug fixes, and commits",
      "Identifies red flag language indicating unverified claims"
    ],
    "limitations": [
      "Provides workflow discipline only, cannot execute verification itself",
      "Does not integrate with specific test frameworks or tools",
      "Relies on the AI agent to identify and run appropriate verification commands"
    ],
    "use_cases": [
      {
        "target_user": "Software developers",
        "title": "Prevent premature code commits",
        "description": "Ensure tests and builds pass with actual evidence before committing code changes"
      },
      {
        "target_user": "QA engineers",
        "title": "Validate bug fix claims",
        "description": "Require reproduction tests and verification commands before accepting bug fixes"
      },
      {
        "target_user": "AI agent operators",
        "title": "Verify agent task completion",
        "description": "Check VCS diffs and run verification when agents report task completion"
      }
    ],
    "prompt_templates": [
      {
        "title": "Simple test verification",
        "scenario": "Agent claims tests pass",
        "prompt": "Have you run the test command fresh? Show me the exact output including exit code and test counts before claiming success."
      },
      {
        "title": "Build verification",
        "scenario": "Agent reports successful build",
        "prompt": "Run the full build command and show the exit code. Build success requires exit code 0, not just absence of errors."
      },
      {
        "title": "Regression testing",
        "scenario": "Agent claims regression test added",
        "prompt": "Show the red-green cycle: the test failing before the fix, passing after the fix, and failing again if reverted."
      },
      {
        "title": "Multi-command workflow",
        "scenario": "Complex verification needed",
        "prompt": "Run lint, build, and test commands in sequence. Report which passed, which failed, and provide exit codes for each."
      }
    ],
    "output_examples": [
      {
        "input": "All tests pass now",
        "output": [
          "❌ No verification command run",
          "Ran: npm test",
          "Exit: 0",
          "Result: 47 passed, 0 failed",
          "✅ All tests pass"
        ]
      },
      {
        "input": "Build completed successfully",
        "output": [
          "Ran: npm run build",
          "Exit: 0",
          "Build completed with 0 errors",
          "✅ Build succeeded"
        ]
      },
      {
        "input": "Bug is fixed",
        "output": [
          "Reproduced bug: crash on input 'abc'",
          "Applied fix",
          "Tested with 'abc': now works",
          "Exit: 0",
          "✅ Bug verified fixed"
        ]
      }
    ],
    "best_practices": [
      "Always run verification commands fresh in the current session",
      "Report exact exit codes and output counts, not just pass/fail status",
      "Verify independently when agents report success rather than trusting their claims"
    ],
    "anti_patterns": [
      "Claiming completion based on 'should work' or 'probably passes'",
      "Trusting previous verification runs from earlier sessions",
      "Accepting linter passing as equivalent to build or test success"
    ],
    "faq": [
      {
        "question": "How do I activate this skill?",
        "answer": "Use the skill command or reference it in your prompt when you want to enforce verification discipline."
      },
      {
        "question": "Can I customize verification commands?",
        "answer": "Yes, identify the specific command that proves your claim and run it fresh each time."
      },
      {
        "question": "When should I verify?",
        "answer": "Verify before any completion claim, commit, PR, or report of success status."
      },
      {
        "question": "What counts as valid evidence?",
        "answer": "Command output with exit code and results showing the claim is confirmed."
      },
      {
        "question": "Does this slow down workflows?",
        "answer": "It adds verification steps but catches issues early, preventing downstream problems."
      },
      {
        "question": "How is this different from normal testing?",
        "answer": "It enforces discipline about requiring evidence before making claims, not just running tests."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 100
    }
  ]
}
