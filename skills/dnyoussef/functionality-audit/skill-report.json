{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T03:23:52.502Z",
    "slug": "dnyoussef-functionality-audit",
    "source_url": "https://github.com/DNYoussef/ai-chrome-extension/tree/main/.claude/skills/functionality-audit",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "c36264225df8a6c036cccf3b9356b88ad9565cc9a893be1f09c7303145374775",
    "tree_hash": "9b82e8ca5e3d59bffce7d0b8622ab05e4c3ccd6aa44f43ab9fcdb0e39afd232f"
  },
  "skill": {
    "name": "functionality-audit",
    "description": "Validates that code actually works through sandbox testing, execution verification, and systematic debugging. Use this skill after code generation or modification to ensure functionality is genuine rather than assumed. The skill creates isolated test environments, executes code with realistic inputs, identifies bugs through systematic analysis, and applies best practices to fix issues without breaking existing functionality. This ensures code delivers its intended behavior reliably.",
    "summary": "Validates that code actually works through sandbox testing, execution verification, and systematic d...",
    "icon": "üîç",
    "version": "1.0.0",
    "author": "DNYoussef",
    "license": "MIT",
    "category": "quality",
    "tags": [
      "testing",
      "debugging",
      "sandbox",
      "verification",
      "quality-assurance"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a prompt-based SOP skill containing only documentation. No executable code, scripts, network calls, file system access, or command execution capabilities. All 37 static findings are false positives triggered by documentation keywords. The skill describes testing methodologies that users may apply in their own environments.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 500,
    "audit_model": "claude",
    "audited_at": "2026-01-17T03:23:52.502Z"
  },
  "content": {
    "user_title": "Verify Code Functionality",
    "value_statement": "Code that looks correct is not the same as code that works correctly. This skill validates code through sandbox testing, execution verification, and systematic debugging to ensure functionality is genuine rather than assumed.",
    "seo_keywords": [
      "functionality audit",
      "code testing",
      "sandbox testing",
      "debugging",
      "Claude Code",
      "code verification",
      "quality assurance",
      "execution verification",
      "software testing",
      "bug detection"
    ],
    "actual_capabilities": [
      "Create isolated sandbox environments for safe code testing",
      "Generate comprehensive test cases covering normal and edge cases",
      "Execute code with realistic inputs and monitor behavior",
      "Identify bugs through systematic failure analysis",
      "Apply best practices to fix issues without regressions",
      "Produce structured reports documenting test results"
    ],
    "limitations": [
      "Does not execute code directly - guides users in testing their own code",
      "Requires users to have testing tools installed such as pytest or Jest",
      "Cannot access production systems or external APIs directly",
      "Relies on user-provided test data and expected behavior specifications"
    ],
    "use_cases": [
      {
        "target_user": "Software Developers",
        "title": "Validate Generated Code",
        "description": "Verify code generated by AI actually works before deploying to production environments"
      },
      {
        "target_user": "QA Engineers",
        "title": "Systematic Bug Discovery",
        "description": "Find bugs through systematic testing and debugging workflows with reproducible test cases"
      },
      {
        "target_user": "DevOps Teams",
        "title": "Pre-Deployment Verification",
        "description": "Ensure code changes work correctly in isolated environments matching production conditions"
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick Audit",
        "scenario": "Test a new function",
        "prompt": "Use the functionality-audit skill to test this code. The code should calculate factorials correctly. Test with inputs: 0, 1, 5, 10, and negative numbers."
      },
      {
        "title": "Integration Test",
        "scenario": "Verify multi-file integration",
        "prompt": "Run a functionality audit on the modules in my-project/src. Verify they work together correctly. The system should process user data through all three modules."
      },
      {
        "title": "Bug Investigation",
        "scenario": "Debug failing code",
        "prompt": "The following code is producing incorrect results for valid inputs. Use functionality-audit to reproduce the bug, identify the root cause, and suggest fixes."
      },
      {
        "title": "Regression Prevention",
        "scenario": "Test after changes",
        "prompt": "Run a comprehensive functionality audit on the entire codebase after recent changes. Verify all critical paths still work correctly."
      }
    ],
    "output_examples": [
      {
        "input": "Audit this code that calculates factorials. Test with inputs: 0, 1, 5, 10, and negative numbers.",
        "output": [
          "Test Results Summary: 5 tests executed, 4 passed, 1 failed",
          "Failed Test: Negative input (-5) produced incorrect result",
          "Root Cause: Function does not handle negative inputs properly",
          "Fix Recommendation: Add input validation to raise ValueError for negative numbers",
          "Recommendation: Add test cases for invalid inputs to prevent future regressions"
        ]
      },
      {
        "input": "Test this API client module. It should make GET requests and parse JSON responses correctly.",
        "output": [
          "Test Results: 8 tests for normal operation, 3 tests for error handling",
          "All tests passed successfully",
          "Verified correct JSON parsing for valid responses",
          "Verified appropriate error handling for network failures",
          "Performance within expected range for all request types"
        ]
      },
      {
        "input": "Run functionality audit on this data processing pipeline after the recent refactoring.",
        "output": [
          "Pipeline Audit: 12 test cases executed across 4 stages",
          "Failed: Stage 3 produces incorrect output for empty input arrays",
          "Root Cause: Recent optimization introduced null pointer in edge case handling",
          "Suggested fix available in test file with regression test case"
        ]
      }
    ],
    "best_practices": [
      "Provide clear specifications of expected behavior when invoking the skill",
      "Include representative test data and known edge cases in your request",
      "Review debugging findings before applying fixes to understand root causes",
      "Run functionality audits after any significant code changes or integrations"
    ],
    "anti_patterns": [
      "Assuming code is correct just because it compiles without errors",
      "Skipping testing on complex code because it seems obviously correct",
      "Applying fixes before understanding the root cause of failures",
      "Using only existing tests without adding new test cases for edge cases"
    ],
    "faq": [
      {
        "question": "What AI tools support this skill?",
        "answer": "The functionality-audit skill works with Claude, Codex, and Claude Code. It integrates into quality assurance workflows for these platforms."
      },
      {
        "question": "What types of code can be tested?",
        "answer": "Any executable code in Python, JavaScript, or other languages. Users must have testing frameworks like pytest or Jest installed."
      },
      {
        "question": "Does this skill modify my code?",
        "answer": "No. This skill analyzes code and recommends fixes. Users must apply fixes themselves in their own development environment."
      },
      {
        "question": "Is my data safe during testing?",
        "answer": "Yes. Testing occurs in sandbox environments you control. No data is sent to external services without your explicit setup."
      },
      {
        "question": "What if tests fail repeatedly?",
        "answer": "The skill provides systematic debugging guidance including binary search, hypothesis-driven debugging, and logging techniques."
      },
      {
        "question": "How is this different from static analysis?",
        "answer": "Static analysis checks code structure. Functionality audit actually executes code to verify it produces correct results."
      }
    ]
  },
  "file_structure": [
    {
      "name": "functionality-audit-process.dot",
      "type": "file",
      "path": "functionality-audit-process.dot",
      "lines": 101
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 214
    }
  ]
}
