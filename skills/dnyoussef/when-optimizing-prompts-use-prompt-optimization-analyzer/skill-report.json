{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T19:16:21.575Z",
    "slug": "dnyoussef-when-optimizing-prompts-use-prompt-optimization-analyzer",
    "source_url": "https://github.com/DNYoussef/ai-chrome-extension/tree/main/.claude/skills/meta-tools/when-optimizing-prompts-use-prompt-optimization-analyzer",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "9e200c9100e71eb545f9f9f76edbf4fcf7eb9def7a9f22d5c4ba1b1f1c3e435c",
    "tree_hash": "0ddbf9a3c1c397770bd015e7a5ab91f0cac9b2b040e3a8e23aae05c57be92ef3"
  },
  "skill": {
    "name": "when-optimizing-prompts-use-prompt-optimization-analyzer",
    "description": "Analyzes prompt quality to detect anti-patterns, identify token waste, and provide optimization recommendations for improved clarity and efficiency.",
    "summary": "Analyze and optimize prompts for token efficiency and clarity",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "DNYoussef",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "prompt-engineering",
      "optimization",
      "analysis",
      "diagnostics",
      "token-management",
      "Claude",
      "Claude Code"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill is safe for publication. All 87 static findings are false positives. The skill contains only documentation files (markdown) and a process diagram (GraphViz). The 23 high-severity findings for weak cryptographic algorithms are file extension detections (MD in .md files). The 62 medium-severity findings for shell execution are documentation code examples in markdown blocks, not executable code. No actual security risks exist.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 1955,
    "audit_model": "claude",
    "audited_at": "2026-01-21T19:16:21.575Z",
    "risk_factors": []
  },
  "content": {
    "user_title": "Optimize Prompts for Maximum Token Efficiency",
    "value_statement": "Writing effective prompts is challenging when token budgets are limited and clarity matters. This skill analyzes your prompts to detect redundancy, vague instructions, and anti-patterns, then provides specific optimization recommendations that can reduce token usage by 20-50% while improving clarity.",
    "seo_keywords": [
      "Claude",
      "Codex",
      "Claude Code",
      "prompt optimization",
      "token efficiency",
      "prompt engineering",
      "anti-pattern detection",
      "prompt analysis",
      "AI optimization",
      "token budget management"
    ],
    "actual_capabilities": [
      "Detects token waste through redundancy analysis and filler word identification",
      "Identifies anti-patterns including vague instructions, ambiguous terminology, and conflicting requirements",
      "Analyzes trigger conditions for clarity, specificity, and edge case coverage",
      "Generates before and after comparisons with specific token savings estimates",
      "Provides structured optimization recommendations prioritized by severity and impact",
      "Measures verbosity and calculates redundancy scores for objective assessment"
    ],
    "limitations": [
      "Analysis quality depends on prompt complexity and length provided",
      "Optimization recommendations require manual review and application",
      "Token savings estimates are approximate and may vary based on model tokenization",
      "Does not automatically rewrite prompts, only provides recommendations"
    ],
    "use_cases": [
      {
        "title": "Pre-Publication Skill Review",
        "description": "Analyze new skill prompts before publishing to ensure they meet quality standards and token efficiency targets. Identify and fix anti-patterns that could cause inconsistent responses.",
        "target_user": "Skill developers preparing skills for marketplace publication"
      },
      {
        "title": "Token Budget Compliance",
        "description": "Reduce prompt token usage when approaching context limits or budget constraints. Identify redundant content and consolidate examples to achieve 20-50% token reduction while maintaining functionality.",
        "target_user": "Developers working with token-constrained AI applications"
      },
      {
        "title": "Prompt Library Maintenance",
        "description": "Audit and optimize existing prompt libraries for consistency and efficiency. Detect patterns of vague instructions or missing definitions across multiple prompts and standardize improvements.",
        "target_user": "Teams maintaining large collections of AI prompts and skills"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Prompt Analysis",
        "prompt": "Analyze this prompt for optimization opportunities: [paste your prompt text here]",
        "scenario": "Quick analysis of a single prompt to identify major issues"
      },
      {
        "title": "Token Reduction Focus",
        "prompt": "Analyze this prompt with focus on token reduction. Current token count is [X]. Target is [Y]. Identify redundancy and suggest consolidation: [paste prompt]",
        "scenario": "When you have a specific token budget to meet"
      },
      {
        "title": "Anti-Pattern Detection",
        "prompt": "Review this prompt for anti-patterns including vague instructions, ambiguous terms, and conflicting requirements. Provide specific examples and fixes: [paste prompt]",
        "scenario": "When responses are inconsistent or unclear"
      },
      {
        "title": "Comprehensive Optimization",
        "prompt": "Perform full optimization analysis on this skill prompt including token efficiency, anti-patterns, trigger clarity, and structural improvements. Provide before/after comparison with savings estimate: [paste prompt]",
        "scenario": "Complete audit before publishing or major prompt updates"
      }
    ],
    "output_examples": [
      {
        "input": "Analyze this prompt: You are a code reviewer. Your job is to review code and make sure it's good. You should look at the code and find problems.",
        "output": "Analysis Results:\n\nToken Waste Detected:\n- Redundancy score: 45/100\n- Issues: Phrase you should appears 4 times, vague terms (good, problems) used without definition\n\nAnti-Patterns Found:\n- Vague instruction: make sure it's good (replace with specific criteria)\n- Missing context: What types of problems? What review standards?\n\nRecommendations:\n- Remove repeated you should, use imperative mood\n- Define specific review criteria (logic errors, style compliance, test coverage)\n- Structure output format explicitly\n\nEstimated savings: 46% token reduction possible"
      },
      {
        "input": "Optimize this trigger: when working with code stuff",
        "output": "Trigger Analysis:\n\nIssues Detected:\n- Vague term: stuff (severity: high)\n- Missing explicit condition (no when/if clause structure)\n- Too narrow scope (only 4 words)\n\nRecommendations:\n- Replace stuff with specific entity: when working with source code files or when analyzing code repositories\n- Add specificity: when reviewing pull requests for code quality\n- Consider broader applicability to cover edge cases\n\nImproved trigger: when reviewing code for quality issues or optimization opportunities"
      }
    ],
    "best_practices": [
      "Run analysis before publishing new skills to catch anti-patterns early and ensure quality standards are met",
      "Focus on high-severity issues first, then address medium and low-severity findings in subsequent iterations",
      "Compare before and after metrics to validate that optimizations actually improve clarity without losing essential information"
    ],
    "anti_patterns": [
      "Applying all recommendations blindly without understanding context or validating that changes preserve intended meaning",
      "Over-optimizing for token reduction at the expense of necessary clarity, examples, or error handling instructions",
      "Ignoring domain-specific terminology that appears verbose but is actually precise and necessary for the use case"
    ],
    "faq": [
      {
        "question": "How much token reduction can I expect from using this skill?",
        "answer": "Typical token reduction ranges from 20-50% depending on the original prompt quality. Prompts with high redundancy or vague instructions see the most improvement. Well-written prompts may see smaller gains of 10-20%."
      },
      {
        "question": "Does this skill automatically rewrite my prompts?",
        "answer": "No, this skill analyzes prompts and provides specific recommendations with examples. You review and apply the suggested changes manually to ensure they preserve your intended meaning and requirements."
      },
      {
        "question": "What types of anti-patterns does this skill detect?",
        "answer": "The skill detects vague instructions, ambiguous terminology, conflicting requirements, missing context or definitions, over-specification, redundant phrases, filler words, and unclear trigger conditions."
      },
      {
        "question": "Can I use this skill on prompts written in languages other than English?",
        "answer": "The current version is optimized for English language prompts. Analysis of non-English prompts may produce less accurate results for filler words and phrase detection."
      },
      {
        "question": "How often should I re-analyze my prompts?",
        "answer": "Analyze prompts before initial publication and whenever you make significant updates. For production skills, monthly maintenance reviews help catch quality drift and identify new optimization opportunities."
      },
      {
        "question": "What is the difference between redundancy score and verbosity score?",
        "answer": "Redundancy score measures repeated phrases and concepts that could be consolidated. Verbosity score measures average word length and unnecessary complexity. Both contribute to token waste but are optimized differently."
      }
    ]
  },
  "file_structure": [
    {
      "name": "process-diagram.gv",
      "type": "file",
      "path": "process-diagram.gv",
      "lines": 163
    },
    {
      "name": "PROCESS.md",
      "type": "file",
      "path": "PROCESS.md",
      "lines": 297
    },
    {
      "name": "README.md",
      "type": "file",
      "path": "README.md",
      "lines": 110
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 477
    }
  ]
}
