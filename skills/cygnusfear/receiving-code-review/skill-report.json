{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T23:46:28.788Z",
    "slug": "cygnusfear-receiving-code-review",
    "source_url": "https://github.com/Cygnusfear/claude-stuff/tree/main/skills/receiving-code-review",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "2f6114025609bed0dcf13a9b4a583f625c9f27b91052c9c90a8833af9a83c804",
    "tree_hash": "a89915730edead4ad8890015fdf1735c29467ae497148f4f8229cad91b5d7441"
  },
  "skill": {
    "name": "receiving-code-review",
    "description": "Use when receiving code review feedback, before implementing suggestions, especially if feedback seems unclear or technically questionable - requires technical rigor and verification, not performative agreement or blind implementation",
    "summary": "Use when receiving code review feedback, before implementing suggestions, especially if feedback see...",
    "icon": "üîç",
    "version": "1.0.0",
    "author": "Cygnusfear",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "code review",
      "code quality",
      "technical communication",
      "feedback handling"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill contains only markdown documentation with behavioral guidelines for code review response patterns. No executable code, no file system access, no network calls, no command execution. All 33 static findings are FALSE POSITIVES caused by the scanner misidentifying markdown syntax and plain text as security-relevant patterns.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 16,
            "line_end": 25
          },
          {
            "file": "SKILL.md",
            "line_start": 25,
            "line_end": 42
          },
          {
            "file": "SKILL.md",
            "line_start": 42,
            "line_end": 48
          },
          {
            "file": "SKILL.md",
            "line_start": 48,
            "line_end": 51
          },
          {
            "file": "SKILL.md",
            "line_start": 51,
            "line_end": 57
          },
          {
            "file": "SKILL.md",
            "line_start": 57,
            "line_end": 68
          },
          {
            "file": "SKILL.md",
            "line_start": 68,
            "line_end": 84
          },
          {
            "file": "SKILL.md",
            "line_start": 84,
            "line_end": 90
          },
          {
            "file": "SKILL.md",
            "line_start": 90,
            "line_end": 96
          },
          {
            "file": "SKILL.md",
            "line_start": 96,
            "line_end": 102
          },
          {
            "file": "SKILL.md",
            "line_start": 102,
            "line_end": 111
          },
          {
            "file": "SKILL.md",
            "line_start": 111,
            "line_end": 134
          },
          {
            "file": "SKILL.md",
            "line_start": 134,
            "line_end": 144
          },
          {
            "file": "SKILL.md",
            "line_start": 144,
            "line_end": 153
          },
          {
            "file": "SKILL.md",
            "line_start": 153,
            "line_end": 160
          },
          {
            "file": "SKILL.md",
            "line_start": 160,
            "line_end": 179
          },
          {
            "file": "SKILL.md",
            "line_start": 179,
            "line_end": 182
          },
          {
            "file": "SKILL.md",
            "line_start": 182,
            "line_end": 185
          },
          {
            "file": "SKILL.md",
            "line_start": 185,
            "line_end": 188
          },
          {
            "file": "SKILL.md",
            "line_start": 188,
            "line_end": 191
          },
          {
            "file": "SKILL.md",
            "line_start": 191,
            "line_end": 194
          },
          {
            "file": "SKILL.md",
            "line_start": 194,
            "line_end": 197
          },
          {
            "file": "SKILL.md",
            "line_start": 197,
            "line_end": 201
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 384,
    "audit_model": "claude",
    "audited_at": "2026-01-16T23:46:28.788Z"
  },
  "content": {
    "user_title": "Evaluate code review feedback with technical rigor",
    "value_statement": "Code review feedback often requires verification before implementation. This skill ensures Claude evaluates suggestions technically, asks clarifying questions when needed, and pushes back when feedback is incorrect or unclear.",
    "seo_keywords": [
      "code review",
      "Claude Code code review",
      "code review feedback",
      "technical verification",
      "code review best practices",
      "handling code review",
      "software development feedback",
      "code quality review",
      "programming feedback",
      "developer communication"
    ],
    "actual_capabilities": [
      "Evaluates code review feedback for technical correctness before implementing",
      "Identifies unclear or technically questionable suggestions requiring clarification",
      "Verifies suggestions against codebase reality and existing functionality",
      "Pushes back on incorrect feedback with technical reasoning",
      "Prevents blind implementation of unchecked suggestions"
    ],
    "limitations": [
      "Does not modify code or execute changes automatically",
      "Does not access external code review tools or platforms",
      "Does not provide automated code analysis or linting"
    ],
    "use_cases": [
      {
        "target_user": "Software Engineers",
        "title": "Verify review suggestions",
        "description": "Evaluate external code review feedback before implementation to prevent introducing bugs or regressions"
      },
      {
        "target_user": "Technical Leads",
        "title": "Maintain code quality",
        "description": "Ensure code review feedback is technically sound and appropriate for the specific codebase context"
      },
      {
        "target_user": "Development Teams",
        "title": "Improve code review process",
        "description": "Establish consistent standards for evaluating and responding to code review feedback"
      }
    ],
    "prompt_templates": [
      {
        "title": "Evaluate suggestion",
        "scenario": "Reviewing external feedback",
        "prompt": "Use the receiving-code-review skill to evaluate this suggestion: [paste feedback]. Check if it is technically correct, breaks existing functionality, or conflicts with project decisions."
      },
      {
        "title": "Request clarification",
        "scenario": "Unclear feedback received",
        "prompt": "I received code review feedback but do not fully understand some items. Use receiving-code-review to help me identify what needs clarification and how to ask effectively."
      },
      {
        "title": "Push back correctly",
        "scenario": "Incorrect suggestion identified",
        "prompt": "I believe a code review suggestion is technically incorrect. Use receiving-code-review to help me construct a technical, evidence-based pushback response."
      },
      {
        "title": "Verify before implementing",
        "scenario": "Ready to implement feedback",
        "prompt": "I want to implement code review feedback but need to verify it first. Use receiving-code-review to guide me through the verification checklist."
      }
    ],
    "output_examples": [
      {
        "input": "Reviewer suggests removing legacy code and adding proper metrics tracking with database, date filters, and CSV export",
        "output": [
          "Suggestion requires YAGNI check - grep codebase for actual usage",
          "Current implementation: endpoint exists but is unused",
          "Recommended response: 'This endpoint has no callers. Remove it (YAGNI)? Or is there usage I'm missing?'",
          "Do not implement database metrics until actual use cases are identified"
        ]
      },
      {
        "input": "Code review feedback says: Fix items 1-6, but I only understand 1, 2, 3, and 6. Items 4 and 5 are unclear.",
        "output": [
          "Do NOT implement items 1, 2, 3, 6 yet - unclear items may be related",
          "Request clarification on items 4 and 5 before proceeding",
          "Recommended response: 'I understand items 1, 2, 3, 6. Need clarification on 4 and 5 before implementing.'"
        ]
      }
    ],
    "best_practices": [
      "Always verify code review feedback against the actual codebase before implementation",
      "Ask clarifying questions when any item is unclear rather than guessing",
      "Use technical reasoning when pushing back, not defensiveness or apologies"
    ],
    "anti_patterns": [
      "Performative agreement like 'You're absolutely right!' or 'Great point!'",
      "Blind implementation of suggestions without technical verification",
      "Batch implementing multiple items without testing each one individually"
    ],
    "faq": [
      {
        "question": "What AI tools support this skill?",
        "answer": "Works with Claude, Codex, and Claude Code for evaluating code review feedback."
      },
      {
        "question": "Does this skill execute code changes?",
        "answer": "No. This skill provides behavioral guidelines for evaluation only. It does not modify code."
      },
      {
        "question": "How is this different from normal Claude behavior?",
        "answer": "Adds systematic verification steps, clarifies when to push back, and avoids performative agreement."
      },
      {
        "question": "What data does this skill access?",
        "answer": "No data access. It is a prompt-based skill providing response guidelines only."
      },
      {
        "question": "What if feedback seems correct but I cannot verify it?",
        "answer": "State the limitation: 'I cannot verify this without [X]. Should I investigate, ask, or proceed?'"
      },
      {
        "question": "How does this compare to a linter or code analyzer?",
        "answer": "This skill guides behavioral response to human feedback, not automated code analysis or error checking."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 210
    }
  ]
}
