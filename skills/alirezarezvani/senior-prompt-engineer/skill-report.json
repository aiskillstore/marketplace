{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T17:02:18.753Z",
    "slug": "alirezarezvani-senior-prompt-engineer",
    "source_url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/senior-prompt-engineer",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "4a167292edfd9290ed49048fa86b79c6b8c36da6b26d8233891bd1184b341bdb",
    "tree_hash": "a6b60b2e0b041ad03d78dc17d0ec5d873982ced1022f9b2d7ed206a351a42f94"
  },
  "skill": {
    "name": "senior-prompt-engineer",
    "description": "World-class prompt engineering skill for LLM optimization, prompt patterns, structured outputs, and AI product development. Expertise in Claude, GPT-4, prompt design patterns, few-shot learning, chain-of-thought, and AI evaluation. Includes RAG optimization, agent design, and LLM system architecture. Use when building AI products, optimizing LLM performance, designing agentic systems, or implementing advanced prompting techniques.",
    "summary": "World-class prompt engineering skill for LLM optimization, prompt patterns, structured outputs, and ...",
    "icon": "ðŸ§ ",
    "version": "1.0.0",
    "author": "alirezarezvani",
    "license": "MIT",
    "category": "data",
    "tags": [
      "prompt-engineering",
      "llm",
      "ai-optimization",
      "agent-design",
      "rag"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 108 static findings are FALSE POSITIVES caused by scanner misinterpretation. The scanner flagged the word 'algorithm' in documentation as cryptographic code (no crypto exists), markdown code blocks as shell execution (documentation only), and standard software terms as reconnaissance (legitimate engineering content). The skill contains benign documentation and Python CLI tools using standard libraries with no network operations, cryptographic functions, or dangerous patterns.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/agent_orchestrator.py",
            "line_start": 70,
            "line_end": 72
          },
          {
            "file": "scripts/prompt_optimizer.py",
            "line_start": 70,
            "line_end": 72
          },
          {
            "file": "scripts/rag_evaluator.py",
            "line_start": 70,
            "line_end": 72
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 9,
    "total_lines": 1430,
    "audit_model": "claude",
    "audited_at": "2026-01-16T17:02:18.753Z"
  },
  "content": {
    "user_title": "Optimize LLM Prompts for Claude and GPT-4",
    "value_statement": "Generic prompts produce inconsistent AI responses. This skill provides proven patterns and optimization tools for production-grade LLM interactions. Transform vague requests into precise, reliable outputs using chain-of-thought, few-shot learning, and structured prompting techniques.",
    "seo_keywords": [
      "prompt engineering",
      "Claude Code",
      "LLM optimization",
      "chain-of-thought",
      "few-shot learning",
      "RAG systems",
      "agent design",
      "AI evaluation",
      "structured outputs",
      "prompt patterns"
    ],
    "actual_capabilities": [
      "Design and optimize prompts for Claude, GPT-4, and other LLMs using proven patterns",
      "Implement chain-of-thought reasoning and few-shot learning techniques",
      "Build RAG (Retrieval-Augmented Generation) systems with vector databases",
      "Create evaluation frameworks to measure LLM performance and quality",
      "Design agentic AI systems with tool calling and state management",
      "Generate structured outputs (JSON, YAML, markdown) from LLMs"
    ],
    "limitations": [
      "Does not provide access to external LLM APIs or compute resources",
      "Requires user to supply their own API keys and authentication",
      "Python scripts are CLI tools that require manual execution",
      "Does not include pre-trained models or fine-tuning capabilities"
    ],
    "use_cases": [
      {
        "target_user": "AI Engineers",
        "title": "Build Production AI Systems",
        "description": "Design and deploy reliable AI products with evaluation frameworks, agent patterns, and structured outputs."
      },
      {
        "target_user": "ML Engineers",
        "title": "Optimize LLM Performance",
        "description": "Improve model outputs through prompt engineering, few-shot examples, and chain-of-thought techniques."
      },
      {
        "target_user": "Product Managers",
        "title": "Create AI-Powered Features",
        "description": "Translate business requirements into effective AI prompts and agent workflows for your products."
      }
    ],
    "prompt_templates": [
      {
        "title": "Chain-of-Thought Prompt",
        "scenario": "Complex reasoning tasks",
        "prompt": "Solve this problem step by step. First, identify the key information. Second, break it into smaller parts. Third, analyze each part. Fourth, combine findings into a final answer."
      },
      {
        "title": "Few-Shot Example",
        "scenario": "Consistent output formatting",
        "prompt": "Classify the sentiment: \"Great product\" â†’ Positive. \"Poor service\" â†’ Negative. \"Decent experience\" â†’ Neutral. \"Amazing quality\" â†’"
      },
      {
        "title": "Structured Output Request",
        "scenario": "JSON/YAML data extraction",
        "prompt": "Extract data in JSON format: {\"name\": \"\", \"category\": \"\", \"sentiment\": \"\", \"confidence\": 0.0}. Return only valid JSON with no additional text."
      },
      {
        "title": "RAG Query Pattern",
        "scenario": "Context-aware responses",
        "prompt": "Use the following context to answer. If the context contains the answer, cite sources. If not, say \"I don't have enough information.\" Context: {retrieved_docs}. Question: {user_query}"
      }
    ],
    "output_examples": [
      {
        "input": "Write a prompt for summarizing customer feedback into categories",
        "output": [
          "Category labels: Bug Report, Feature Request, Compliment, Complaint, Question",
          "Each summary includes sentiment (positive/negative/neutral)",
          "Priority level assigned based on severity",
          "Direct quotes included for context"
        ]
      },
      {
        "input": "Create a system prompt for a coding assistant",
        "output": [
          "Language: Python (primary), JavaScript, TypeScript, Go, Rust",
          "Response format: Explanation first, then code block",
          "Code includes comments explaining complex logic",
          "Tests provided for critical functions",
          "Security considerations noted"
        ]
      },
      {
        "input": "Design prompts for a customer support AI agent",
        "output": [
          "Escalation rules for complex issues to human agents",
          "Response tone: empathetic, professional, solution-focused",
          "Knowledge base references included in responses",
          "Satisfaction follow-up questions after resolution"
        ]
      }
    ],
    "best_practices": [
      "Start with clear, specific instructions and add constraints progressively based on testing results",
      "Use consistent formatting and delimiters to separate instructions from examples from data",
      "Test prompts with diverse edge cases and adversarial inputs before production deployment"
    ],
    "anti_patterns": [
      "Avoid vague instructions like 'be helpful' - use specific behavioral constraints instead",
      "Do not include sensitive information, credentials, or PII in prompt templates",
      "Avoid over-relying on system prompts - implement guardrails and input validation separately"
    ],
    "faq": [
      {
        "question": "What LLM providers does this skill support?",
        "answer": "The patterns work with any instruction-tuned LLM including Claude, GPT-4, Gemini, and open-source models."
      },
      {
        "question": "Do I need coding experience to use this skill?",
        "answer": "Basic familiarity with prompt engineering concepts helps. Python scripts run via command line."
      },
      {
        "question": "How is this skill different from prompt libraries?",
        "answer": "Provides frameworks and methodologies, not just templates. Focuses on building production systems."
      },
      {
        "question": "Can I use these prompts in commercial products?",
        "answer": "Yes. All patterns are general techniques. Adapt to your specific use case and model."
      },
      {
        "question": "How do I evaluate prompt effectiveness?",
        "answer": "Use the evaluation framework to test consistency, accuracy, and edge case handling with test suites."
      },
      {
        "question": "What token optimization techniques are included?",
        "answer": "Templates focus on concise instruction wording, efficient few-shot examples, and context compression."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "agentic_system_design.md",
          "type": "file",
          "path": "references/agentic_system_design.md",
          "lines": 81
        },
        {
          "name": "llm_evaluation_frameworks.md",
          "type": "file",
          "path": "references/llm_evaluation_frameworks.md",
          "lines": 81
        },
        {
          "name": "prompt_engineering_patterns.md",
          "type": "file",
          "path": "references/prompt_engineering_patterns.md",
          "lines": 81
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "agent_orchestrator.py",
          "type": "file",
          "path": "scripts/agent_orchestrator.py",
          "lines": 101
        },
        {
          "name": "prompt_optimizer.py",
          "type": "file",
          "path": "scripts/prompt_optimizer.py",
          "lines": 101
        },
        {
          "name": "rag_evaluator.py",
          "type": "file",
          "path": "scripts/rag_evaluator.py",
          "lines": 101
        }
      ]
    },
    {
      "name": "evaluation_output.json",
      "type": "file",
      "path": "evaluation_output.json",
      "lines": 389
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 227
    }
  ]
}
