{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T17:00:25.401Z",
    "slug": "alirezarezvani-senior-ml-engineer",
    "source_url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/senior-ml-engineer",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "05955e286d61b9d7729d3feb5e628661c72fd7e4b67f5e0a04d65878757eb2e0",
    "tree_hash": "680100d5ed47d1fc2378d50e23641812d975cc8dc59a5b6db1979a3b267f71f0"
  },
  "skill": {
    "name": "senior-ml-engineer",
    "description": "World-class ML engineering skill for productionizing ML models, MLOps, and building scalable ML systems. Expertise in PyTorch, TensorFlow, model deployment, feature stores, model monitoring, and ML infrastructure. Includes LLM integration, fine-tuning, RAG systems, and agentic AI. Use when deploying ML models, building ML platforms, implementing MLOps, or integrating LLMs into production systems.",
    "summary": "World-class ML engineering skill for productionizing ML models, MLOps, and building scalable ML syst...",
    "icon": "ðŸ¤–",
    "version": "1.0.0",
    "author": "alirezarezvani",
    "license": "MIT",
    "category": "data",
    "tags": [
      "mlops",
      "machine-learning",
      "model-deployment",
      "llm"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 78 static findings evaluated as false positives. The static scanner incorrectly flagged documentation text (headings, bullet points) containing common English words as 'weak cryptographic algorithm' patterns. Python scripts contain only standard library imports and logging. Bash code blocks in SKILL.md are documentation examples, not shell execution. No malicious code exists in this skill.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 14,
            "line_end": 23
          },
          {
            "file": "SKILL.md",
            "line_start": 23,
            "line_end": 54
          },
          {
            "file": "SKILL.md",
            "line_start": 54,
            "line_end": 65
          },
          {
            "file": "SKILL.md",
            "line_start": 65,
            "line_end": 75
          },
          {
            "file": "SKILL.md",
            "line_start": 75,
            "line_end": 167
          },
          {
            "file": "SKILL.md",
            "line_start": 167,
            "line_end": 185
          },
          {
            "file": "SKILL.md",
            "line_start": 185,
            "line_end": 189
          },
          {
            "file": "SKILL.md",
            "line_start": 189,
            "line_end": 190
          },
          {
            "file": "SKILL.md",
            "line_start": 190,
            "line_end": 191
          },
          {
            "file": "SKILL.md",
            "line_start": 191,
            "line_end": 192
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 9,
    "total_lines": 1614,
    "audit_model": "claude",
    "audited_at": "2026-01-16T17:00:25.401Z"
  },
  "content": {
    "user_title": "Deploy Production ML Models",
    "value_statement": "Building ML pipelines requires expertise in model deployment, monitoring, and MLOps practices. This skill provides production-ready tools for deploying ML models, integrating LLMs, and building scalable RAG systems with enterprise-grade reliability.",
    "seo_keywords": [
      "mlops",
      "machine learning",
      "model deployment",
      "llm integration",
      "rag systems",
      "ml pipeline",
      "claude code",
      "claude",
      "codex",
      "tensorflow"
    ],
    "actual_capabilities": [
      "Deploy ML models to production with containerized serving infrastructure",
      "Build RAG systems for LLM-powered applications with vector database integration",
      "Implement MLOps pipelines with MLflow, monitoring, and automated retraining",
      "Create production monitoring suites for model drift detection and performance tracking",
      "Design scalable ML architectures with feature stores and distributed computing"
    ],
    "limitations": [
      "Does not include pre-trained models - requires user-supplied trained models",
      "No model training capabilities - focused on deployment and operations",
      "Does not replace domain expertise for model selection and architecture design"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Deploy models to production",
        "description": "Package and deploy trained ML models with Docker, Kubernetes, and REST API endpoints"
      },
      {
        "target_user": "Data Scientists",
        "title": "Build RAG applications",
        "description": "Create retrieval-augmented generation systems using LlamaIndex and LangChain"
      },
      {
        "target_user": "DevOps Teams",
        "title": "Implement MLOps",
        "description": "Set up model versioning, monitoring, and automated retraining pipelines with MLflow"
      }
    ],
    "prompt_templates": [
      {
        "title": "Deploy model",
        "scenario": "Deploy a trained PyTorch model",
        "prompt": "Help me deploy my trained PyTorch classification model to production. I need a REST API endpoint, Docker container, and Kubernetes deployment configuration. Include monitoring for model latency and accuracy drift."
      },
      {
        "title": "Build RAG system",
        "scenario": "Create a RAG pipeline",
        "prompt": "Build a RAG system for my documentation. I have PDF and markdown files. Use a vector database, implement chunking strategies, and create a query interface that works with my internal LLM API."
      },
      {
        "title": "MLOps setup",
        "scenario": "Set up ML pipeline",
        "prompt": "Set up MLOps for my machine learning team. We need MLflow for model versioning, automated training pipelines triggered by data changes, and alerts for model performance degradation."
      },
      {
        "title": "Advanced LLM integration",
        "scenario": "Enterprise LLM deployment",
        "prompt": "Design an enterprise LLM integration architecture. Include prompt caching, rate limiting, cost tracking by team, and integration with our existing authentication system. Support both OpenAI and self-hosted models."
      }
    ],
    "output_examples": [
      {
        "input": "Help me deploy my trained TensorFlow model for real-time inference with sub-50ms latency",
        "output": [
          "Containerize the model using Docker with TensorFlow Serving",
          "Create Kubernetes deployment with HPA for auto-scaling",
          "Implement batch inference for throughput optimization",
          "Add Prometheus metrics for latency, throughput, and error rates",
          "Configure load balancer with health checks and circuit breakers"
        ]
      },
      {
        "input": "Build a RAG system that queries my internal knowledge base with citation support",
        "output": [
          "Set up embedding model and vector database (Pinecone or Weaviate)",
          "Implement document chunking with overlap for context preservation",
          "Create hybrid search combining semantic and keyword matching",
          "Add source citation and confidence scoring to responses",
          "Implement re-ranking for improved result quality"
        ]
      }
    ],
    "best_practices": [
      "Design for 10x scale from day one with horizontal scaling architecture",
      "Implement comprehensive monitoring including latency, drift detection, and data quality checks",
      "Use feature stores to ensure consistency between training and inference"
    ],
    "anti_patterns": [
      "Deploying models without version control or rollback capabilities",
      "Skipping model drift detection until production issues arise",
      "Hardcoding API keys or credentials instead of using secrets management"
    ],
    "faq": [
      {
        "question": "What ML frameworks does this skill support?",
        "answer": "PyTorch, TensorFlow, Scikit-learn, and XGBoost are fully supported with deployment tools."
      },
      {
        "question": "Can I use this for LLM fine-tuning?",
        "answer": "The skill focuses on deployment and integration. For fine-tuning, use Claude's codex capabilities."
      },
      {
        "question": "Does this include cloud credentials?",
        "answer": "No. You provide your own AWS, GCP, or Azure credentials for deployment."
      },
      {
        "question": "What monitoring tools are integrated?",
        "answer": "MLflow, Weights & Biases, and Prometheus for metrics, plus custom dashboard templates."
      },
      {
        "question": "Can I deploy on-premises?",
        "answer": "Yes. Docker and Kubernetes configurations work on any K8s cluster or container runtime."
      },
      {
        "question": "How do I handle model updates?",
        "answer": "Use feature flags and canary deployments for safe rollouts with quick rollback capability."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "llm_integration_guide.md",
          "type": "file",
          "path": "references/llm_integration_guide.md",
          "lines": 81
        },
        {
          "name": "mlops_production_patterns.md",
          "type": "file",
          "path": "references/mlops_production_patterns.md",
          "lines": 81
        },
        {
          "name": "rag_system_architecture.md",
          "type": "file",
          "path": "references/rag_system_architecture.md",
          "lines": 81
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "ml_monitoring_suite.py",
          "type": "file",
          "path": "scripts/ml_monitoring_suite.py",
          "lines": 101
        },
        {
          "name": "model_deployment_pipeline.py",
          "type": "file",
          "path": "scripts/model_deployment_pipeline.py",
          "lines": 101
        },
        {
          "name": "rag_system_builder.py",
          "type": "file",
          "path": "scripts/rag_system_builder.py",
          "lines": 101
        }
      ]
    },
    {
      "name": "marketplace_content.json",
      "type": "file",
      "path": "marketplace_content.json",
      "lines": 360
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 227
    }
  ]
}
