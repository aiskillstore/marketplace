{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T12:23:50.357Z",
    "slug": "21pounder-web-scrape",
    "source_url": "https://github.com/21pounder/terminalAgent/tree/main/deepresearch/.claude/skills/web-scrape",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "8e2cbc33eae0d5a6cc4022b4af72d3a8097b28abcab4dc746478abd24fb2e1af",
    "tree_hash": "58d046c9b2f50e1acd69a968b7a68f298d60dd4224a081b29e2ae18cde4ece5e"
  },
  "skill": {
    "name": "web-scrape",
    "description": "Intelligent web scraper with content extraction, multiple output formats, and error handling",
    "summary": "Intelligent web scraper with content extraction, multiple output formats, and error handling",
    "icon": "üï∏Ô∏è",
    "version": "3.0.0",
    "author": "21pounder",
    "license": "MIT",
    "category": "research",
    "tags": [
      "web scraping",
      "content extraction",
      "browser automation",
      "markdown"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate web scraping skill. 68 of 69 static findings are false positives: the scanner misinterpreted Cheerio selector syntax as shell commands, markdown formatting as cryptographic patterns, and documentation URLs as exfiltration targets. The single true positive (network access) is the intended functionality for a web scraper. No malicious intent, command injection, or data exfiltration patterns exist.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 35,
            "line_end": 36
          },
          {
            "file": "SKILL.md",
            "line_start": 23,
            "line_end": 25
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [
      {
        "finding": "Network access via MCP Playwright",
        "description": "Skill makes HTTP requests to scrape URLs",
        "file": "SKILL.md",
        "lines": "35-36, 44-51"
      }
    ],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 498,
    "audit_model": "claude",
    "audited_at": "2026-01-16T12:23:50.357Z"
  },
  "content": {
    "user_title": "Extract clean content from any webpage",
    "value_statement": "Web scraping is time-consuming and error-prone when done manually. This skill uses intelligent content extraction to pull clean, structured content from any URL in seconds. It handles dynamic pages, removes noise like ads and navigation, and outputs in markdown, JSON, or plain text.",
    "seo_keywords": [
      "web scraping",
      "content extraction",
      "Claude web scraper",
      "markdown converter",
      "browser automation",
      "playwright scraping",
      "HTML to markdown",
      "web data extraction",
      "Claude Code web scraping",
      "AI web research"
    ],
    "actual_capabilities": [
      "Navigate to any URL and extract main content using Playwright browser automation",
      "Remove noise elements like ads, navigation, footers, and comments automatically",
      "Output content in markdown, JSON, or plain text formats",
      "Handle dynamic pages with scroll-to-load and click-to-reveal interactions",
      "Take full-page screenshots of scraped content",
      "Detect and handle errors including CAPTCHAs, paywalls, and access restrictions"
    ],
    "limitations": [
      "Cannot bypass authentication or login-protected content",
      "Cannot execute JavaScript from the target page (only safe scroll scripts)",
      "Cannot scrape sites that actively block automated access",
      "Cannot submit forms or interact with login pages"
    ],
    "use_cases": [
      {
        "target_user": "Researchers",
        "title": "Research data gathering",
        "description": "Extract article content, documentation, and research papers from multiple sources into structured notes"
      },
      {
        "target_user": "Developers",
        "title": "API documentation capture",
        "description": "Save API docs and technical content for offline reference or integration work"
      },
      {
        "target_user": "Content creators",
        "title": "Content aggregation",
        "description": "Collect and curate content from multiple web sources for analysis or inspiration"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic page scrape",
        "scenario": "Extract an article",
        "prompt": "Scrape https://example.com/article and return the content as markdown"
      },
      {
        "title": "Product data extraction",
        "scenario": "Extract product details",
        "prompt": "Extract product information from https://shop.example.com/product as JSON with title, price, and description"
      },
      {
        "title": "Multi-page documentation",
        "scenario": "Capture documentation site",
        "prompt": "Scrape the documentation at https://docs.example.com/getting-started. Check if there are multiple pages and ask if you should continue"
      },
      {
        "title": "Visual capture",
        "scenario": "Take a screenshot",
        "prompt": "Navigate to https://example.com and take a full-page screenshot saved as example_page.png"
      }
    ],
    "output_examples": [
      {
        "input": "Scrape https://example.com/blog/post-title as markdown",
        "output": [
          "# How to Build a REST API",
          "**Source:** https://example.com/blog/post-title",
          "**Date:** January 10, 2025",
          "**Author:** Jane Developer",
          "---",
          "REST APIs are the backbone of modern web applications...",
          "## Getting Started",
          "First, install your preferred HTTP client..."
        ]
      },
      {
        "input": "Extract product info from https://shop.example.com/widget as JSON",
        "output": [
          "url: https://shop.example.com/widget",
          "title: Super Widget 3000",
          "type: product",
          "content: { main: 'The ultimate widget for all your needs...', metadata: { price: '$29.99', in_stock: true } }"
        ]
      }
    ],
    "best_practices": [
      "Start with the simplest scrape command and add options like --scroll or --screenshot only when needed",
      "Review the extracted content for accuracy, especially for complex pages with dynamic elements",
      "Respect website terms of service and robots.txt when scraping content"
    ],
    "anti_patterns": [
      "Do not use this skill to scrape login-protected or subscription-only content without authorization",
      "Do not attempt to bypass CAPTCHAs or access restrictions‚Äîthis will fail and waste resources",
      "Do not scrape high-frequency or real-time data without appropriate rate limiting"
    ],
    "faq": [
      {
        "question": "What platforms is this skill compatible with?",
        "answer": "Works with Claude, Codex, and Claude Code when Playwright MCP is configured."
      },
      {
        "question": "What are the rate limits?",
        "answer": "Limits depend on your Playwright MCP server configuration and target website policies."
      },
      {
        "question": "Can I integrate this with other tools?",
        "answer": "Yes, use the JSON output format for structured data that integrates with workflows."
      },
      {
        "question": "Is my scraping activity tracked?",
        "answer": "Activity stays local‚Äîonly your Playwright instance and target server see the requests."
      },
      {
        "question": "Why did my scrape fail?",
        "answer": "Common causes include timeout, 403/404 errors, CAPTCHAs, or JavaScript-heavy pages that need scroll options."
      },
      {
        "question": "How is this different from curl or wget?",
        "answer": "This skill renders JavaScript, handles dynamic content, extracts clean text, and provides structured outputs automatically."
      }
    ]
  },
  "file_structure": [
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "html_clean.js",
          "type": "file",
          "path": "scripts/html_clean.js",
          "lines": 47
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 259
    }
  ]
}
