{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T07:05:49.355Z",
    "slug": "ruvnet-agentdb-learning-plugins",
    "source_url": "https://github.com/ruvnet/claude-flow/tree/main/.claude/skills/agentdb-learning",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "c3c2e7183166863f0ced076a057c8104d1c9c280f0427bc84299955e8e5acee9",
    "tree_hash": "551b415ff130265c07077065e5592177e637d2a71c6df2a5378cb5e96e8e5a96"
  },
  "skill": {
    "name": "agentdb-learning-plugins",
    "description": "Create and train AI learning plugins with AgentDB's 9 reinforcement learning algorithms. Includes Decision Transformer, Q-Learning, SARSA, Actor-Critic, and more. Use when building self-learning agents, implementing RL, or optimizing agent behavior through experience.",
    "summary": "Create and train AI learning plugins with AgentDB's 9 reinforcement learning algorithms. Includes De...",
    "icon": "ðŸ§ ",
    "version": "1.0.0",
    "author": "ruvnet",
    "license": "MIT",
    "category": "data",
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "ai-agents",
      "neural-networks",
      "training"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill contains only documentation (SKILL.md) with code examples shown in markdown. No executable code, scripts, or binaries are present. All 76 static findings are false positives caused by pattern matching against documentation content, markdown code fences, and algorithm names.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          },
          {
            "file": "SKILL.md",
            "line_start": 537,
            "line_end": 537
          },
          {
            "file": "SKILL.md",
            "line_start": 539,
            "line_end": 539
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 26,
            "line_end": 38
          },
          {
            "file": "SKILL.md",
            "line_start": 38,
            "line_end": 42
          },
          {
            "file": "SKILL.md",
            "line_start": 42,
            "line_end": 52
          },
          {
            "file": "SKILL.md",
            "line_start": 52,
            "line_end": 56
          },
          {
            "file": "SKILL.md",
            "line_start": 56,
            "line_end": 64
          },
          {
            "file": "SKILL.md",
            "line_start": 64,
            "line_end": 70
          },
          {
            "file": "SKILL.md",
            "line_start": 70,
            "line_end": 111
          },
          {
            "file": "SKILL.md",
            "line_start": 111,
            "line_end": 123
          },
          {
            "file": "SKILL.md",
            "line_start": 123,
            "line_end": 125
          },
          {
            "file": "SKILL.md",
            "line_start": 125,
            "line_end": 134
          },
          {
            "file": "SKILL.md",
            "line_start": 134,
            "line_end": 143
          },
          {
            "file": "SKILL.md",
            "line_start": 143,
            "line_end": 151
          },
          {
            "file": "SKILL.md",
            "line_start": 151,
            "line_end": 153
          },
          {
            "file": "SKILL.md",
            "line_start": 153,
            "line_end": 162
          },
          {
            "file": "SKILL.md",
            "line_start": 162,
            "line_end": 170
          },
          {
            "file": "SKILL.md",
            "line_start": 170,
            "line_end": 178
          },
          {
            "file": "SKILL.md",
            "line_start": 178,
            "line_end": 180
          },
          {
            "file": "SKILL.md",
            "line_start": 180,
            "line_end": 188
          },
          {
            "file": "SKILL.md",
            "line_start": 188,
            "line_end": 195
          },
          {
            "file": "SKILL.md",
            "line_start": 195,
            "line_end": 203
          },
          {
            "file": "SKILL.md",
            "line_start": 203,
            "line_end": 205
          },
          {
            "file": "SKILL.md",
            "line_start": 205,
            "line_end": 213
          },
          {
            "file": "SKILL.md",
            "line_start": 213,
            "line_end": 221
          },
          {
            "file": "SKILL.md",
            "line_start": 221,
            "line_end": 289
          },
          {
            "file": "SKILL.md",
            "line_start": 289,
            "line_end": 317
          },
          {
            "file": "SKILL.md",
            "line_start": 317,
            "line_end": 321
          },
          {
            "file": "SKILL.md",
            "line_start": 321,
            "line_end": 337
          },
          {
            "file": "SKILL.md",
            "line_start": 337,
            "line_end": 341
          },
          {
            "file": "SKILL.md",
            "line_start": 341,
            "line_end": 356
          },
          {
            "file": "SKILL.md",
            "line_start": 356,
            "line_end": 364
          },
          {
            "file": "SKILL.md",
            "line_start": 364,
            "line_end": 377
          },
          {
            "file": "SKILL.md",
            "line_start": 377,
            "line_end": 381
          },
          {
            "file": "SKILL.md",
            "line_start": 381,
            "line_end": 395
          },
          {
            "file": "SKILL.md",
            "line_start": 395,
            "line_end": 399
          },
          {
            "file": "SKILL.md",
            "line_start": 399,
            "line_end": 406
          },
          {
            "file": "SKILL.md",
            "line_start": 406,
            "line_end": 415
          },
          {
            "file": "SKILL.md",
            "line_start": 415,
            "line_end": 423
          },
          {
            "file": "SKILL.md",
            "line_start": 423,
            "line_end": 437
          },
          {
            "file": "SKILL.md",
            "line_start": 437,
            "line_end": 441
          },
          {
            "file": "SKILL.md",
            "line_start": 441,
            "line_end": 453
          },
          {
            "file": "SKILL.md",
            "line_start": 453,
            "line_end": 461
          },
          {
            "file": "SKILL.md",
            "line_start": 461,
            "line_end": 477
          },
          {
            "file": "SKILL.md",
            "line_start": 477,
            "line_end": 483
          },
          {
            "file": "SKILL.md",
            "line_start": 483,
            "line_end": 495
          },
          {
            "file": "SKILL.md",
            "line_start": 495,
            "line_end": 502
          },
          {
            "file": "SKILL.md",
            "line_start": 502,
            "line_end": 509
          },
          {
            "file": "SKILL.md",
            "line_start": 509,
            "line_end": 512
          },
          {
            "file": "SKILL.md",
            "line_start": 512,
            "line_end": 524
          },
          {
            "file": "SKILL.md",
            "line_start": 524,
            "line_end": 527
          },
          {
            "file": "SKILL.md",
            "line_start": 527,
            "line_end": 530
          },
          {
            "file": "SKILL.md",
            "line_start": 530,
            "line_end": 538
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 724,
    "audit_model": "claude",
    "audited_at": "2026-01-17T07:05:49.354Z"
  },
  "content": {
    "user_title": "Create AI Learning Plugins with 9 RL Algorithms",
    "value_statement": "Building self-learning agents requires complex reinforcement learning setup. This skill provides templates and guidance for 9 algorithms including Decision Transformer, Q-Learning, and Actor-Critic.",
    "seo_keywords": [
      "reinforcement learning",
      "Q-Learning",
      "Decision Transformer",
      "Actor-Critic",
      "AI agent training",
      "machine learning plugins",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Create learning plugins from 9 RL algorithm templates via CLI",
      "Configure Decision Transformer for offline learning from logged data",
      "Set up Q-Learning and SARSA for value-based learning",
      "Implement Actor-Critic for continuous action spaces",
      "Store and retrieve training experiences with AgentDB adapter",
      "Apply experience replay and prioritized replay techniques"
    ],
    "limitations": [
      "Requires AgentDB v1.0.7+ and Node.js 18+",
      "No pre-trained models included",
      "Understanding of reinforcement learning concepts recommended",
      "Training performance depends on available compute resources"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Build Self-Learning Agents",
        "description": "Create autonomous agents that improve through experience using Decision Transformer or Q-Learning algorithms."
      },
      {
        "target_user": "Research Scientists",
        "title": "Compare RL Algorithms",
        "description": "Experiment with 9 different reinforcement learning approaches to find optimal solutions for your domain."
      },
      {
        "target_user": "AI Developers",
        "title": "Add Learning to Production Agents",
        "description": "Integrate experience-based learning into existing agent systems using the AgentDB adapter API."
      }
    ],
    "prompt_templates": [
      {
        "title": "Create Q-Learning Agent",
        "scenario": "Start with simple value-based learning",
        "prompt": "Create a Q-Learning plugin named nav-agent for discrete navigation tasks"
      },
      {
        "title": "Offline Learning Setup",
        "scenario": "Learn from historical data",
        "prompt": "Create a Decision Transformer plugin to learn from existing experience logs"
      },
      {
        "title": "Train with Experience Replay",
        "scenario": "Improve sample efficiency",
        "prompt": "Show how to implement prioritized experience replay for training my agent"
      },
      {
        "title": "Multi-Agent Configuration",
        "scenario": "Coordinate learning across agents",
        "prompt": "Set up multi-agent training with shared model and separate experience buffers"
      }
    ],
    "output_examples": [
      {
        "input": "Create a Decision Transformer plugin for learning from game replays",
        "output": [
          "Created plugin: decision-transformer/game-learner",
          "Algorithm: Decision Transformer (offline RL)",
          "Configuration: context_length 20, embed_dim 128, n_heads 8",
          "Ideal for: Learning from logged replay data without environment interaction"
        ]
      },
      {
        "input": "What algorithm should I use for a robot control task?",
        "output": [
          "Recommended: Actor-Critic algorithm",
          "Reason: Handles continuous action spaces well",
          "Alternative: Q-Learning if discretizing actions is acceptable",
          "Setup command: npx agentdb@latest create-plugin -t actor-critic -n robot-control"
        ]
      }
    ],
    "best_practices": [
      "Start with Q-Learning for discrete action problems before trying complex algorithms",
      "Always use validation split during training to detect overfitting early",
      "Store experiences with TD error priority for more efficient replay sampling"
    ],
    "anti_patterns": [
      "Using continuous action algorithms for discrete spaces without modification",
      "Training without validation split leads to undetected overfitting",
      "Setting epsilon too low prevents adequate exploration during learning"
    ],
    "faq": [
      {
        "question": "Which algorithm should I start with?",
        "answer": "Q-Learning for discrete actions or Decision Transformer for offline learning from logged data. Both are stable starting points."
      },
      {
        "question": "What hardware is needed for training?",
        "answer": "CPU works for basic Q-Learning. GPU recommended for Decision Transformer and larger neural network models."
      },
      {
        "question": "Can I integrate with other AI frameworks?",
        "answer": "Yes. AgentDB provides an adapter API that integrates with any agent framework or custom implementation."
      },
      {
        "question": "Where is training data stored?",
        "answer": "Locally in .agentdb/learning.db by default. No data is sent to external servers unless you configure it."
      },
      {
        "question": "Why is my training not converging?",
        "answer": "Try reducing learning rate, increasing epochs, or normalizing rewards. Check the Troubleshooting section for details."
      },
      {
        "question": "How does this compare to Stable Baselines3?",
        "answer": "AgentDB integrates learning with agent memory systems. Use Stable Baselines3 for pure research benchmarks."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 546
    }
  ]
}
