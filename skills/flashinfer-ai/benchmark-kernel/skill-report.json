{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-06T07:39:32.566Z",
    "slug": "flashinfer-ai-benchmark-kernel",
    "source_url": "https://github.com/flashinfer-ai/flashinfer/tree/main/.claude/skills/benchmark-kernel",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "87ba341e412c819522e61fe3f9391c922ac1e3cb8c7dee409f1cc19b5cb87ff9",
    "tree_hash": "a139c3cd59917a48f3d477b6324579acaaaf6b63f0f941c629c4fe32b8d6ff39"
  },
  "skill": {
    "name": "benchmark-kernel",
    "description": "Guide for benchmarking FlashInfer kernels with CUPTI timing",
    "summary": "Guide for benchmarking FlashInfer kernels with CUPTI timing",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "flashinfer-ai",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "benchmarking",
      "gpu",
      "cuda",
      "performance",
      "profiling"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a documentation-only skill containing tutorial instructions for benchmarking GPU kernels. No executable code, network calls, or file system access beyond documentation reading.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 423,
    "audit_model": "claude",
    "audited_at": "2026-01-06T07:39:32.566Z"
  },
  "content": {
    "user_title": "Benchmark GPU kernels with CUPTI timing",
    "value_statement": "Accurate GPU kernel benchmarking is challenging due to measurement overhead. This skill provides step-by-step instructions for using CUPTI hardware profiling to get precise kernel execution times.",
    "seo_keywords": [
      "GPU benchmarking",
      "CUPTI profiling",
      "CUDA performance",
      "FlashInfer kernels",
      "Claude Code",
      "kernel timing",
      "hardware profiling",
      "GPU optimization"
    ],
    "actual_capabilities": [
      "Provides tutorial for CUPTI hardware-level GPU profiling",
      "Includes command-line examples for FlashInfer benchmarks",
      "Shows Python API usage with bench_gpu_time() function",
      "Documents timing accuracy differences between CUPTI and CUDA events"
    ],
    "limitations": [
      "Requires CUDA 13+ for CUPTI installation",
      "Documentation only - no automated execution",
      "Specific to FlashInfer kernel library"
    ],
    "use_cases": [
      {
        "target_user": "GPU kernel developers",
        "title": "Measure kernel performance accurately",
        "description": "Use CUPTI hardware profiling to get precise GPU execution times without host overhead"
      },
      {
        "target_user": "ML performance engineers",
        "title": "Compare attention kernel backends",
        "description": "Benchmark different attention implementations to find the fastest for your workload"
      },
      {
        "target_user": "CUDA optimization specialists",
        "title": "Profile FP8 GEMM operations",
        "description": "Measure performance of mixed-precision matrix multiplication kernels"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic kernel timing",
        "scenario": "Timing a simple kernel",
        "prompt": "Help me benchmark my CUDA kernel using bench_gpu_time() with 30 iterations and 5 warmup runs"
      },
      {
        "title": "Compare backends",
        "scenario": "Testing multiple implementations",
        "prompt": "Show me how to benchmark BatchDecodeWithPagedKVCacheWrapper with fa2, cudnn, and cutlass backends"
      },
      {
        "title": "Batch benchmarks",
        "scenario": "Running multiple configurations",
        "prompt": "Create a test list file to benchmark different batch sizes and sequence lengths"
      },
      {
        "title": "Custom Python benchmarking",
        "scenario": "Integrating into research code",
        "prompt": "Write a Python script that benchmarks my custom attention kernel with CUPTI timing"
      }
    ],
    "output_examples": [
      {
        "input": "Benchmark decode attention with different backends",
        "output": [
          "fa2: median 0.145ms, 125.3 TFLOPS",
          "fa2_tc: median 0.138ms, 131.5 TFLOPS",
          "cudnn: median 0.142ms, 127.8 TFLOPS",
          "Best: fa2_tc with 131.5 TFLOPS throughput"
        ]
      }
    ],
    "best_practices": [
      "Install CUPTI for hardware-level accuracy when possible",
      "Use reference checking to verify kernel correctness during benchmarking",
      "Run sufficient iterations (30+) for statistical significance"
    ],
    "anti_patterns": [
      "Benchmarking without warmup iterations",
      "Using only CUDA events when CUPTI is available",
      "Comparing backends without verifying output correctness"
    ],
    "faq": [
      {
        "question": "Do I need CUPTI installed?",
        "answer": "No, the framework automatically falls back to CUDA events if CUPTI is unavailable"
      },
      {
        "question": "What's the minimum CUDA version?",
        "answer": "CUPTI requires CUDA 13+, but CUDA events work with any CUDA version"
      },
      {
        "question": "Can I benchmark non-FlashInfer kernels?",
        "answer": "Yes, use bench_gpu_time() with any CUDA kernel function"
      },
      {
        "question": "Is my data safe during benchmarking?",
        "answer": "Yes, benchmarking only measures execution time without accessing your data"
      },
      {
        "question": "Why are my results inconsistent?",
        "answer": "Increase warmup and measurement iterations, check for thermal throttling"
      },
      {
        "question": "How accurate is CUPTI vs CUDA events?",
        "answer": "CUPTI is more accurate for fast kernels (<50Î¼s), difference is negligible for longer kernels"
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
