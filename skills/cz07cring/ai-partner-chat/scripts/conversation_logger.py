"""
å¯¹è¯å†å²è®°å½•å™¨ - Conversation Logger

åŠŸèƒ½ï¼š
1. ä¿å­˜æ¯æ¬¡å¯¹è¯åˆ° Markdown æ–‡ä»¶
2. è¯„ä¼°å¯¹è¯é‡è¦æ€§ï¼ˆ1-5åˆ†ï¼‰
3. å°†é‡è¦å¯¹è¯å‘é‡åŒ–å­˜å…¥æ•°æ®åº“
4. ç”Ÿæˆæ¯å‘¨å¯¹è¯æ‘˜è¦

ç‰ˆæœ¬: 2.0
"""

import json
from pathlib import Path
from typing import Optional, Dict
from datetime import datetime, timedelta
import sys

# å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from chunk_schema import create_conversation_metadata, IMPORTANCE_MEDIUM
from vector_indexer import VectorIndexer


class ConversationLogger:
    """å¯¹è¯å†å²è®°å½•å™¨"""

    def __init__(
        self,
        conversations_dir: str = "./conversations",
        db_path: str = "./vector_db"
    ):
        """
        åˆå§‹åŒ–å¯¹è¯è®°å½•å™¨

        Args:
            conversations_dir: å¯¹è¯å†å²å­˜å‚¨ç›®å½•
            db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        """
        self.conversations_dir = Path(conversations_dir)
        self.raw_dir = self.conversations_dir / "raw"
        self.summary_dir = self.conversations_dir / "summary"
        self.db_path = db_path

        # åˆ›å»ºç›®å½•
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.summary_dir.mkdir(parents=True, exist_ok=True)

        # å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = self.conversations_dir / "metadata.json"
        self.metadata = self._load_metadata()

    def _load_metadata(self) -> Dict:
        """åŠ è½½å¯¹è¯å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"conversations": [], "total_count": 0}

    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)

    def evaluate_importance(
        self,
        user_message: str,
        ai_response: str
    ) -> int:
        """
        è¯„ä¼°å¯¹è¯é‡è¦æ€§ï¼ˆç®€å•è§„åˆ™ï¼Œå®é™…åº”ç”¨å¯ç”¨ LLMï¼‰

        è¯„åˆ†æ ‡å‡†:
        5åˆ† - é‡å¤§å†³ç­–ã€çªç ´æ€§ç†è§£ã€å…³é”®ç»“è®º
        4åˆ† - æ·±å…¥è®¨è®ºã€é—®é¢˜è§£å†³ã€çŸ¥è¯†å…³è”
        3åˆ† - æœ‰ç”¨ä¿¡æ¯ã€å¸¸è§„å­¦ä¹ è®°å½•
        2åˆ† - ç®€å•é—®ç­”ã€äº‹å®æŸ¥è¯¢
        1åˆ† - å¯’æš„ã€ç¡®è®¤ã€æ— å®è´¨å†…å®¹

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            ai_response: AIå›å¤

        Returns:
            é‡è¦æ€§è¯„åˆ† 1-5
        """
        # ç®€å•è§„åˆ™åˆ¤æ–­ï¼ˆç”Ÿäº§ç¯å¢ƒåº”è¯¥ç”¨ LLMï¼‰
        combined = (user_message + " " + ai_response).lower()

        # å…³é”®è¯åˆ¤æ–­
        high_importance_keywords = [
            'é‡è¦', 'å†³å®š', 'çªç ´', 'ç†è§£äº†', 'åŸæ¥å¦‚æ­¤',
            'é—®é¢˜è§£å†³', 'æ·±å…¥', 'å…³é”®', 'å‘ç°'
        ]

        medium_keywords = [
            'å­¦ä¹ ', 'ç¬”è®°', 'è®°å½•', 'æ•´ç†', 'æ€»ç»“',
            'æ€è€ƒ', 'åˆ†æ', 'è®¨è®º'
        ]

        low_keywords = [
            'è°¢è°¢', 'ä½ å¥½', 'å¥½çš„', 'æ˜ç™½', 'çŸ¥é“äº†',
            'hi', 'hello', 'thanks', 'ok'
        ]

        # é•¿åº¦åˆ¤æ–­ï¼ˆé•¿å¯¹è¯é€šå¸¸æ›´é‡è¦ï¼‰
        total_length = len(user_message) + len(ai_response)

        if any(kw in combined for kw in high_importance_keywords):
            return 5 if total_length > 500 else 4

        if any(kw in combined for kw in medium_keywords):
            return 3 if total_length > 200 else 2

        if any(kw in combined for kw in low_keywords) and total_length < 50:
            return 1

        # é»˜è®¤ä¸­ç­‰é‡è¦æ€§
        return IMPORTANCE_MEDIUM

    def log_conversation(
        self,
        user_message: str,
        ai_response: str,
        topic: Optional[str] = None,
        importance: Optional[int] = None
    ) -> str:
        """
        è®°å½•ä¸€æ¬¡å¯¹è¯

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            ai_response: AIå›å¤
            topic: å¯¹è¯ä¸»é¢˜ï¼ˆå¯é€‰ï¼Œå¯ç”± LLM æå–ï¼‰
            importance: é‡è¦æ€§è¯„åˆ†ï¼ˆå¯é€‰ï¼Œå¦‚ä¸æä¾›åˆ™è‡ªåŠ¨è¯„ä¼°ï¼‰

        Returns:
            å¯¹è¯ID
        """
        now = datetime.now()
        conversation_id = f"conv_{now.strftime('%Y%m%d_%H%M%S')}"

        # è¯„ä¼°é‡è¦æ€§
        if importance is None:
            importance = self.evaluate_importance(user_message, ai_response)

        # 1. ä¿å­˜åˆ° Markdown æ–‡ä»¶
        self._save_to_markdown(
            conversation_id, user_message, ai_response,
            now, topic, importance
        )

        # 2. å‘é‡åŒ–ï¼ˆåªæœ‰é‡è¦å¯¹è¯ â‰¥3åˆ†ï¼‰
        if importance >= IMPORTANCE_MEDIUM:
            self._save_to_vector_db(
                conversation_id, user_message, ai_response,
                now, topic, importance
            )

        # 3. æ›´æ–°å…ƒæ•°æ®
        self._update_metadata(conversation_id, now, topic, importance)

        return conversation_id

    def _save_to_markdown(
        self,
        conversation_id: str,
        user_msg: str,
        ai_msg: str,
        timestamp: datetime,
        topic: Optional[str],
        importance: int
    ):
        """ä¿å­˜å¯¹è¯åˆ° Markdown æ–‡ä»¶"""
        # æŒ‰æœˆä»½ç»„ç»‡
        month_dir = self.raw_dir / timestamp.strftime('%Y-%m')
        month_dir.mkdir(parents=True, exist_ok=True)

        # æ¯å¤©ä¸€ä¸ªæ–‡ä»¶
        daily_file = month_dir / f"{timestamp.strftime('%Y-%m-%d')}.md"

        # é‡è¦æ€§æ ‡è®°
        importance_emoji = {
            5: 'ğŸ”´',  # å…³é”®
            4: 'ğŸŸ ',  # é‡è¦
            3: 'ğŸŸ¡',  # ä¸­ç­‰
            2: 'âšª',  # ä¸€èˆ¬
            1: 'âš«'   # çç¢
        }

        # å¯¹è¯å†…å®¹
        conversation_content = f"""
## {importance_emoji.get(importance, 'ğŸŸ¡')} [{timestamp.strftime('%H:%M:%S')}] {topic or 'å¯¹è¯'}
**ID**: `{conversation_id}` | **é‡è¦æ€§**: {importance}/5

**ç”¨æˆ·**:
{user_msg}

**AI**:
{ai_msg}

---

"""

        # è¿½åŠ åˆ°æ–‡ä»¶
        with open(daily_file, 'a', encoding='utf-8') as f:
            f.write(conversation_content)

    def _save_to_vector_db(
        self,
        conversation_id: str,
        user_msg: str,
        ai_msg: str,
        timestamp: datetime,
        topic: Optional[str],
        importance: int
    ):
        """å°†å¯¹è¯å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“"""
        # åˆ›å»º chunk
        chunk = {
            'content': f"""å¯¹è¯ä¸»é¢˜: {topic or 'æœªåˆ†ç±»'}

ç”¨æˆ·é—®é¢˜: {user_msg}

AIå›å¤: {ai_msg}""",
            'metadata': create_conversation_metadata(
                conversation_id=conversation_id,
                importance=importance,
                date=timestamp.strftime('%Y-%m-%d'),
                created_at=timestamp.isoformat(),
                topic=topic
            )
        }

        # è¿½åŠ åˆ°å‘é‡åº“
        try:
            indexer = VectorIndexer(db_path=self.db_path)
            indexer.append_chunks([chunk])
            print(f"âœ… å¯¹è¯å·²å‘é‡åŒ–: {conversation_id} (é‡è¦æ€§: {importance})")
        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")

    def _update_metadata(
        self,
        conversation_id: str,
        timestamp: datetime,
        topic: Optional[str],
        importance: int
    ):
        """æ›´æ–°å…ƒæ•°æ®"""
        self.metadata['conversations'].append({
            'id': conversation_id,
            'timestamp': timestamp.isoformat(),
            'topic': topic,
            'importance': importance
        })
        self.metadata['total_count'] += 1
        self._save_metadata()

    def get_recent_conversations(
        self,
        days: int = 7,
        min_importance: int = 3
    ) -> list:
        """
        è·å–æœ€è¿‘çš„é‡è¦å¯¹è¯

        Args:
            days: æœ€è¿‘å¤šå°‘å¤©
            min_importance: æœ€ä½é‡è¦æ€§

        Returns:
            å¯¹è¯åˆ—è¡¨
        """
        cutoff_date = datetime.now() - timedelta(days=days)

        recent = [
            conv for conv in self.metadata['conversations']
            if (datetime.fromisoformat(conv['timestamp']) > cutoff_date
                and conv['importance'] >= min_importance)
        ]

        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        recent.sort(
            key=lambda x: (x['importance'], x['timestamp']),
            reverse=True
        )

        return recent

    def generate_weekly_summary(self) -> str:
        """
        ç”Ÿæˆæœ¬å‘¨å¯¹è¯æ‘˜è¦

        Returns:
            æ‘˜è¦æ–‡ä»¶è·¯å¾„
        """
        # è·å–æœ¬å‘¨å¯¹è¯
        weekly = self.get_recent_conversations(days=7, min_importance=2)

        if not weekly:
            return None

        # ç”Ÿæˆæ‘˜è¦æ–‡ä»¶
        now = datetime.now()
        week_num = now.strftime('%Y-W%W')
        summary_file = self.summary_dir / f"weekly-{week_num}.md"

        # æŒ‰ä¸»é¢˜åˆ†ç»„
        by_topic = {}
        for conv in weekly:
            topic = conv.get('topic') or 'æœªåˆ†ç±»'
            if topic not in by_topic:
                by_topic[topic] = []
            by_topic[topic].append(conv)

        # ç”Ÿæˆæ‘˜è¦å†…å®¹
        summary_content = f"""# å¯¹è¯æ‘˜è¦ - {week_num}

ç”Ÿæˆæ—¶é—´: {now.strftime('%Y-%m-%d %H:%M')}

## ç»Ÿè®¡

- æ€»å¯¹è¯æ•°: {len(weekly)}
- é‡è¦å¯¹è¯ (â‰¥4åˆ†): {len([c for c in weekly if c['importance'] >= 4])}
- æ¶‰åŠä¸»é¢˜: {len(by_topic)}

## æŒ‰ä¸»é¢˜åˆ†ç±»

"""

        for topic, convs in sorted(by_topic.items()):
            summary_content += f"### {topic}\n\n"
            summary_content += f"å¯¹è¯æ¬¡æ•°: {len(convs)}\n"
            summary_content += f"å¹³å‡é‡è¦æ€§: {sum(c['importance'] for c in convs) / len(convs):.1f}\n\n"

            # åˆ—å‡ºå¯¹è¯
            for conv in convs[:5]:  # æ¯ä¸ªä¸»é¢˜æœ€å¤šåˆ—5æ¡
                timestamp = datetime.fromisoformat(conv['timestamp'])
                summary_content += f"- [{timestamp.strftime('%m-%d %H:%M')}] "
                summary_content += f"é‡è¦æ€§: {conv['importance']}/5\n"

            summary_content += "\n"

        # ä¿å­˜æ‘˜è¦
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)

        print(f"ğŸ“Š å·²ç”Ÿæˆå‘¨æ‘˜è¦: {summary_file}")
        return str(summary_file)


# LLM æç¤ºè¯æ¨¡æ¿ï¼ˆä¾› Claude Code ä½¿ç”¨ï¼‰

IMPORTANCE_EVALUATION_PROMPT = """
è¯„ä¼°ä»¥ä¸‹å¯¹è¯çš„é‡è¦æ€§ï¼ˆ1-5åˆ†ï¼‰ï¼š

ç”¨æˆ·: {user_message}

AI: {ai_response}

è¯„åˆ†æ ‡å‡†:
5åˆ† - é‡å¤§å†³ç­–ã€çªç ´æ€§ç†è§£ã€å…³é”®ç»“è®º
4åˆ† - æ·±å…¥è®¨è®ºã€é—®é¢˜è§£å†³ã€çŸ¥è¯†å…³è”
3åˆ† - æœ‰ç”¨ä¿¡æ¯ã€å¸¸è§„å­¦ä¹ è®°å½•
2åˆ† - ç®€å•é—®ç­”ã€äº‹å®æŸ¥è¯¢
1åˆ† - å¯’æš„ã€ç¡®è®¤ã€æ— å®è´¨å†…å®¹

åªè¿”å›æ•°å­—1-5ã€‚
"""

TOPIC_EXTRACTION_PROMPT = """
ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–ä¸»è¦ä¸»é¢˜ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰ï¼š

ç”¨æˆ·: {user_message}

AI: {ai_response}

åªè¿”å›ç®€çŸ­çš„ä¸»é¢˜ï¼ˆä¸è¶…è¿‡10ä¸ªå­—ï¼‰ã€‚
"""
