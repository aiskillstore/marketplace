{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T02:00:52.349Z",
    "slug": "davila7-senior-prompt-engineer",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/development/senior-prompt-engineer",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "4a167292edfd9290ed49048fa86b79c6b8c36da6b26d8233891bd1184b341bdb",
    "tree_hash": "4ec10a6d727730f2a8eb1a1188a28f2881aee3723ecb0828d1c95c047e637f5c"
  },
  "skill": {
    "name": "senior-prompt-engineer",
    "description": "World-class prompt engineering skill for LLM optimization, prompt patterns, structured outputs, and AI product development. Expertise in Claude, GPT-4, prompt design patterns, few-shot learning, chain-of-thought, and AI evaluation. Includes RAG optimization, agent design, and LLM system architecture. Use when building AI products, optimizing LLM performance, designing agentic systems, or implementing advanced prompting techniques.",
    "summary": "World-class prompt engineering skill for LLM optimization, prompt patterns, structured outputs, and ...",
    "icon": "âš¡",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "development",
    "tags": [
      "prompt-engineering",
      "llm",
      "ai-development",
      "agent-design"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Documentation-focused skill containing reference guides and template Python scripts for prompt optimization, RAG evaluation, and agent orchestration. All scripts are skeleton implementations with standard file I/O only. No network calls, credential access, or shell command execution detected. All 68 static findings are false positives caused by scanner misinterpretation of documentation keywords and markdown code fence syntax.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/rag_evaluator.py",
            "line_start": 1,
            "line_end": 101
          },
          {
            "file": "scripts/agent_orchestrator.py",
            "line_start": 1,
            "line_end": 101
          },
          {
            "file": "scripts/prompt_optimizer.py",
            "line_start": 1,
            "line_end": 101
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/rag_evaluator.py",
            "line_start": 73,
            "line_end": 74
          },
          {
            "file": "scripts/agent_orchestrator.py",
            "line_start": 73,
            "line_end": 74
          },
          {
            "file": "scripts/prompt_optimizer.py",
            "line_start": 73,
            "line_end": 74
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 1042,
    "audit_model": "claude",
    "audited_at": "2026-01-17T02:00:52.348Z"
  },
  "content": {
    "user_title": "Master LLM Prompt Engineering",
    "value_statement": "Creating effective prompts for large language models requires deep expertise in patterns, frameworks, and optimization techniques. This skill provides production-ready prompt engineering strategies for Claude, GPT-4, and other LLMs, including structured outputs, chain-of-thought reasoning, and agentic system design.",
    "seo_keywords": [
      "prompt engineering",
      "Claude prompt templates",
      "LLM optimization",
      "chain-of-thought prompting",
      "few-shot learning",
      "structured outputs",
      "AI agent design",
      "RAG optimization",
      "Claude Code skill",
      "prompt patterns"
    ],
    "actual_capabilities": [
      "Design and optimize prompts for Claude, GPT-4, and other LLMs",
      "Implement advanced prompting patterns including chain-of-thought and few-shot learning",
      "Create structured output formats and JSON response schemas",
      "Build and evaluate RAG (Retrieval-Augmented Generation) systems",
      "Design agentic AI systems with multi-step reasoning workflows",
      "Apply LLM evaluation frameworks for performance measurement"
    ],
    "limitations": [
      "Does not execute code or interact with external APIs directly",
      "Does not provide access to paid model APIs or rate limit bypasses",
      "Does not include pre-trained models or model weights",
      "Requires user to provide their own API keys for LLM services"
    ],
    "use_cases": [
      {
        "target_user": "AI Engineers",
        "title": "Build Production AI Systems",
        "description": "Design robust prompting pipelines for production AI applications with evaluation frameworks and monitoring strategies."
      },
      {
        "target_user": "Product Managers",
        "title": "Define AI Product Requirements",
        "description": "Create clear prompt specifications and success metrics for AI-powered features and user experiences."
      },
      {
        "target_user": "Data Scientists",
        "title": "Optimize LLM Performance",
        "description": "Apply systematic evaluation techniques to improve model outputs and reduce costs through prompt optimization."
      }
    ],
    "prompt_templates": [
      {
        "title": "Chain-of-Thought",
        "scenario": "Complex reasoning tasks",
        "prompt": "Solve this problem step by step. First, identify the key components. Second, analyze each component. Third, reason through the relationships. Finally, derive the conclusion with supporting evidence."
      },
      {
        "title": "Few-Shot Examples",
        "scenario": "Learning from demonstrations",
        "prompt": "Complete the following tasks based on these examples: Example 1: [input] -> [output]. Example 2: [input] -> [output]. Now complete: [new input] -> ?"
      },
      {
        "title": "Structured Output",
        "scenario": "JSON/API responses",
        "prompt": "Provide your response as a valid JSON object with these exact fields: { \"summary\": \"brief summary\", \"confidence\": 0.0-1.0, \"reasoning\": \"explanation\" }. Do not include any other text."
      },
      {
        "title": "System Persona",
        "scenario": "Consistent voice and behavior",
        "prompt": "You are an expert [role] with [years] years of experience. You always: 1) Start with clear understanding, 2) Provide structured responses, 3) Include practical examples, 4) Ask clarifying questions when needed."
      }
    ],
    "output_examples": [
      {
        "input": "Create a prompt that helps developers write clean, documented Python code with type hints",
        "output": [
          "Define clear role and expertise level for the AI",
          "Specify output structure and format requirements",
          "Include example code with inline comments",
          "Add validation criteria for successful outputs",
          "Provide guidelines for handling edge cases"
        ]
      },
      {
        "input": "Design a few-shot prompt for sentiment analysis of customer reviews",
        "output": [
          "Provide 3-5 labeled examples showing positive, negative, and neutral reviews",
          "Include brief explanations for each example classification",
          "Add format instructions for consistent response structure",
          "Specify handling for ambiguous cases"
        ]
      },
      {
        "input": "Create a chain-of-thought prompt for mathematical problem solving",
        "output": [
          "Break down the problem into explicit steps",
          "Show intermediate calculations and reasoning",
          "Verify each step before proceeding to the next",
          "Final answer with supporting work shown"
        ]
      }
    ],
    "best_practices": [
      "Start with clear role definition and context setting for the AI model",
      "Use specific output formats like JSON or structured lists to reduce variability",
      "Iterate systematically by testing prompts and measuring results against metrics",
      "Include error handling and fallback instructions for edge cases"
    ],
    "anti_patterns": [
      "Avoid vague instructions that leave interpretation to the model",
      "Do not overload prompts with too many constraints simultaneously",
      "Avoid assuming the AI has context you have not explicitly provided",
      "Do not use prompts that try to circumvent safety guidelines"
    ],
    "faq": [
      {
        "question": "Which LLMs work with these prompts?",
        "answer": "These patterns work with Claude, GPT-4, Gemini, and most modern instruction-tuned LLMs. Some adjustments may be needed for optimal results."
      },
      {
        "question": "What are token limits for prompts?",
        "answer": "Most models support 4K-128K context tokens. Keep system prompts under 2K tokens to leave room for user content and responses."
      },
      {
        "question": "Can I combine multiple patterns?",
        "answer": "Yes. Chain-of-thought works well with structured outputs. Few-shot examples enhance role-based prompts. Start simple and add complexity gradually."
      },
      {
        "question": "How do I protect sensitive data?",
        "answer": "Never include personal information, credentials, or proprietary code in prompts. Use placeholder values and document data requirements separately."
      },
      {
        "question": "Why does my prompt work differently over time?",
        "answer": "Model updates can change behavior. Version your prompts, test regularly, and update prompts when model versions change."
      },
      {
        "question": "How is this different from basic prompting?",
        "answer": "This skill covers production patterns including evaluation, optimization, and system design. Basic prompting provides single-turn responses only."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "agentic_system_design.md",
          "type": "file",
          "path": "references/agentic_system_design.md",
          "lines": 81
        },
        {
          "name": "llm_evaluation_frameworks.md",
          "type": "file",
          "path": "references/llm_evaluation_frameworks.md",
          "lines": 81
        },
        {
          "name": "prompt_engineering_patterns.md",
          "type": "file",
          "path": "references/prompt_engineering_patterns.md",
          "lines": 81
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "agent_orchestrator.py",
          "type": "file",
          "path": "scripts/agent_orchestrator.py",
          "lines": 101
        },
        {
          "name": "prompt_optimizer.py",
          "type": "file",
          "path": "scripts/prompt_optimizer.py",
          "lines": 101
        },
        {
          "name": "rag_evaluator.py",
          "type": "file",
          "path": "scripts/rag_evaluator.py",
          "lines": 101
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 227
    }
  ]
}
