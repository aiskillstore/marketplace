{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T02:48:35.634Z",
    "slug": "davila7-nowait-reasoning-optimizer",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/productivity/nowait",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "fd71e79f36014d423e296fa7e71e48aa5ec437c15a8d096033766a898afc8a25",
    "tree_hash": "d12c39b1f3ad32db50fc5e523559052b81034191a3aee58fcc8285afc7863ff5"
  },
  "skill": {
    "name": "nowait-reasoning-optimizer",
    "description": "Implements the NOWAIT technique for efficient reasoning in R1-style LLMs. Use when optimizing inference of reasoning models (QwQ, DeepSeek-R1, Phi4-Reasoning, Qwen3, Kimi-VL, QvQ), reducing chain-of-thought token usage by 27-51% while preserving accuracy. Triggers on \"optimize reasoning\", \"reduce thinking tokens\", \"efficient inference\", \"suppress reflection tokens\", or when working with verbose CoT outputs.",
    "summary": "Implements the NOWAIT technique for efficient reasoning in R1-style LLMs. Use when optimizing infere...",
    "icon": "âš¡",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "llm-optimization",
      "reasoning-models",
      "token-reduction",
      "inference"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "external_commands",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Legitimate ML optimization utility implementing a published research paper technique. Pure Python inference-time token manipulation with no network access, no file I/O beyond tokenizer loading, and no external command execution. All static findings are false positives from markdown code examples and benign ML patterns.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "refrences/keywords.md",
            "line_start": 7,
            "line_end": 7
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 11,
            "line_end": 31
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 31,
            "line_end": 37
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 37,
            "line_end": 47
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 47,
            "line_end": 83
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 83,
            "line_end": 83
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 83,
            "line_end": 83
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 83,
            "line_end": 83
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 83,
            "line_end": 87
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 87,
            "line_end": 87
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 87,
            "line_end": 87
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 87,
            "line_end": 91
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 91,
            "line_end": 91
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 91,
            "line_end": 91
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 91,
            "line_end": 91
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 91,
            "line_end": 95
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 95,
            "line_end": 95
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 95,
            "line_end": 95
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 95,
            "line_end": 99
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 99,
            "line_end": 99
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 99,
            "line_end": 99
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 99,
            "line_end": 114
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 114,
            "line_end": 115
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 115,
            "line_end": 118
          },
          {
            "file": "refrences/keywords.md",
            "line_start": 118,
            "line_end": 130
          },
          {
            "file": "SKILL.md",
            "line_start": 37,
            "line_end": 49
          },
          {
            "file": "SKILL.md",
            "line_start": 49,
            "line_end": 53
          },
          {
            "file": "SKILL.md",
            "line_start": 53,
            "line_end": 55
          },
          {
            "file": "SKILL.md",
            "line_start": 55,
            "line_end": 58
          },
          {
            "file": "SKILL.md",
            "line_start": 58,
            "line_end": 66
          },
          {
            "file": "SKILL.md",
            "line_start": 66,
            "line_end": 72
          },
          {
            "file": "SKILL.md",
            "line_start": 72,
            "line_end": 95
          },
          {
            "file": "SKILL.md",
            "line_start": 95,
            "line_end": 111
          },
          {
            "file": "SKILL.md",
            "line_start": 111,
            "line_end": 115
          },
          {
            "file": "SKILL.md",
            "line_start": 115,
            "line_end": 126
          },
          {
            "file": "SKILL.md",
            "line_start": 126,
            "line_end": 145
          },
          {
            "file": "SKILL.md",
            "line_start": 145,
            "line_end": 146
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 785,
    "audit_model": "claude",
    "audited_at": "2026-01-17T02:48:35.634Z"
  },
  "content": {
    "user_title": "Reduce LLM reasoning tokens by 50%",
    "value_statement": "Chain-of-thought reasoning models generate verbose self-reflection tokens that increase costs and latency. This skill implements the NOWAIT technique to suppress unnecessary reflection tokens during inference, reducing token usage by 27-51% while maintaining accuracy on RL-based reasoning models.",
    "seo_keywords": [
      "NOWAIT technique LLM",
      "Claude Code reasoning optimizer",
      "chain-of-thought token reduction",
      "QwQ-32B optimization",
      "DeepSeek-R1 inference",
      "reasoning model efficiency",
      "token suppression ML",
      "LLM cost optimization",
      "Phi4-Reasoning-Plus",
      "Qwen3 inference"
    ],
    "actual_capabilities": [
      "Suppresses self-reflection tokens like wait, hmm, but, however during LLM generation",
      "Reduces chain-of-thought tokens by 27-51% on RL-based reasoning models",
      "Works with QwQ-32B, Phi4-Reasoning-Plus, Qwen3-32B, Kimi-VL, and QvQ",
      "Integrates with HuggingFace Transformers and vLLM frameworks",
      "Provides configurable keywords and exclusion patterns for custom tuning",
      "Offers both logit suppression and stopping criteria approaches"
    ],
    "limitations": [
      "Less effective on distilled models which may show accuracy degradation on hard tasks",
      "Simple problems with minimal CoT see reduced benefits",
      "Some domains may require model-specific keyword tuning for optimal results"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Optimize production inference",
        "description": "Deploy efficient reasoning models with reduced compute costs and latency for production systems"
      },
      {
        "target_user": "Researchers",
        "title": "Reduce benchmarking costs",
        "description": "Run large-scale reasoning benchmarks with 30-50% fewer tokens while preserving accuracy"
      },
      {
        "target_user": "API Developers",
        "title": "Cut token usage fees",
        "description": "Lower API costs when using reasoning models by suppressing verbose reflection patterns"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic NOWAIT setup",
        "scenario": "Initialize token suppression",
        "prompt": "Use the NOWAIT Reasoning Optimizer to suppress self-reflection tokens during generation. Initialize NOWAITLogitProcessor with the model's tokenizer and apply it during model.generate() with max_new_tokens=32768."
      },
      {
        "title": "vLLM integration",
        "scenario": "Optimize vLLM deployment",
        "prompt": "Configure vLLM to use NOWAIT by calling get_nowait_bad_words_ids() with the tokenizer and pass the result to SamplingParams for efficient batch inference."
      },
      {
        "title": "Custom keywords",
        "scenario": "Tune suppression for domain",
        "prompt": "Create a custom NOWAITConfig with domain-specific keywords to suppress, excluding false positives like butterfly or checkout that should not be filtered."
      },
      {
        "title": "Hybrid approach",
        "scenario": "Balance suppression and flexibility",
        "prompt": "Use NOWAITStoppingCriteria instead of full suppression to allow some reflection tokens but stop generation if reflection count exceeds a configurable threshold."
      }
    ],
    "output_examples": [
      {
        "input": "Optimize inference for QwQ-32B to reduce thinking tokens",
        "output": [
          "Initialized NOWAIT with 17 reflection keywords",
          "Suppressing: wait, hmm, but, however, alternatively, check, verify...",
          "Excluded false positives: ohio, button, checkout, checksum...",
          "Token set built: approximately N tokens identified for suppression",
          "Ready for logits_processor integration with 27-51% expected token reduction"
        ]
      },
      {
        "input": "Apply NOWAIT to Kimi-VL-A3B for visual QA task",
        "output": [
          "Configured NOWAIT for multimodal model",
          "Expected token reduction: 40-60% on visual QA tasks",
          "Applied all default reflection keywords",
          "Model will skip unnecessary self-reflection while preserving visual reasoning"
        ]
      },
      {
        "input": "Benchmark Qwen3-32B with and without NOWAIT",
        "output": [
          "Baseline: 15000 tokens on AIME math problem",
          "NOWAIT: 10500 tokens with 30% reduction",
          "Accuracy maintained at approximately 66-68%",
          "Significant cost savings for large-scale evaluation"
        ]
      }
    ],
    "best_practices": [
      "Test token reduction on your specific model before production deployment",
      "Monitor accuracy on hard tasks when using NOWAIT on distilled models",
      "Use the exclusion patterns to prevent false positives on legitimate words"
    ],
    "anti_patterns": [
      "Applying NOWAIT to distilled small models without accuracy validation",
      "Using NOWAIT on non-reasoning models that do not generate reflection tokens",
      "Suppressing keywords without checking excluded patterns first"
    ],
    "faq": [
      {
        "question": "Which models work best with NOWAIT?",
        "answer": "RL-based models like QwQ-32B, Phi4-Reasoning-Plus, and Qwen3-32B show 27-51% token reduction. Distilled models may lose accuracy on hard tasks."
      },
      {
        "question": "What token reduction can I expect?",
        "answer": "Math tasks see 30% reduction, visual QA up to 50%, and video QA around 27%. Results vary by model and task complexity."
      },
      {
        "question": "Does NOWAIT affect answer accuracy?",
        "answer": "RL-based models maintain stable accuracy. Distilled models may show degradation on challenging tasks. Always validate for your use case."
      },
      {
        "question": "Can I customize which tokens are suppressed?",
        "answer": "Yes, provide custom keywords and excluded patterns via NOWAITConfig for domain-specific tuning."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "Yes, NOWAIT runs locally during inference and does not transmit data externally. It only manipulates model logits."
      },
      {
        "question": "How does NOWAIT compare to other optimization techniques?",
        "answer": "NOWAIT is training-free and works at inference time. It complements other techniques like quantization and KV cache optimization."
      }
    ]
  },
  "file_structure": [
    {
      "name": "refrences",
      "type": "dir",
      "path": "refrences",
      "children": [
        {
          "name": "keywords.md",
          "type": "file",
          "path": "refrences/keywords.md",
          "lines": 135
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "nowait_processor.py",
          "type": "file",
          "path": "scripts/nowait_processor.py",
          "lines": 309
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 146
    }
  ]
}
