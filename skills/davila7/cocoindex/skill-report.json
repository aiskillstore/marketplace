{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T01:52:53.937Z",
    "slug": "davila7-cocoindex",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/development/cocoindex",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "dd7d222b973ca1aac753db73f1b02b004f90af10fdb3f3b43df7a9609756a28b",
    "tree_hash": "39870592d5e4e5e8f4be266b1ca3525df4c9aad48b977a35d3979b3ed6b6a475"
  },
  "skill": {
    "name": "cocoindex",
    "description": "Comprehensive toolkit for developing with the CocoIndex library. Use when users need to create data transformation pipelines (flows), write custom functions, or operate flows via CLI or API. Covers building ETL workflows for AI data processing, including embedding documents into vector databases, building knowledge graphs, creating search indexes, or processing data streams with incremental updates.",
    "summary": "Comprehensive toolkit for developing with the CocoIndex library. Use when users need to create data ...",
    "icon": "ðŸ”„",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "development",
    "tags": [
      "data pipelines",
      "vector embeddings",
      "knowledge graphs",
      "ETL",
      "AI"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "external_commands",
      "network",
      "env_access"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Documentation-only skill containing markdown reference files. No executable code, network calls, file access, or system operations. The skill provides guidance for using the CocoIndex library through code examples that users copy into their own projects. All 445 static findings are false positives triggered by documentation patterns, not actual security issues.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "references/api_operations.md",
            "line_start": 259,
            "line_end": 259
          },
          {
            "file": "references/flow_patterns.md",
            "line_start": 328,
            "line_end": 328
          },
          {
            "file": "SKILL.md",
            "line_start": 562,
            "line_end": 562
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "references/api_operations.md",
            "line_start": 1,
            "line_end": 563
          },
          {
            "file": "references/cli_operations.md",
            "line_start": 1,
            "line_end": 399
          },
          {
            "file": "references/custom_functions.md",
            "line_start": 1,
            "line_end": 466
          },
          {
            "file": "references/flow_patterns.md",
            "line_start": 1,
            "line_end": 454
          },
          {
            "file": "SKILL.md",
            "line_start": 1,
            "line_end": 832
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 26,
            "line_end": 831
          }
        ]
      },
      {
        "factor": "env_access",
        "evidence": [
          {
            "file": "references/api_operations.md",
            "line_start": 1,
            "line_end": 563
          },
          {
            "file": "references/cli_operations.md",
            "line_start": 1,
            "line_end": 399
          },
          {
            "file": "references/custom_functions.md",
            "line_start": 1,
            "line_end": 466
          },
          {
            "file": "SKILL.md",
            "line_start": 1,
            "line_end": 832
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 6,
    "total_lines": 2957,
    "audit_model": "claude",
    "audited_at": "2026-01-17T01:52:53.937Z"
  },
  "content": {
    "user_title": "Build AI Data Pipelines with CocoIndex",
    "value_statement": "Creating data pipelines for AI applications is complex and time-consuming. CocoIndex provides an ultra-performant framework for building indexing flows with incremental processing, live updates, and support for multiple data sources and vector databases.",
    "seo_keywords": [
      "CocoIndex",
      "AI data pipeline",
      "vector embedding",
      "knowledge graph",
      "document indexing",
      "incremental processing",
      "Claude Code",
      "Codex",
      "vector search",
      "ETL pipeline"
    ],
    "actual_capabilities": [
      "Create indexing flows for ETL pipelines that extract, transform, and load data",
      "Build vector embeddings for documents, code, and images using local or cloud models",
      "Construct knowledge graphs by extracting structured information with LLM APIs",
      "Implement live updates to continuously sync source changes to targets",
      "Export to multiple targets including Postgres, Qdrant, Neo4j, and LanceDB",
      "Write custom transformation functions for specialized processing needs"
    ],
    "limitations": [
      "Requires running CocoIndex database for internal storage and tracking",
      "Needs external database setup for vector stores and graph databases",
      "LLM-based features require API keys for providers like OpenAI or Anthropic",
      "Flows must be defined in Python and executed in user environment"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Build Vector Search Index",
        "description": "Create embeddings from documents and export to vector database for semantic search"
      },
      {
        "target_user": "Data Engineers",
        "title": "Construct Knowledge Graphs",
        "description": "Extract entities and relationships using LLMs and build Neo4j knowledge graphs"
      },
      {
        "target_user": "App Developers",
        "title": "Create Live Data Pipelines",
        "description": "Build continuous sync pipelines that update indexes when source data changes"
      }
    ],
    "prompt_templates": [
      {
        "title": "Simple Embedding Flow",
        "scenario": "Create a document embedding pipeline",
        "prompt": "Build a CocoIndex flow that embeds markdown documents from a local folder and exports them to Postgres with vector similarity search"
      },
      {
        "title": "Code Index",
        "scenario": "Index codebase for semantic search",
        "prompt": "Create a flow that indexes Python and Rust source files with language-aware chunking and stores embeddings in Qdrant"
      },
      {
        "title": "Knowledge Graph",
        "scenario": "Extract entities from documents",
        "prompt": "Build a knowledge graph flow using GPT-4 to extract products and categories from JSON files and export to Neo4j"
      },
      {
        "title": "Live Updates",
        "scenario": "Continuous document sync",
        "prompt": "Set up a live update flow that monitors a local folder and incrementally indexes new documents to LanceDB"
      }
    ],
    "output_examples": [
      {
        "input": "Build a CocoIndex flow that embeds documents from local files into Postgres with vector search",
        "output": [
          "Flow definition with LocalFile source",
          "Text chunking with SplitRecursively",
          "SentenceTransformer embeddings",
          "Postgres export with cosine similarity index",
          "Setup, update, and CLI commands"
        ]
      },
      {
        "input": "Create a knowledge graph flow that extracts entities from PDF documents using Claude",
        "output": [
          "PDF source configuration with text extraction",
          "LLM extraction function for entity detection",
          "Neo4j target with node and relationship mapping",
          "Knowledge graph schema design guidance"
        ]
      },
      {
        "input": "Set up a live document indexing pipeline that syncs changes from S3",
        "output": [
          "AmazonS3 source with change detection",
          "Live update configuration with refresh interval",
          "Incremental processing to avoid full rebuilds",
          "Qdrant target for vector search"
        ]
      }
    ],
    "best_practices": [
      "Use evaluate command before running update to test flow logic without side effects",
      "Always call cocoindex.init() before using any APIs and load environment variables with dotenv",
      "Enable caching for expensive operations like LLM calls and model inference"
    ],
    "anti_patterns": [
      "Using local variables for transformation results instead of assigning to row fields",
      "Creating unnecessary dataclasses to mirror flow field schemas",
      "Running updates without first setting up the flow"
    ],
    "faq": [
      {
        "question": "What databases does CocoIndex support?",
        "answer": "CocoIndex supports Postgres with pgvector, Qdrant, LanceDB, Neo4j, and Kuzu. Each has specific setup requirements."
      },
      {
        "question": "How does incremental processing work?",
        "answer": "CocoIndex tracks processed data and only re-runs transformations on changed or new items, avoiding full rebuilds."
      },
      {
        "question": "Can I use cloud LLM providers?",
        "answer": "Yes. CocoIndex supports OpenAI, Anthropic, Gemini, Voyage, and local Ollama models through built-in functions."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "This skill only provides documentation. All data processing happens in your environment with your database and API keys."
      },
      {
        "question": "Why is my live update stopping immediately?",
        "answer": "Live updates require a change capture mechanism. Add refresh_interval to your source or use source-specific change capture."
      },
      {
        "question": "How is this different from LangChain or LlamaIndex?",
        "answer": "CocoIndex focuses on persistent, production-ready ETL pipelines with incremental updates rather than in-memory RAG operations."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "api_operations.md",
          "type": "file",
          "path": "references/api_operations.md",
          "lines": 571
        },
        {
          "name": "cli_operations.md",
          "type": "file",
          "path": "references/cli_operations.md",
          "lines": 402
        },
        {
          "name": "custom_functions.md",
          "type": "file",
          "path": "references/custom_functions.md",
          "lines": 468
        },
        {
          "name": "flow_patterns.md",
          "type": "file",
          "path": "references/flow_patterns.md",
          "lines": 479
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 832
    }
  ]
}
