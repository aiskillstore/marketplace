{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T01:45:04.142Z",
    "slug": "davila7-senior-data-engineer",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/development/senior-data-engineer",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "47443a66dba421d32f6e8208e24305574eb4015f7ba85f34075bdc86d5c10bbd",
    "tree_hash": "bd560a5a5676f95e4a57d8c400eda8a308e061773b9c13a0c9cb4314af66863b"
  },
  "skill": {
    "name": "senior-data-engineer",
    "description": "World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data infrastructure. Expertise in Python, SQL, Spark, Airflow, dbt, Kafka, and modern data stack. Includes data modeling, pipeline orchestration, data quality, and DataOps. Use when designing data architectures, building data pipelines, optimizing data workflows, or implementing data governance.",
    "summary": "World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data i...",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "data",
    "tags": [
      "data-engineering",
      "pipelines",
      "etl",
      "dataops",
      "architecture"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Static analysis flagged 57 patterns as HIGH risk, but ALL are false positives. The scanner misinterpreted documentation text ('algorithms', 'encryption') and template code (argparse calls, markdown backticks) as malicious patterns. Actual code contains only standard Python libraries, no network calls, no credential access, no external command execution. Scripts are production-ready templates with safe implementations.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/pipeline_orchestrator.py",
            "line_start": 1,
            "line_end": 101
          },
          {
            "file": "scripts/etl_performance_optimizer.py",
            "line_start": 1,
            "line_end": 101
          },
          {
            "file": "scripts/data_quality_validator.py",
            "line_start": 1,
            "line_end": 101
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/pipeline_orchestrator.py",
            "line_start": 7,
            "line_end": 12
          },
          {
            "file": "scripts/etl_performance_optimizer.py",
            "line_start": 7,
            "line_end": 12
          },
          {
            "file": "scripts/data_quality_validator.py",
            "line_start": 7,
            "line_end": 12
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 1038,
    "audit_model": "claude",
    "audited_at": "2026-01-17T01:45:04.142Z"
  },
  "content": {
    "user_title": "Build scalable data pipelines and ETL systems",
    "value_statement": "Design and implement production-grade data pipelines with senior-level expertise. Transform raw data into reliable, scalable analytics infrastructure using Python, SQL, Spark, and modern data stack tools.",
    "seo_keywords": [
      "claude data engineer",
      "data pipeline design",
      "etl automation",
      "data architecture",
      "apache spark",
      "airflow dags",
      "data quality",
      "claude code data engineering",
      "big data processing",
      "data infrastructure"
    ],
    "actual_capabilities": [
      "Design scalable data pipeline architectures with fault tolerance",
      "Implement ETL and ELT workflows using Python, SQL, and Spark",
      "Build pipeline orchestration with Airflow and modern scheduling tools",
      "Establish data quality validation and monitoring frameworks",
      "Apply DataOps best practices for production data systems",
      "Optimize data workflow performance and cost efficiency"
    ],
    "limitations": [
      "Does not execute code or deploy infrastructure directly",
      "Requires user to have data tools and environments configured",
      "Provides guidance rather than automated pipeline execution"
    ],
    "use_cases": [
      {
        "target_user": "Data Engineers",
        "title": "Design Pipeline Architecture",
        "description": "Create robust data pipeline designs with proper error handling and monitoring strategies."
      },
      {
        "target_user": "Analytics Teams",
        "title": "Improve Data Quality",
        "description": "Implement validation frameworks to ensure data accuracy and consistency across pipelines."
      },
      {
        "target_user": "ML Engineers",
        "title": "Scale Data Infrastructure",
        "description": "Build production-ready data infrastructure for machine learning workloads and real-time inference."
      }
    ],
    "prompt_templates": [
      {
        "title": "Pipeline Design",
        "scenario": "Create new data pipeline",
        "prompt": "Design a production-grade data pipeline architecture for [use case]. Include source systems, transformation logic, and target data warehouse schema. Recommend appropriate tools from the modern data stack."
      },
      {
        "title": "ETL Optimization",
        "scenario": "Improve pipeline performance",
        "prompt": "Analyze and optimize my ETL pipeline for [workload type]. Identify bottlenecks and suggest improvements for throughput and latency using [specific tool]."
      },
      {
        "title": "Data Quality Framework",
        "scenario": "Implement validation",
        "prompt": "Create a comprehensive data quality validation framework for [data type]. Include checks for completeness, accuracy, consistency, and timeliness."
      },
      {
        "title": "DataOps Implementation",
        "scenario": "Establish operational practices",
        "prompt": "Define DataOps best practices for our data team including CI/CD for data pipelines, monitoring strategies, and incident response procedures."
      }
    ],
    "output_examples": [
      {
        "input": "Design a data pipeline for processing customer clickstream data in real-time",
        "output": [
          "Source: Kafka topics for raw clickstream events",
          "Ingestion: Apache Kafka with exactly-once semantics",
          "Stream Processing: Apache Flink for windowed aggregations",
          "Storage: Bronze (raw), Silver (cleaned), Gold (aggregated) layers in Delta Lake",
          "Orchestration: Airflow for batch reconciliation jobs",
          "Monitoring: Data quality checks at each layer, latency alerts under 100ms P99"
        ]
      },
      {
        "input": "How do I improve data quality in my ETL pipelines",
        "output": [
          "Implement schema validation at ingestion with Great Expectations or similar",
          "Add data profiling to detect anomalies and drift early",
          "Use checksum validation to ensure data integrity through transformations",
          "Build automated alerts for data quality failures",
          "Create data contracts between producers and consumers"
        ]
      },
      {
        "input": "What are best practices for monitoring data pipelines",
        "output": [
          "Track pipeline latency, throughput, and error rates at each stage",
          "Implement alerting for SLA breaches and data freshness issues",
          "Use lineage tracking to understand data flow and impact",
          "Monitor data quality metrics: completeness, accuracy, freshness",
          "Set up automated incident response for common failures"
        ]
      }
    ],
    "best_practices": [
      "Design for failure with proper error handling and retry mechanisms",
      "Implement data quality checks at ingestion, transformation, and output stages",
      "Use incremental processing and strategic caching to optimize performance"
    ],
    "anti_patterns": [
      "Building monolithic pipelines without proper error handling",
      "Skipping data validation and not monitoring pipeline health",
      "Processing data in real-time when batch processing would suffice"
    ],
    "faq": [
      {
        "question": "Which tools does this skill support?",
        "answer": "Python, SQL, Spark, Airflow, dbt, Kafka, Databricks, Snowflake, BigQuery, and major cloud platforms."
      },
      {
        "question": "What scale can this skill help design for?",
        "answer": "Designs support from startup scale to enterprise workloads with proper horizontal scaling patterns."
      },
      {
        "question": "How does this integrate with existing data infrastructure?",
        "answer": "Provides architecture guidance compatible with existing ETL tools, data warehouses, and cloud platforms."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "This is a prompt-based skill. No data is stored or transmitted. All processing happens in your environment."
      },
      {
        "question": "Why are the Python scripts stub implementations?",
        "answer": "Scripts serve as templates. Actual implementation depends on your specific environment and requirements."
      },
      {
        "question": "How does this differ from hiring a data engineer?",
        "answer": "This skill provides expert guidance and patterns. Actual implementation, deployment, and maintenance still require human engineers."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "data_modeling_patterns.md",
          "type": "file",
          "path": "references/data_modeling_patterns.md",
          "lines": 81
        },
        {
          "name": "data_pipeline_architecture.md",
          "type": "file",
          "path": "references/data_pipeline_architecture.md",
          "lines": 81
        },
        {
          "name": "dataops_best_practices.md",
          "type": "file",
          "path": "references/dataops_best_practices.md",
          "lines": 81
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "data_quality_validator.py",
          "type": "file",
          "path": "scripts/data_quality_validator.py",
          "lines": 101
        },
        {
          "name": "etl_performance_optimizer.py",
          "type": "file",
          "path": "scripts/etl_performance_optimizer.py",
          "lines": 101
        },
        {
          "name": "pipeline_orchestrator.py",
          "type": "file",
          "path": "scripts/pipeline_orchestrator.py",
          "lines": 101
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 227
    }
  ]
}
