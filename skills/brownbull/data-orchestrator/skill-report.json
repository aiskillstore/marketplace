{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T15:33:15.775Z",
    "slug": "brownbull-data-orchestrator",
    "source_url": "https://github.com/Brownbull/ayni_core/tree/main/.claude/skills/data-orchestrator",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "c9649b6306012295a7e87d0542d8bb371d6805ccd0fd4e47f0b8e41707557e3c",
    "tree_hash": "ea9da643d2c40f6beaca0d3cda7e1d5e58e803d5f122bde8562d5ba2ada8bde0"
  },
  "skill": {
    "name": "data-orchestrator",
    "description": "Coordinates data pipeline tasks including ETL processes, analytics workflows, and feature engineering. Enforces data quality standards with 95 percent minimum threshold for completeness and accuracy.",
    "summary": "Orchestrates data pipelines, ETL operations, and analytics workflows with quality enforcement.",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "Brownbull",
    "license": "MIT",
    "category": "data",
    "tags": [
      "data-pipeline",
      "etl",
      "analytics",
      "feature-engineering",
      "data-quality"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Static analysis detected 34 pattern matches, all evaluated as false positives. The skill contains documentation and configuration examples for data orchestration workflows. No executable code, network requests, or security risks identified. Safe for publication.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 647,
    "audit_model": "claude",
    "audited_at": "2026-01-21T15:33:15.775Z",
    "risk_factors": []
  },
  "content": {
    "user_title": "Orchestrate Data Pipelines with Quality Enforcement",
    "value_statement": "Managing complex data workflows across ETL processes, analytics, and feature engineering requires coordinated orchestration and quality enforcement. This skill acts as a CTO-Data role, coordinating data tasks while maintaining 95 percent minimum quality standards.",
    "seo_keywords": [
      "Claude",
      "Codex",
      "Claude Code",
      "data pipeline",
      "ETL orchestration",
      "data quality",
      "feature engineering",
      "analytics workflow",
      "data governance",
      "pipeline automation"
    ],
    "actual_capabilities": [
      "Coordinates ETL and ELT pipeline workflows with quality validation",
      "Manages feature engineering tasks with dependency tracking",
      "Enforces data quality standards including completeness and accuracy checks",
      "Maintains pipeline state and context in structured JSON format",
      "Tracks data lineage and schema changes across sources",
      "Orchestrates multiple data skills for complex workflows"
    ],
    "limitations": [
      "Does not execute actual data processing code, only coordinates tasks",
      "Requires existing data infrastructure and tools to be in place",
      "Cannot enforce quality standards without configured validation rules",
      "Documentation-based skill without runtime code execution capabilities"
    ],
    "use_cases": [
      {
        "title": "Daily ETL Pipeline Coordination",
        "description": "Coordinate daily aggregation pipelines that process customer transactions, validate data quality, and update analytics tables with proper error handling and monitoring.",
        "target_user": "Data Engineers"
      },
      {
        "title": "Feature Engineering Workflow",
        "description": "Manage feature creation tasks for machine learning models, tracking dependencies between features and ensuring consistent calculation logic across environments.",
        "target_user": "Data Scientists"
      },
      {
        "title": "Data Quality Monitoring",
        "description": "Orchestrate quality checks across multiple data sources, tracking completeness, accuracy, and timeliness metrics to maintain data governance standards.",
        "target_user": "Analytics Teams"
      }
    ],
    "prompt_templates": [
      {
        "title": "Set Up Daily Pipeline",
        "prompt": "Set up a daily ETL pipeline that extracts customer transactions, aggregates revenue by product category, and loads results to the analytics database with quality checks.",
        "scenario": "Initial pipeline setup for business intelligence reporting"
      },
      {
        "title": "Create Revenue Feature",
        "prompt": "Create a feature called revenue_30d that calculates total customer revenue over the last 30 days, including handling for missing data and edge cases.",
        "scenario": "Feature engineering for customer segmentation model"
      },
      {
        "title": "Implement Quality Monitoring",
        "prompt": "Implement data quality monitoring for the transactions table, checking for completeness, valid date ranges, and duplicate detection with alerting.",
        "scenario": "Adding quality gates to existing data pipeline"
      },
      {
        "title": "Coordinate Multi-Source Pipeline",
        "prompt": "Coordinate a pipeline that combines customer data from three sources, resolves conflicts using defined business rules, validates cross-dataset consistency, and updates the master customer table.",
        "scenario": "Complex data integration with multiple dependencies"
      }
    ],
    "output_examples": [
      {
        "input": "Set up a daily pipeline for customer segmentation",
        "output": "I will coordinate the customer segmentation pipeline setup. First, I will create the pipeline definition in ai-state/active/data/pipelines.json with the daily schedule. Next, I will configure data quality checks for completeness and accuracy. Then I will set up the feature dependencies including customer transactions and demographics. Finally, I will add monitoring for pipeline success rate and processing time."
      },
      {
        "input": "What quality checks should I add for the transactions table?",
        "output": "For the transactions table, I recommend these quality checks: Completeness checks for required fields like transaction_id, customer_id, amount, and timestamp. Accuracy validation ensuring amounts are positive and within expected ranges. Consistency rules checking that customer_ids exist in the customers table. Timeliness metrics confirming data arrives within 1 hour of transaction time. Uniqueness constraints on transaction_id. Validity checks for proper date formats and currency codes."
      },
      {
        "input": "How do I track dependencies between features?",
        "output": "Feature dependencies are tracked in ai-state/active/data/features.json. Each feature definition includes a dependencies array listing source tables and other features required for calculation. When you create a new feature, document the upstream dependencies and the skill will validate they exist before execution. This ensures proper ordering in pipeline execution and helps identify impact when source data changes."
      }
    ],
    "best_practices": [
      "Define clear data quality thresholds before implementing pipelines and enforce them at each transformation stage to catch issues early.",
      "Maintain comprehensive data lineage tracking from source to destination to enable impact analysis when schemas change or data issues occur.",
      "Implement checkpoint and recovery mechanisms in long-running pipelines to avoid reprocessing from the beginning after failures."
    ],
    "anti_patterns": [
      "Processing data without input validation leads to cascading errors downstream. Always validate data quality at pipeline entry points.",
      "Hardcoding transformation logic without documentation makes pipelines unmaintainable. Document business rules and version control feature definitions.",
      "Running pipelines without monitoring or alerting delays detection of failures. Implement comprehensive observability from the start."
    ],
    "faq": [
      {
        "question": "What is the minimum data quality score required?",
        "answer": "The skill enforces a 95 percent minimum quality threshold for completeness and accuracy metrics. Pipelines that fall below this threshold will trigger alerts and may be blocked from loading data to production tables."
      },
      {
        "question": "How does the skill coordinate between different data tasks?",
        "answer": "The skill maintains pipeline state in ai-state/active/data directory using JSON files for pipelines, features, and quality metrics. It assigns tasks to specialized skills like etl-skill or feature-engineering-skill with full context packages including dependencies and standards."
      },
      {
        "question": "Can this skill execute actual ETL code?",
        "answer": "No, this is a coordination skill that orchestrates data workflows but does not execute processing code. It works with existing data infrastructure and tools, providing task coordination and quality enforcement."
      },
      {
        "question": "What happens when a pipeline fails quality checks?",
        "answer": "When quality checks fail, the skill logs the failure details, calculates quality scores, and can prevent data loading if below thresholds. It broadcasts events to notify other systems and updates the quality metrics in ai-state for tracking trends."
      },
      {
        "question": "How are feature dependencies tracked?",
        "answer": "Features are registered in features.json with explicit dependency arrays listing required tables and upstream features. The skill validates dependencies exist before execution and maintains version history for reproducibility."
      },
      {
        "question": "Does this work with real-time streaming data?",
        "answer": "Yes, the skill supports both batch and stream processing patterns. For streaming, it provides guidance on windowing, late arrival handling, and state management while maintaining the same quality standards."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 236
    }
  ]
}
