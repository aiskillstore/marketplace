{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T14:50:36.366Z",
    "slug": "acskamloops-coercion-duress",
    "source_url": "https://github.com/ACSKamloops/shs-engine/tree/master/.codex/skills/coercion-duress",
    "source_ref": "master",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "d25eeae4310441abc8c9919bfb00597d60408202d143e4e32677fdb1c8f3de72",
    "tree_hash": "0bce24ecc263a06315c451c35d2a4e9ffc917e3e85e639da78f0c06e4e741103"
  },
  "skill": {
    "name": "coercion-duress",
    "description": "Clerk for forced surrenders, threats, procedural irregularities, and lack of informed consent; use for Coercion_Duress queue.",
    "summary": "Analyze and index historical legal documents for evidence of coercion, duress, and procedural irregularities.",
    "icon": "ðŸ“‹",
    "version": "1.0.0",
    "author": "ACSKamloops",
    "license": "MIT",
    "tags": [
      "legal-documents",
      "evidence-analysis",
      "archival-research",
      "transcription"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Static scanner flagged patterns related to external commands and file operations. Evaluation confirms these are legitimate document processing workflows. No malicious intent found. The skill runs hardcoded Python scripts for fetching, analyzing, and submitting legal document analysis tasks. All commands are predefined workflow operations with no user input injection risk.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 35,
            "line_end": 36
          },
          {
            "file": "SKILL.md",
            "line_start": 124,
            "line_end": 125
          },
          {
            "file": "SKILL.md",
            "line_start": 133,
            "line_end": 134
          },
          {
            "file": "SKILL.md",
            "line_start": 138,
            "line_end": 139
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 40,
            "line_end": 43
          },
          {
            "file": "SKILL.md",
            "line_start": 62,
            "line_end": 91
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 965,
    "audit_model": "claude",
    "audited_at": "2026-01-21T14:50:36.366Z"
  },
  "content": {
    "user_title": "Analyze Documents for Coercion Evidence",
    "value_statement": "Organizations processing historical legal documents need accurate, consistent analysis of evidence related to forced surrenders and duress. This clerk skill provides a structured workflow for identifying and indexing such evidence while maintaining strict factual standards.",
    "seo_keywords": [
      "coercion analysis",
      "duress evidence",
      "legal document clerk",
      "historical document processing",
      "transcription",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Read and analyze JSON task files containing historical document metadata",
      "Identify evidence of coercion, threats, and procedural irregularities in text",
      "Apply legal-grade verbatim protocol for factual document summaries",
      "Generate structured JSON analysis output with evidence quotes",
      "Submit processed tasks to workflow pipeline for review"
    ],
    "limitations": [
      "Does not access external URLs or make network requests during processing",
      "Does not modify source documents - works on copies only",
      "Does not make legal conclusions - provides factual summaries only",
      "Does not automate decision-making - human clerk workflow only"
    ],
    "use_cases": [
      {
        "title": "Historical Archive Processing",
        "description": "Process batches of historical legal documents for the Coercion_Duress queue. Identify and index evidence of forced surrenders, threats, and lack of informed consent.",
        "target_user": "Archive clerks and legal researchers"
      },
      {
        "title": "Commission Evidence Review",
        "description": "Analyze McKenna-McBride Commission documents (1913-1916) for evidence of coercive adjustments. Apply factual baseline standards for reliable conclusions.",
        "target_user": "Commission researchers and historians"
      },
      {
        "title": "Document Provenance Verification",
        "description": "Validate document provenance and reliability scores. Flag tasks with unknown provenance for additional review before processing.",
        "target_user": "Quality assurance teams"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Document Analysis",
        "prompt": "Read the JSON task file and analyze each document for evidence of coercion, duress, or procedural irregularities. For each task, extract: doc_id, title, date, provenance, reliability, and key evidence quotes. Output to [Batch_ID]_Analysis.json.",
        "scenario": "Starting a new batch of document analysis tasks"
      },
      {
        "title": "Coercion Pattern Recognition",
        "prompt": "Search the document text for language indicating pressure or threats. Look for phrases like 'advised to sign', 'better for them', or 'inevitable'. Document X marks without witnesses and meetings without interpreters. Quote exact text for each finding.",
        "scenario": "Identifying implicit pressure or coercion indicators"
      },
      {
        "title": "Metadata Validation",
        "prompt": "Verify each document has complete metadata: doc_id (not filename), title from content, 4-digit year date, and provenance. Reject 'Unknown' values when information exists in text. Apply validation gates before submission.",
        "scenario": "Ensuring metadata integrity before submission"
      },
      {
        "title": "Submission and Flagging",
        "prompt": "Run submission validation: check for forbidden opinion words, verify length requirements, confirm metadata integrity. If issues found, fix or flag with appropriate reason (Irrelevant, OCR_Failure, Provenance_Failure). Submit clean batches via submit-task.",
        "scenario": "Finalizing and submitting analyzed batches"
      }
    ],
    "output_examples": [
      {
        "input": "JSON task file with historical document references",
        "output": [
          "Batch analysis JSON with task results",
          "Each result includes doc_id, title, date, provenance, reliability",
          "Evidence quotes with page numbers and significance"
        ]
      },
      {
        "input": "Document with procedural irregularities",
        "output": [
          "Identified X mark without witness",
          "Quote: 'Document signed with X mark, no witness present'",
          "Significance: 'Indicates potential lack of informed consent'"
        ]
      },
      {
        "input": "Corrupt or irrelevant file",
        "output": [
          "Task flagged with reason: Irrelevant",
          "Logged to Flagged_Tasks.tsv",
          "Available for Investigator Agent audit"
        ]
      }
    ],
    "best_practices": [
      "Follow the Context Refresh Protocol - re-read instructions after every 5 tasks to prevent hallucination",
      "Apply Legal-Grade Verbatim Protocol - extract exact quotes and avoid opinions or conclusions",
      "Enforce Submission Validation Gates - check length, forbidden words, and metadata completeness before submit"
    ],
    "anti_patterns": [
      "Do not write Python scripts to scan or filter tasks - manual evaluation is required",
      "Do not use opinion words like 'suggests', 'implies', or 'likely' in analysis output",
      "Do not return 'Unknown' for metadata when the information exists in the document content"
    ],
    "faq": [
      {
        "question": "What documents should I flag as provenance failures?",
        "answer": "Flag tasks when the provenance field is 'Incoming' or 'Unknown'. The source must have verifiable origin for legal-grade analysis."
      },
      {
        "question": "How do I handle documents with garbled text?",
        "answer": "Use the flag-task command with reason 'OCR_Failure'. This automatically moves the source file to the Vision Pipeline for re-processing."
      },
      {
        "question": "What date formats are accepted?",
        "answer": "Use 4-digit Year (YYYY) or 'Undated'. 'Unknown' is forbidden if any date information exists in the document."
      },
      {
        "question": "Can I use the document filename as doc_id?",
        "answer": "Only if doc_id is missing in the input. Prefer StableID (e.g., D123) from the source system when available."
      },
      {
        "question": "How do I identify implicit coercion?",
        "answer": "Look for language of threat ('must agree or lose everything'), questionable signatures ('X mark' without witnesses), and protests about lack of understanding or translation."
      },
      {
        "question": "What happens after I submit a batch?",
        "answer": "Tasks move to ManagerReview status. The batch is not final until a Manager runs the manager-approve command."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 161
    }
  ]
}
