{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T11:18:04.115Z",
    "slug": "bellabe-reasoning-abductive",
    "source_url": "https://github.com/BellaBe/lean-os/tree/main/.claude/skills/reasoning-abductive",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "278173750c92cfde5e381bd278b5ccdc0c9aba31b34b5bb801a4b078d29562db",
    "tree_hash": "6e1950594518dde6f559486d44825604bcaeff126bef1db0c50630e1e00af771"
  },
  "skill": {
    "name": "reasoning-abductive",
    "description": "Generate and evaluate explanatory hypotheses from incomplete observations. Use when diagnosing anomalies, explaining unexpected outcomes, or inferring causes from effects. Produces ranked hypotheses with evidence and confidence scores.",
    "summary": "Generate and evaluate explanatory hypotheses from incomplete observations. Use when diagnosing anoma...",
    "icon": "üîç",
    "version": "1.0.0",
    "author": "BellaBe",
    "license": "MIT",
    "category": "research",
    "tags": [
      "reasoning",
      "diagnosis",
      "hypothesis",
      "inference"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure prompt-based skill containing only documentation. No executable code, network operations, filesystem access, or external command execution. This is a safe documentation skill for reasoning guidance.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 310,
    "audit_model": "claude",
    "audited_at": "2026-01-10T11:18:04.115Z"
  },
  "content": {
    "user_title": "Generate Explanatory Hypotheses from Observations",
    "value_statement": "Diagnose unexpected outcomes and anomalies by generating ranked explanations with evidence. This skill helps identify root causes when you have incomplete data and need to infer what happened.",
    "seo_keywords": [
      "abductive reasoning",
      "hypothesis generation",
      "diagnostic reasoning",
      "root cause analysis",
      "Claude Code",
      "Claude",
      "OpenAI Codex",
      "inference to best explanation",
      "anomaly diagnosis"
    ],
    "actual_capabilities": [
      "Transform raw observations into structured anomaly descriptions",
      "Generate diverse explanatory hypotheses across technical, product, market, and operational categories",
      "Score hypotheses against evidence using explanatory power, simplicity, coherence, testability, and prior probability",
      "Rank hypotheses with confidence scores and identify the most probable cause",
      "Provide ranked output with supporting and contradicting evidence",
      "Suggest next reasoning mode and testable actions"
    ],
    "limitations": [
      "Does not access external data sources or perform live investigation",
      "Requires user to provide initial observation data",
      "Confidence scores are estimates based on provided evidence",
      "Does not execute actions or run tests"
    ],
    "use_cases": [
      {
        "target_user": "Software Engineers",
        "title": "Debug Production Incidents",
        "description": "Diagnose why metrics dropped or errors occurred when direct evidence is incomplete"
      },
      {
        "target_user": "Product Managers",
        "title": "Explain Metric Changes",
        "description": "Understand why conversion, engagement, or other business metrics changed unexpectedly"
      },
      {
        "target_user": "Data Analysts",
        "title": "Investigate Anomalies",
        "description": "Generate candidate explanations for data anomalies and rank them by likelihood"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Diagnosis",
        "scenario": "Something unexpected happened",
        "prompt": "Use abductive reasoning to explain this observation: [describe what happened, when, and the unexpected aspect]"
      },
      {
        "title": "With Context",
        "scenario": "Have baseline data",
        "prompt": "Use abductive reasoning. Observation: [current situation]. Baseline: [expected state]. Help me generate hypotheses and evaluate them against available evidence: [list any data you have]"
      },
      {
        "title": "Technical Issue",
        "scenario": "System or code issue",
        "prompt": "Diagnose this technical anomaly using abductive reasoning. What I observed: [error, behavior, or metric change]. Available logs/data: [relevant information]. Generate and rank possible causes."
      },
      {
        "title": "Business Metric",
        "scenario": "Business metric dropped",
        "prompt": "Apply the four-stage abductive process to this business metric change: [describe the metric, its historical baseline, and the current deviation]. Generate at least 5 hypotheses across technical, product, market, and operational categories. Score each against available evidence."
      }
    ],
    "output_examples": [
      {
        "input": "Use abductive reasoning. Our enterprise conversion dropped from 15% to 9% last quarter. SMB remained stable at 12%.",
        "output": [
          "Primary Cause: Sales cycle elongation (not true drop) + AM departures",
          "Confidence: 0.72",
          "Mechanism: Economic uncertainty extended CFO approval cycles by 45 days; AM departures created relationship gaps in 6 key accounts",
          "Top Hypothesis: Website performance degradation (score: 0.78, confidence: 0.75)",
          "Supporting Evidence: Page load time increased 2s in Q4, Mobile bounce rate up 15%",
          "Next Actions: Wait 45 days to verify delayed deals close; Immediately backfill AM roles"
        ]
      }
    ],
    "best_practices": [
      "Provide as much context and evidence as available to improve hypothesis scoring accuracy",
      "Generate at least 5 hypotheses across different categories before evaluating them",
      "Explicitly look for contradicting evidence to avoid confirmation bias",
      "Use the confidence score to determine if more evidence gathering is needed"
    ],
    "anti_patterns": [
      "Accepting the first plausible explanation without generating alternative hypotheses",
      "Only seeking evidence that supports your preferred hypothesis",
      "Stopping before completing all four stages when confidence is below 0.6",
      "Using this skill when the cause is already known (use causal reasoning instead)"
    ],
    "faq": [
      {
        "question": "What platforms is this skill compatible with?",
        "answer": "This skill works with Claude, OpenAI Codex, and Claude Code. It is a prompt-based skill that provides structured reasoning templates."
      },
      {
        "question": "What data do I need to provide?",
        "answer": "Provide the observation (what happened, when, how surprising), any baseline or context data, and available evidence for evaluation."
      },
      {
        "question": "Can I chain this with other reasoning skills?",
        "answer": "Yes. Use abductive to diagnose, then switch to causal for execution or counterfactual to evaluate alternatives."
      },
      {
        "question": "Is my data stored or transmitted?",
        "answer": "No. This is a pure prompt skill. Your observation data is only used to generate the reasoning output in your current session."
      },
      {
        "question": "Why are my confidence scores low?",
        "answer": "Low confidence typically means limited evidence or conflicting data points. Gather more supporting/contradicting evidence or refine the observation."
      },
      {
        "question": "How is this different from causal reasoning?",
        "answer": "Abductive reasoning explains observations from effects to causes (inference to best explanation). Causal reasoning executes actions to achieve known outcomes."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
