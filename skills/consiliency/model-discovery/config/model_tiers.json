{
  "$schema": "./model_tiers.schema.json",
  "version": "1.0.0",
  "last_updated": "2025-12-17",
  "description": "Static mapping of AI models to performance tiers (fast/default/heavy) for routing decisions",

  "tiers": {
    "fast": {
      "description": "Optimized for speed and cost efficiency. Use for simple tasks, quick iterations, and high-volume operations.",
      "priority": ["latency", "cost", "throughput"]
    },
    "default": {
      "description": "Balanced performance for general-purpose tasks. Good reasoning with reasonable speed/cost.",
      "priority": ["quality", "latency", "cost"]
    },
    "heavy": {
      "description": "Maximum capability for complex reasoning, research, and difficult tasks. Higher latency and cost acceptable.",
      "priority": ["quality", "reasoning", "context"]
    }
  },

  "providers": {
    "anthropic": {
      "display_name": "Anthropic Claude",
      "api_key_env": "ANTHROPIC_API_KEY",
      "multimodal": true,
      "models": {
        "fast": {
          "id": "claude-haiku-4-5",
          "name": "Claude Haiku 4.5",
          "cli_name": "haiku",
          "context_window": 200000,
          "multimodal": true,
          "notes": "Fastest Claude model, excellent for quick tasks"
        },
        "default": {
          "id": "claude-sonnet-4-5",
          "name": "Claude Sonnet 4.5",
          "cli_name": "sonnet",
          "context_window": 200000,
          "multimodal": true,
          "notes": "Best balance of speed and capability"
        },
        "heavy": {
          "id": "claude-opus-4-5",
          "name": "Claude Opus 4.5",
          "cli_name": "opus",
          "context_window": 200000,
          "multimodal": true,
          "notes": "Most capable Claude model for complex reasoning"
        }
      },
      "deprecated": []
    },

    "openai": {
      "display_name": "OpenAI",
      "api_key_env": "OPENAI_API_KEY",
      "multimodal": true,
      "models": {
        "fast": {
          "id": "gpt-5.2-mini",
          "name": "GPT-5.2 Mini",
          "cli_name": "gpt-5.2-mini",
          "context_window": 128000,
          "multimodal": true,
          "notes": "Fast and cost-effective for simple tasks"
        },
        "default": {
          "id": "gpt-5.2",
          "name": "GPT-5.2",
          "cli_name": "gpt-5.2",
          "context_window": 128000,
          "multimodal": true,
          "notes": "Latest flagship model, balanced performance"
        },
        "heavy": {
          "id": "gpt-5.2-pro",
          "name": "GPT-5.2 Pro",
          "cli_name": "gpt-5.2-pro",
          "context_window": 128000,
          "multimodal": true,
          "notes": "Maximum capability, extended reasoning"
        }
      },
      "reasoning_models": {
        "fast": "gpt-5.2-mini",
        "default": "gpt-5.2",
        "heavy": "gpt-5.2-pro"
      },
      "codex_models": {
        "fast": "gpt-5.2-codex-mini",
        "default": "gpt-5.2-codex",
        "heavy": "gpt-5.2-codex-max"
      },
      "deprecated": []
    },

    "gemini": {
      "display_name": "Google Gemini",
      "api_key_env": "GOOGLE_API_KEY",
      "multimodal": true,
      "models": {
        "fast": {
          "id": "models/gemini-3-flash-lite",
          "name": "Gemini 3 Flash-Lite",
          "cli_name": "gemini-3-flash-lite",
          "context_window": 1048576,
          "multimodal": true,
          "notes": "Fast and cost-efficient with 1M context window"
        },
        "default": {
          "id": "models/gemini-3-pro",
          "name": "Gemini 3 Pro",
          "cli_name": "gemini-3-pro",
          "context_window": 1048576,
          "multimodal": true,
          "notes": "Production-ready with massive context"
        },
        "heavy": {
          "id": "models/gemini-3-deep-think",
          "name": "Gemini 3 Deep Think",
          "cli_name": "gemini-3-deep-think",
          "context_window": 1048576,
          "multimodal": true,
          "notes": "Latest and most capable Gemini model"
        }
      },
      "deprecated": []
    },

    "ollama": {
      "display_name": "Ollama (Local)",
      "api_key_env": null,
      "host_env": "OLLAMA_HOST",
      "multimodal": false,
      "models": {
        "fast": {
          "id": "phi3.5:latest",
          "name": "Phi 3.5",
          "cli_name": "phi3.5",
          "context_window": 128000,
          "multimodal": false,
          "notes": "Small local model for quick tasks"
        },
        "default": {
          "id": "llama3.2:latest",
          "name": "Llama 3.2",
          "cli_name": "llama3.2",
          "context_window": 128000,
          "multimodal": false,
          "notes": "Balanced local model"
        },
        "heavy": {
          "id": "llama3.3:70b",
          "name": "Llama 3.3 70B",
          "cli_name": "llama3.3:70b",
          "context_window": 128000,
          "multimodal": false,
          "notes": "Largest local model for complex tasks"
        }
      },
      "notes": "Models depend on what user has pulled locally"
    }
  },

  "cli_mappings": {
    "claude-code": {
      "provider": "anthropic",
      "fast": "haiku",
      "default": "sonnet",
      "heavy": "opus"
    },
    "codex-cli": {
      "provider": "openai",
      "model_family": "codex_models",
      "fast": "gpt-5.2-codex-mini",
      "default": "gpt-5.2-codex",
      "heavy": "gpt-5.2-codex-max"
    },
    "gemini-cli": {
      "provider": "gemini",
      "fast": "gemini-3-flash-lite",
      "default": "gemini-3-pro",
      "heavy": "gemini-3-deep-think"
    },
    "cursor-cli": {
      "provider": "mixed",
      "fast": "gpt-5.2",
      "default": "sonnet-4.5",
      "heavy": "sonnet-4.5-thinking"
    },
    "opencode-cli": {
      "provider": "anthropic",
      "format": "provider/model",
      "fast": "anthropic/claude-haiku-4-5",
      "default": "anthropic/claude-sonnet-4-5",
      "heavy": "anthropic/claude-opus-4-5"
    },
    "copilot-cli": {
      "provider": "anthropic",
      "fast": "claude-sonnet-4.5",
      "default": "claude-sonnet-4.5",
      "heavy": "claude-sonnet-4.5",
      "notes": "Model selection via /model command in-session"
    }
  },

  "model_patterns": {
    "tier_inference": {
      "heavy_patterns": ["-pro", "-opus", "-max", "thinking", "deep-research"],
      "fast_patterns": ["-mini", "-nano", "-flash", "-lite", "-haiku"],
      "default_patterns": ["-sonnet", "^gpt-5\\.2$", "^gpt-5\\.2-codex$", "^gemini-3-pro$"]
    },
    "multimodal_patterns": ["gpt-5", "claude", "gemini", "vision"],
    "reasoning_patterns": ["thinking", "deep-research", "reasoner"]
  }
}
