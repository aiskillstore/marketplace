# æœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å— (v2.3)

## ğŸ¯ å·¥å…·æ¦‚è¿°
**åŠŸèƒ½**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–  
**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼  

**æ–°å¢åŠŸèƒ½**ï¼š
- âœ… **XGBoost 1.7.6**ï¼šé«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘æ¨¡å‹
- âœ… **pmdarima 2.0.4**ï¼šè‡ªåŠ¨åŒ–ARIMAæ—¶é—´åºåˆ—å»ºæ¨¡
- âœ… å¢å¼ºçš„æ—¶é—´åºåˆ—åˆ†æèƒ½åŠ›
- âœ… éçº¿æ€§æ¨¡å‹ä¸çº¿æ€§æ¨¡å‹çš„å¯¹æ¯”åˆ†æ

## ğŸ“Š åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡æ¿

### æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

def prepare_ml_data():
    """æœºå™¨å­¦ä¹ æ•°æ®å‡†å¤‡ç¤ºä¾‹"""
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
    np.random.seed(42)
    n_samples = 1000
    
    # å›å½’é—®é¢˜æ•°æ®
    X_reg = np.random.normal(0, 1, (n_samples, 5))
    y_reg = 2 * X_reg[:, 0] + 1.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.normal(0, 0.5, n_samples)
    
    # åˆ†ç±»é—®é¢˜æ•°æ®
    X_clf = np.random.normal(0, 1, (n_samples, 4))
    y_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)
    
    print("=== æ•°æ®å‡†å¤‡å®Œæˆ ===")
    print(f"æ ·æœ¬æ•°é‡: {n_samples}")
    print(f"å›å½’ç‰¹å¾ç»´åº¦: {X_reg.shape[1]}")
    print(f"åˆ†ç±»ç‰¹å¾ç»´åº¦: {X_clf.shape[1]}")
    print(f"åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_clf, return_counts=True)}")
    
    return X_reg, y_reg, X_clf, y_clf

# ä½¿ç”¨ç¤ºä¾‹
# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()
```

### æ ‡å‡†æœºå™¨å­¦ä¹ å·¥ä½œæµ
```python
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report
from sklearn.model_selection import cross_val_score

def standard_ml_pipeline(X, y, problem_type='regression'):
    """æ ‡å‡†æœºå™¨å­¦ä¹ æµç¨‹"""
    
    print(f"=== å¼€å§‹ {problem_type} æ¨¡å‹è®­ç»ƒ ===")
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42,
        stratify=y if problem_type == 'classification' else None
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # é€‰æ‹©æ¨¡å‹
    if problem_type == 'regression':
        model = RandomForestRegressor(n_estimators=100, random_state=42)
    else:
        model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train_scaled, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test_scaled)
    
    # æ¨¡å‹è¯„ä¼°
    if problem_type == 'regression':
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        
        print(f"å›å½’æ¨¡å‹æ€§èƒ½:")
        print(f"  MSE: {mse:.4f}")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  RÂ²: {r2:.4f}")
        
        metrics = {'mse': mse, 'rmse': rmse, 'r2': r2}
    else:
        accuracy = accuracy_score(y_test, y_pred)
        print(f"åˆ†ç±»æ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))
        
        metrics = {'accuracy': accuracy}
    
    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, 
                               scoring='r2' if problem_type == 'regression' else 'accuracy')
    print(f"äº¤å‰éªŒè¯å¹³å‡å¾—åˆ†: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    
    return {
        'model': model,
        'metrics': metrics,
        'X_test': X_test,
        'y_test': y_test,
        'y_pred': y_pred,
        'cv_scores': cv_scores
    }

# ä½¿ç”¨ç¤ºä¾‹
# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()
# regression_results = standard_ml_pipeline(X_reg, y_reg, 'regression')
# classification_results = standard_ml_pipeline(X_clf, y_clf, 'classification')
```

## ğŸ“ˆ å›å½’åˆ†æå®Œæ•´å·¥ä½œæµ

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

def complete_regression_analysis():
    """å®Œæ•´çš„å›å½’åˆ†æå·¥ä½œæµ"""
    
    print("=== å¼€å§‹å›å½’åˆ†æ ===")
    
    # 1. æ•°æ®ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 500
    
    # åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾
    feature1 = np.random.normal(50, 15, n_samples)  # å¹´é¾„
    feature2 = np.random.normal(100, 25, n_samples) # æ”¶å…¥
    feature3 = np.random.normal(10, 3, n_samples)   # æ•™è‚²å¹´é™
    feature4 = np.random.normal(0, 1, n_samples)    # å™ªå£°ç‰¹å¾
    
    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿæˆ¿ä»·ï¼‰
    target = (50 * feature1 + 80 * feature2 + 5000 * feature3 + 
              10 * feature1 * feature3 + np.random.normal(0, 10000, n_samples))
    
    df = pd.DataFrame({
        'å¹´é¾„': feature1,
        'æ”¶å…¥': feature2,
        'æ•™è‚²å¹´é™': feature3,
        'å™ªå£°ç‰¹å¾': feature4,
        'æˆ¿ä»·': target
    })
    
    print("æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
    print(f"ç‰¹å¾åˆ—è¡¨: {list(df.columns[:-1])}")
    print(f"ç›®æ ‡å˜é‡: {df.columns[-1]}")
    
    # 2. æ•°æ®æ¢ç´¢
    print("\n=== æ•°æ®æ¢ç´¢ ===")
    print("æ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
    print(df.describe())
    
    # ç›¸å…³æ€§åˆ†æ
    correlation = df.corr()['æˆ¿ä»·'].sort_values(ascending=False)
    print("\nç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§:")
    for feature, corr in correlation.items():
        if feature != 'æˆ¿ä»·':
            print(f"  {feature}: {corr:.3f}")
    
    # 3. æ¨¡å‹è®­ç»ƒ
    X = df.drop('æˆ¿ä»·', axis=1)
    y = df['æˆ¿ä»·']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    # 4. æ¨¡å‹è¯„ä¼°
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    print(f"\n=== æ¨¡å‹æ€§èƒ½ ===")
    print(f"å‡æ–¹è¯¯å·® (MSE): {mse:,.2f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:,.2f}")
    print(f"å†³å®šç³»æ•° (RÂ²): {r2:.4f}")
    
    # 5. ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'ç‰¹å¾': X.columns,
        'é‡è¦æ€§': model.feature_importances_
    }).sort_values('é‡è¦æ€§', ascending=False)
    
    print(f"\n=== ç‰¹å¾é‡è¦æ€§ ===")
    for _, row in feature_importance.iterrows():
        print(f"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}")
    
    # 6. å¯è§†åŒ–åˆ†æ
    plt.figure(figsize=(15, 10))
    
    # å®é™…å€¼ vs é¢„æµ‹å€¼
    plt.subplot(2, 3, 1)
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('å®é™…å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')
    plt.grid(True, alpha=0.3)
    
    # æ®‹å·®åˆ†æ
    plt.subplot(2, 3, 2)
    residuals = y_test - y_pred
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®åˆ†æ')
    plt.grid(True, alpha=0.3)
    
    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
    plt.subplot(2, 3, 3)
    top_features = feature_importance.head(5)
    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])
    plt.xlabel('é‡è¦æ€§')
    plt.title('Top 5 ç‰¹å¾é‡è¦æ€§')
    plt.gca().invert_yaxis()
    
    # è¯¯å·®åˆ†å¸ƒ
    plt.subplot(2, 3, 4)
    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')
    plt.xlabel('æ®‹å·®')
    plt.ylabel('é¢‘æ•°')
    plt.title('è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    # ç›¸å¯¹è¯¯å·®
    plt.subplot(2, 3, 5)
    relative_error = np.abs(residuals / y_test) * 100
    plt.hist(relative_error, bins=30, alpha=0.7, edgecolor='black')
    plt.xlabel('ç›¸å¯¹è¯¯å·® (%)')
    plt.ylabel('é¢‘æ•°')
    plt.title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    # é¢„æµ‹è¯¯å·®ç®±çº¿å›¾
    plt.subplot(2, 3, 6)
    plt.boxplot(relative_error)
    plt.ylabel('ç›¸å¯¹è¯¯å·® (%)')
    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 7. æ¨¡å‹è§£é‡Š
    print(f"\n=== æ¨¡å‹è§£é‡Š ===")
    print(f"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if r2 > 0.8 else 'è‰¯å¥½' if r2 > 0.6 else 'ä¸€èˆ¬'}")
    print(f"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}")
    print(f"å»ºè®®: å…³æ³¨{feature_importance.iloc[0]['ç‰¹å¾']}å’Œ{feature_importance.iloc[1]['ç‰¹å¾']}çš„ä¼˜åŒ–")
    
    return {
        'model': model,
        'metrics': {'mse': mse, 'rmse': rmse, 'r2': r2},
        'feature_importance': feature_importance,
        'predictions': y_pred
    }

# ä½¿ç”¨ç¤ºä¾‹
# regression_results = complete_regression_analysis()
```

## ğŸ” åˆ†ç±»åˆ†æå®Œæ•´å·¥ä½œæµ

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import make_classification

def complete_classification_analysis():
    """å®Œæ•´çš„åˆ†ç±»åˆ†æå·¥ä½œæµ"""
    
    print("=== å¼€å§‹åˆ†ç±»åˆ†æ ===")
    
    # 1. æ•°æ®ç”Ÿæˆ
    X, y = make_classification(
        n_samples=1000,
        n_features=8,
        n_informative=5,
        n_redundant=2,
        n_classes=3,
        random_state=42
    )
    
    feature_names = [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]
    df = pd.DataFrame(X, columns=feature_names)
    df['ç±»åˆ«'] = y
    
    print("æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
    print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"ç±»åˆ«æ•°é‡: {len(np.unique(y))}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.unique(y, return_counts=True)}")
    
    # 2. æ•°æ®æ¢ç´¢
    print("\n=== æ•°æ®æ¢ç´¢ ===")
    print("æ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
    print(df.describe())
    
    # 3. æ¨¡å‹è®­ç»ƒ
    X_data = df.drop('ç±»åˆ«', axis=1)
    y_data = df['ç±»åˆ«']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_data, y_data, test_size=0.2, random_state=42, stratify=y_data
    )
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    # 4. æ¨¡å‹è¯„ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\n=== æ¨¡å‹æ€§èƒ½ ===")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # 5. ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'ç‰¹å¾': X_data.columns,
        'é‡è¦æ€§': model.feature_importances_
    }).sort_values('é‡è¦æ€§', ascending=False)
    
    print(f"\n=== ç‰¹å¾é‡è¦æ€§ ===")
    for _, row in feature_importance.iterrows():
        print(f"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}")
    
    # 6. å¯è§†åŒ–åˆ†æ
    plt.figure(figsize=(15, 10))
    
    # æ··æ·†çŸ©é˜µ
    plt.subplot(2, 3, 1)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title('æ··æ·†çŸ©é˜µ')
    
    # ç‰¹å¾é‡è¦æ€§
    plt.subplot(2, 3, 2)
    top_features = feature_importance.head(8)
    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])
    plt.xlabel('é‡è¦æ€§')
    plt.title('ç‰¹å¾é‡è¦æ€§æ’å')
    plt.gca().invert_yaxis()
    
    # ç±»åˆ«åˆ†å¸ƒ
    plt.subplot(2, 3, 3)
    unique, counts = np.unique(y, return_counts=True)
    plt.pie(counts, labels=[f'ç±»åˆ« {cls}' for cls in unique], autopct='%1.1f%%')
    plt.title('ç±»åˆ«åˆ†å¸ƒ')
    
    # åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾
    plt.subplot(2, 3, 4)
    report_dict = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose().iloc[:-3, :-1]
    sns.heatmap(report_df, annot=True, cmap='YlOrRd', fmt='.3f')
    plt.title('åˆ†ç±»æŒ‡æ ‡çƒ­åŠ›å›¾')
    
    # å­¦ä¹ æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰
    plt.subplot(2, 3, 5)
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_scores = []
    test_scores = []
    
    for size in train_sizes:
        n_train = int(size * len(X_train))
        X_train_sub = X_train.iloc[:n_train]
        y_train_sub = y_train.iloc[:n_train]
        
        model_temp = RandomForestClassifier(n_estimators=50, random_state=42)
        model_temp.fit(X_train_sub, y_train_sub)
        
        train_score = model_temp.score(X_train_sub, y_train_sub)
        test_score = model_temp.score(X_test, y_test)
        
        train_scores.append(train_score)
        test_scores.append(test_score)
    
    plt.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒå¾—åˆ†')
    plt.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•å¾—åˆ†')
    plt.xlabel('è®­ç»ƒæ ·æœ¬æ¯”ä¾‹')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('å­¦ä¹ æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ
    plt.subplot(2, 3, 6)
    pred_counts = pd.Series(y_pred).value_counts().sort_index()
    true_counts = pd.Series(y_test).value_counts().sort_index()
    
    x = np.arange(len(true_counts))
    width = 0.35
    
    plt.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.7)
    plt.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.7)
    plt.xlabel('ç±»åˆ«')
    plt.ylabel('æ ·æœ¬æ•°')
    plt.title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 7. æ¨¡å‹è§£é‡Š
    print(f"\n=== æ¨¡å‹è§£é‡Š ===")
    print(f"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if accuracy > 0.9 else 'è‰¯å¥½' if accuracy > 0.8 else 'ä¸€èˆ¬'}")
    print(f"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}")
    print(f"æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«: æŸ¥çœ‹æ··æ·†çŸ©é˜µå¯¹è§’çº¿å¤–çš„æœ€å¤§å€¼")
    
    return {
        'model': model,
        'metrics': {'accuracy': accuracy},
        'feature_importance': feature_importance,
        'predictions': y_pred
    }

# ä½¿ç”¨ç¤ºä¾‹
# classification_results = complete_classification_analysis()
```

## ğŸ“Š ç»Ÿè®¡å»ºæ¨¡åˆ†æ

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf

def statistical_modeling_analysis():
    """ç»Ÿè®¡å»ºæ¨¡åˆ†æ"""
    
    print("=== å¼€å§‹ç»Ÿè®¡å»ºæ¨¡åˆ†æ ===")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 200
    
    data = pd.DataFrame({
        'å¹¿å‘ŠæŠ•å…¥': np.random.normal(1000, 300, n_samples),
        'ä»·æ ¼': np.random.normal(50, 15, n_samples),
        'ä¿ƒé”€æ´»åŠ¨': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),
        'å­£èŠ‚æ€§': np.random.choice([0, 1], n_samples, p=[0.5, 0.5])
    })
    
    # ç”Ÿæˆé”€å”®é¢ï¼ˆä¸ç‰¹å¾æœ‰çœŸå®å…³ç³»ï¼‰
    data['é”€å”®é¢'] = (
        500 + 0.8 * data['å¹¿å‘ŠæŠ•å…¥'] - 5 * data['ä»·æ ¼'] + 
        200 * data['ä¿ƒé”€æ´»åŠ¨'] + 150 * data['å­£èŠ‚æ€§'] + 
        np.random.normal(0, 100, n_samples)
    )
    
    print("æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"æ ·æœ¬æ•°é‡: {len(data)}")
    print(f"ç‰¹å¾: {list(data.columns[:-1])}")
    print("\næ•°æ®æè¿°:")
    print(data.describe())
    
    # 1. OLS å›å½’åˆ†æ
    print("\n=== OLS å›å½’åˆ†æ ===")
    model = smf.ols('é”€å”®é¢ ~ å¹¿å‘ŠæŠ•å…¥ + ä»·æ ¼ + ä¿ƒé”€æ´»åŠ¨ + å­£èŠ‚æ€§', data=data).fit()
    
    print("å›å½’ç»“æœæ‘˜è¦:")
    print(model.summary())
    
    # 2. å…³é”®ç»Ÿè®¡æŒ‡æ ‡
    print(f"\n=== å…³é”®ç»Ÿè®¡æŒ‡æ ‡ ===")
    print(f"RÂ²: {model.rsquared:.4f}")
    print(f"è°ƒæ•´RÂ²: {model.rsquared_adj:.4f}")
    print(f"Fç»Ÿè®¡é‡: {model.fvalue:.2f}")
    print(f"Fç»Ÿè®¡é‡på€¼: {model.f_pvalue:.4f}")
    
    # 3. ç³»æ•°è§£é‡Š
    print(f"\n=== ç³»æ•°è§£é‡Š ===")
    for feature, coef in model.params.items():
        p_value = model.pvalues[feature]
        significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
        print(f"{feature}: {coef:.2f} {significance} (på€¼: {p_value:.4f})")
    
    # 4. æ®‹å·®åˆ†æ
    print(f"\n=== æ®‹å·®åˆ†æ ===")
    residuals = model.resid
    print(f"æ®‹å·®å‡å€¼: {residuals.mean():.4f}")
    print(f"æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}")
    
    # 5. å¯è§†åŒ–åˆ†æ
    plt.figure(figsize=(15, 10))
    
    # å®é™…å€¼ vs é¢„æµ‹å€¼
    plt.subplot(2, 3, 1)
    y_pred_ols = model.predict(data[['å¹¿å‘ŠæŠ•å…¥', 'ä»·æ ¼', 'ä¿ƒé”€æ´»åŠ¨', 'å­£èŠ‚æ€§']])
    plt.scatter(data['é”€å”®é¢'], y_pred_ols, alpha=0.6)
    plt.plot([data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 
             [data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 'r--', lw=2)
    plt.xlabel('å®é™…é”€å”®é¢')
    plt.ylabel('é¢„æµ‹é”€å”®é¢')
    plt.title(f'OLSé¢„æµ‹æ•ˆæœ (RÂ² = {model.rsquared:.3f})')
    plt.grid(True, alpha=0.3)
    
    # æ®‹å·®å›¾
    plt.subplot(2, 3, 2)
    plt.scatter(y_pred_ols, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®åˆ†æ')
    plt.grid(True, alpha=0.3)
    
    # Q-Qå›¾
    plt.subplot(2, 3, 3)
    sm.qqplot(residuals, line='45', ax=plt.gca())
    plt.title('Q-Qå›¾ï¼ˆæ®‹å·®æ­£æ€æ€§æ£€éªŒï¼‰')
    
    # ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»
    plt.subplot(2, 3, 4)
    plt.scatter(data['å¹¿å‘ŠæŠ•å…¥'], data['é”€å”®é¢'], alpha=0.6)
    plt.xlabel('å¹¿å‘ŠæŠ•å…¥')
    plt.ylabel('é”€å”®é¢')
    plt.title('å¹¿å‘ŠæŠ•å…¥ vs é”€å”®é¢')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 3, 5)
    plt.scatter(data['ä»·æ ¼'], data['é”€å”®é¢'], alpha=0.6)
    plt.xlabel('ä»·æ ¼')
    plt.ylabel('é”€å”®é¢')
    plt.title('ä»·æ ¼ vs é”€å”®é¢')
    plt.grid(True, alpha=0.3)
    
    # ç³»æ•°å¯è§†åŒ–
    plt.subplot(2, 3, 6)
    coefficients = model.params.iloc[1:]  # æ’é™¤æˆªè·é¡¹
    colors = ['green' if p < 0.05 else 'red' for p in model.pvalues.iloc[1:]]
    plt.barh(coefficients.index, coefficients.values, color=colors)
    plt.axvline(x=0, color='black', linestyle='-')
    plt.xlabel('ç³»æ•°å€¼')
    plt.title('ç‰¹å¾ç³»æ•°ï¼ˆç»¿è‰²è¡¨ç¤ºæ˜¾è‘—ï¼‰')
    
    plt.tight_layout()
    plt.show()
    
    # 6. ä¸šåŠ¡è§£é‡Š
    print(f"\n=== ä¸šåŠ¡è§£é‡Š ===")
    print(f"æ¨¡å‹è§£é‡ŠåŠ›: {'å¼º' if model.rsquared > 0.7 else 'ä¸­ç­‰' if model.rsquared > 0.5 else 'å¼±'}")
    
    significant_features = []
    for feature in model.params.index[1:]:  # æ’é™¤æˆªè·
        if model.pvalues[feature] < 0.05:
            significant_features.append(feature)
    
    if significant_features:
        print(f"æ˜¾è‘—å½±å“ç‰¹å¾: {', '.join(significant_features)}")
    else:
        print("æ²¡æœ‰å‘ç°ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾")
    
    return {
        'model': model,
        'rsquared': model.rsquared,
        'significant_features': significant_features,
        'residuals': residuals
    }

# ä½¿ç”¨ç¤ºä¾‹
# stats_results = statistical_modeling_analysis()
```

## â° æ—¶é—´åºåˆ—åˆ†æï¼ˆv2.3æ–°å¢ï¼‰

### ä½¿ç”¨pmdarimaè¿›è¡Œè‡ªåŠ¨åŒ–ARIMAå»ºæ¨¡

```python
from pmdarima import auto_arima
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error

def time_series_arima_analysis(series, seasonal_period=7, forecast_steps=30):
    """è‡ªåŠ¨åŒ–ARIMAæ—¶é—´åºåˆ—åˆ†æ"""
    
    print("=== å¼€å§‹æ—¶é—´åºåˆ—ARIMAåˆ†æ ===")
    
    # 1. æ•°æ®æ£€æŸ¥
    print(f"æ—¶é—´åºåˆ—é•¿åº¦: {len(series)}")
    print(f"æ•°æ®ç±»å‹: {type(series)}")
    
    # 2. è‡ªåŠ¨ARIMAå»ºæ¨¡
    print("\n=== è‡ªåŠ¨ARIMAå‚æ•°é€‰æ‹© ===")
    try:
        model = auto_arima(
            series,
            seasonal=True,
            m=seasonal_period,  # å­£èŠ‚æ€§å‘¨æœŸï¼ˆ7å¤©ä¸ºå‘¨å­£èŠ‚æ€§ï¼‰
            stepwise=True,      # ä½¿ç”¨é€æ­¥æœç´¢ï¼ŒèŠ‚çœå†…å­˜
            suppress_warnings=True,
            error_action='ignore',
            trace=True,         # æ˜¾ç¤ºæœç´¢è¿‡ç¨‹
            random_state=42
        )
        
        print(f"æœ€ä½³ARIMAå‚æ•°: {model.order}")
        print(f"æœ€ä½³å­£èŠ‚æ€§å‚æ•°: {model.seasonal_order}")
        print(f"æ¨¡å‹AIC: {model.aic():.2f}")
        
    except Exception as e:
        print(f"è‡ªåŠ¨ARIMAå¤±è´¥: {e}")
        return None
    
    # 3. æ¨¡å‹æ‘˜è¦
    print("\n=== æ¨¡å‹æ‘˜è¦ ===")
    print(model.summary())
    
    # 4. é¢„æµ‹
    print(f"\n=== æœªæ¥{forecast_steps}æœŸé¢„æµ‹ ===")
    forecast, conf_int = model.predict(
        n_periods=forecast_steps,
        return_conf_int=True,
        alpha=0.05  # 95%ç½®ä¿¡åŒºé—´
    )
    
    # 5. æ¨¡å‹è¯„ä¼°ï¼ˆä½¿ç”¨è®­ç»ƒé›†æœ€åéƒ¨åˆ†ä½œä¸ºéªŒè¯ï¼‰
    train_size = int(len(series) * 0.8)
    train = series[:train_size]
    test = series[train_size:]
    
    # åœ¨è®­ç»ƒé›†ä¸Šé‡æ–°æ‹Ÿåˆæ¨¡å‹
    model.fit(train)
    predictions = model.predict(n_periods=len(test))
    
    # è®¡ç®—æŒ‡æ ‡
    mae = mean_absolute_error(test, predictions)
    rmse = np.sqrt(mean_squared_error(test, predictions))
    mape = np.mean(np.abs((test - predictions) / test)) * 100
    
    print(f"\n=== æ¨¡å‹æ€§èƒ½è¯„ä¼° ===")
    print(f"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.2f}")
    print(f"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.2f}")
    print(f"MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%")
    
    # 6. å¯è§†åŒ–
    plt.figure(figsize=(15, 10))
    
    # åŸå§‹åºåˆ—ä¸æ‹Ÿåˆå€¼
    plt.subplot(2, 2, 1)
    plt.plot(series.index, series, label='åŸå§‹åºåˆ—', alpha=0.7)
    plt.plot(series.index, model.predict_in_sample(), label='æ‹Ÿåˆå€¼', alpha=0.7)
    plt.xlabel('æ—¶é—´')
    plt.ylabel('å€¼')
    plt.title('åŸå§‹åºåˆ—ä¸æ¨¡å‹æ‹Ÿåˆ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ®‹å·®åˆ†æ
    plt.subplot(2, 2, 2)
    residuals = series - model.predict_in_sample()
    plt.plot(residuals.index, residuals, alpha=0.7)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('æ—¶é—´')
    plt.ylabel('æ®‹å·®')
    plt.title('æ¨¡å‹æ®‹å·®')
    plt.grid(True, alpha=0.3)
    
    # é¢„æµ‹ç»“æœ
    plt.subplot(2, 2, 3)
    last_n = min(100, len(series))
    plt.plot(series.index[-last_n:], series.values[-last_n:], label='å†å²æ•°æ®')
    
    # åˆ›å»ºæœªæ¥æ—¶é—´ç´¢å¼•
    if hasattr(series.index, 'freq'):
        future_index = pd.date_range(start=series.index[-1], periods=forecast_steps+1, freq=series.index.freq)[1:]
    else:
        future_index = range(len(series), len(series) + forecast_steps)
    
    plt.plot(future_index, forecast, label='é¢„æµ‹å€¼', color='red')
    plt.fill_between(future_index, conf_int[:, 0], conf_int[:, 1], color='pink', alpha=0.3, label='95%ç½®ä¿¡åŒºé—´')
    plt.xlabel('æ—¶é—´')
    plt.ylabel('å€¼')
    plt.title(f'æœªæ¥{forecast_steps}æœŸé¢„æµ‹')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ®‹å·®åˆ†å¸ƒ
    plt.subplot(2, 2, 4)
    plt.hist(residuals.dropna(), bins=30, alpha=0.7, edgecolor='black')
    plt.xlabel('æ®‹å·®å€¼')
    plt.ylabel('é¢‘æ•°')
    plt.title('æ®‹å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return {
        'model': model,
        'order': model.order,
        'seasonal_order': model.seasonal_order,
        'forecast': forecast,
        'confidence_interval': conf_int,
        'metrics': {'mae': mae, 'rmse': rmse, 'mape': mape},
        'residuals': residuals
    }

# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾dfæ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—DataFrameï¼Œindexä¸ºæ—¥æœŸï¼Œæœ‰ä¸€åˆ—'é”€å”®é¢'
# results = time_series_arima_analysis(df['é”€å”®é¢'], seasonal_period=7, forecast_steps=30)
```

### ä½¿ç”¨XGBoostè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹

```python
def time_series_xgboost_analysis(df, target_col, lag_features=7, forecast_steps=30):
    """ä½¿ç”¨XGBoostè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹"""
    
    print("=== å¼€å§‹æ—¶é—´åºåˆ—XGBooståˆ†æ ===")
    
    # 1. å‡†å¤‡ç‰¹å¾
    print("å‡†å¤‡æ—¶é—´åºåˆ—ç‰¹å¾...")
    features_df = pd.DataFrame(index=df.index)
    
    # æ»åç‰¹å¾
    for lag in range(1, lag_features + 1):
        features_df[f'lag_{lag}'] = df[target_col].shift(lag)
    
    # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
    for window in [3, 7, 14, 30]:
        features_df[f'ma_{window}'] = df[target_col].rolling(window).mean().shift(1)
        features_df[f'std_{window}'] = df[target_col].rolling(window).std().shift(1)
    
    # æ—¥æœŸç‰¹å¾
    if hasattr(df.index, 'month'):
        features_df['month'] = df.index.month
        features_df['dayofweek'] = df.index.dayofweek
        features_df['dayofmonth'] = df.index.day
        features_df['quarter'] = df.index.quarter
    
    # å¤–éƒ¨ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    external_features = ['Temperature', 'Promotion', 'Competitor_Price', 'Holiday']
    for feat in external_features:
        if feat in df.columns:
            features_df[feat] = df[feat]
    
    # ç›®æ ‡å˜é‡
    features_df['target'] = df[target_col]
    
    # ç§»é™¤ç¼ºå¤±å€¼
    features_df = features_df.dropna()
    
    print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {features_df.shape}")
    
    # 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X = features_df.drop('target', axis=1)
    y = features_df['target']
    
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # 3. è®­ç»ƒXGBoostæ¨¡å‹
    print("\nè®­ç»ƒXGBoostæ¨¡å‹...")
    
    xgb_model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        tree_method='hist',  # å†…å­˜å‹å¥½
        n_jobs=2,           # 6GBå†…å­˜ä¸‹ä½¿ç”¨2ä¸ªçº¿ç¨‹
        random_state=42,
        verbosity=0
    )
    
    xgb_model.fit(X_train, y_train)
    
    # 4. æ¨¡å‹è¯„ä¼°
    y_pred = xgb_model.predict(X_test)
    
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    print(f"\n=== XGBoostæ¨¡å‹æ€§èƒ½ ===")
    print(f"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.2f}")
    print(f"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.2f}")
    print(f"MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%")
    
    # 5. ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'ç‰¹å¾': X.columns,
        'é‡è¦æ€§': xgb_model.feature_importances_
    }).sort_values('é‡è¦æ€§', ascending=False)
    
    print(f"\n=== ç‰¹å¾é‡è¦æ€§ï¼ˆTop 10ï¼‰===")
    for _, row in feature_importance.head(10).iterrows():
        print(f"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}")
    
    # 6. å¯è§†åŒ–
    plt.figure(figsize=(15, 10))
    
    # é¢„æµ‹ vs å®é™…
    plt.subplot(2, 3, 1)
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('å®é™…å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title(f'XGBoosté¢„æµ‹æ•ˆæœ (MAE = {mae:.2f})')
    plt.grid(True, alpha=0.3)
    
    # ç‰¹å¾é‡è¦æ€§
    plt.subplot(2, 3, 2)
    top_features = feature_importance.head(10)
    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])
    plt.xlabel('é‡è¦æ€§')
    plt.title('Top 10 ç‰¹å¾é‡è¦æ€§')
    plt.gca().invert_yaxis()
    
    # æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”
    plt.subplot(2, 3, 3)
    plt.plot(y_test.index, y_test.values, label='å®é™…å€¼', alpha=0.7)
    plt.plot(y_test.index, y_pred, label='é¢„æµ‹å€¼', alpha=0.7)
    plt.xlabel('æ—¶é—´')
    plt.ylabel('å€¼')
    plt.title('æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ®‹å·®åˆ†æ
    plt.subplot(2, 3, 4)
    residuals = y_test - y_pred
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®åˆ†æ')
    plt.grid(True, alpha=0.3)
    
    # è¯¯å·®åˆ†å¸ƒ
    plt.subplot(2, 3, 5)
    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')
    plt.xlabel('æ®‹å·®')
    plt.ylabel('é¢‘æ•°')
    plt.title('è¯¯å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    # æ»šåŠ¨é¢„æµ‹
    plt.subplot(2, 3, 6)
    # å–æœ€å100ä¸ªç‚¹å±•ç¤º
    last_n = min(100, len(y_test))
    plt.plot(y_test.index[-last_n:], y_test.values[-last_n:], label='å®é™…å€¼')
    plt.plot(y_test.index[-last_n:], y_pred[-last_n:], label='é¢„æµ‹å€¼')
    plt.xlabel('æ—¶é—´')
    plt.ylabel('å€¼')
    plt.title('æ»šåŠ¨é¢„æµ‹å¯¹æ¯”ï¼ˆæœ€å100ç‚¹ï¼‰')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 7. æœªæ¥é¢„æµ‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if forecast_steps > 0:
        print(f"\n=== æœªæ¥{forecast_steps}æœŸé¢„æµ‹ ===")
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä¸šåŠ¡é€»è¾‘å®ç°æ»šåŠ¨é¢„æµ‹
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨æœ€ålag_featuresä¸ªç‚¹ä½œä¸ºåˆå§‹ç‰¹å¾
        
        last_features = X.iloc[-1:].copy()
        future_predictions = []
        
        for i in range(forecast_steps):
            # é¢„æµ‹ä¸‹ä¸€æ­¥
            pred = xgb_model.predict(last_features)[0]
            future_predictions.append(pred)
            
            # æ›´æ–°ç‰¹å¾ï¼ˆå¦‚æœæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œéœ€è¦æ›´æ–°æ»åç‰¹å¾ï¼‰
            # è¿™é‡Œç®€åŒ–ä¸ºåªä½¿ç”¨æœ€æ–°é¢„æµ‹å€¼
            # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®ç‰¹å¾å·¥ç¨‹é€»è¾‘æ›´æ–°
            
        print(f"æœªæ¥é¢„æµ‹å€¼: {future_predictions}")
    
    return {
        'model': xgb_model,
        'metrics': {'mae': mae, 'rmse': rmse, 'mape': mape},
        'feature_importance': feature_importance,
        'predictions': y_pred,
        'future_predictions': future_predictions if forecast_steps > 0 else None
    }

# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾dfæ˜¯ä¸€ä¸ªDataFrameï¼ŒåŒ…å«æ—¶é—´åºåˆ—å’Œå¤–éƒ¨ç‰¹å¾
# results = time_series_xgboost_analysis(df, target_col='Sales', lag_features=14, forecast_steps=30)
```

### æ—¶é—´åºåˆ—æ¨¡å‹å¯¹æ¯”

```python
def compare_time_series_models(df, target_col, seasonal_period=7, lag_features=14):
    """å¯¹æ¯”ä¸åŒæ—¶é—´åºåˆ—æ¨¡å‹æ€§èƒ½"""
    
    print("=== æ—¶é—´åºåˆ—æ¨¡å‹å¯¹æ¯”åˆ†æ ===")
    
    # å‡†å¤‡æ•°æ®
    series = df[target_col]
    
    # 1. ARIMAæ¨¡å‹
    print("\n1. è®­ç»ƒARIMAæ¨¡å‹...")
    arima_results = time_series_arima_analysis(series, seasonal_period, forecast_steps=0)
    
    # 2. XGBoostæ¨¡å‹
    print("\n2. è®­ç»ƒXGBoostæ¨¡å‹...")
    xgb_results = time_series_xgboost_analysis(df, target_col, lag_features, forecast_steps=0)
    
    # 3. LightGBMæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        import lightgbm as lgb
        print("\n3. è®­ç»ƒLightGBMæ¨¡å‹...")
        
        # å‡†å¤‡ç‰¹å¾ï¼ˆå¤ç”¨XGBoostçš„ç‰¹å¾ï¼‰
        features_df = pd.DataFrame(index=df.index)
        for lag in range(1, lag_features + 1):
            features_df[f'lag_{lag}'] = df[target_col].shift(lag)
        
        for window in [3, 7, 14, 30]:
            features_df[f'ma_{window}'] = df[target_col].rolling(window).mean().shift(1)
        
        if hasattr(df.index, 'month'):
            features_df['month'] = df.index.month
            features_df['dayofweek'] = df.index.dayofweek
        
        external_features = ['Temperature', 'Promotion', 'Competitor_Price', 'Holiday']
        for feat in external_features:
            if feat in df.columns:
                features_df[feat] = df[feat]
        
        features_df['target'] = df[target_col]
        features_df = features_df.dropna()
        
        X = features_df.drop('target', axis=1)
        y = features_df['target']
        
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # è®­ç»ƒLightGBM
        lgb_model = lgb.LGBMRegressor(
            num_leaves=31,
            learning_rate=0.05,
            n_estimators=100,
            n_jobs=2,
            random_state=42,
            verbose=-1
        )
        
        lgb_model.fit(X_train, y_train)
        y_pred_lgb = lgb_model.predict(X_test)
        
        mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
        rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
        
        print(f"LightGBMæ€§èƒ½: MAE={mae_lgb:.2f}, RMSE={rmse_lgb:.2f}")
        
        lgb_results = {
            'model': lgb_model,
            'metrics': {'mae': mae_lgb, 'rmse': rmse_lgb}
        }
        
    except ImportError:
        print("LightGBMä¸å¯ç”¨ï¼Œè·³è¿‡")
        lgb_results = None
    
    # 4. æ¨¡å‹å¯¹æ¯”
    print("\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===")
    
    comparison_data = []
    
    if arima_results:
        comparison_data.append({
            'æ¨¡å‹': 'ARIMA',
            'MAE': arima_results['metrics']['mae'],
            'RMSE': arima_results['metrics']['rmse'],
            'MAPE': arima_results['metrics']['mape']
        })
    
    if xgb_results:
        comparison_data.append({
            'æ¨¡å‹': 'XGBoost',
            'MAE': xgb_results['metrics']['mae'],
            'RMSE': xgb_results['metrics']['rmse'],
            'MAPE': xgb_results['metrics']['mape']
        })
    
    if lgb_results:
        comparison_data.append({
            'æ¨¡å‹': 'LightGBM',
            'MAE': lgb_results['metrics']['mae'],
            'RMSE': lgb_results['metrics']['rmse'],
            'MAPE': None
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # 5. å¯è§†åŒ–å¯¹æ¯”
    if len(comparison_data) > 1:
        plt.figure(figsize=(12, 5))
        
        # MAEå¯¹æ¯”
        plt.subplot(1, 2, 1)
        models = [d['æ¨¡å‹'] for d in comparison_data]
        maes = [d['MAE'] for d in comparison_data]
        
        bars = plt.bar(models, maes, alpha=0.7)
        plt.xlabel('æ¨¡å‹')
        plt.ylabel('MAE')
        plt.title('æ¨¡å‹MAEå¯¹æ¯”')
        plt.grid(True, alpha=0.3)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, mae in zip(bars, maes):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), 
                    f'{mae:.2f}', ha='center', va='bottom')
        
        # RMSEå¯¹æ¯”
        plt.subplot(1, 2, 2)
        rmses = [d['RMSE'] for d in comparison_data]
        
        bars = plt.bar(models, rmses, alpha=0.7, color='orange')
        plt.xlabel('æ¨¡å‹')
        plt.ylabel('RMSE')
        plt.title('æ¨¡å‹RMSEå¯¹æ¯”')
        plt.grid(True, alpha=0.3)
        
        for bar, rmse in zip(bars, rmses):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), 
                    f'{rmse:.2f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        # æ¨èæ¨¡å‹
        best_model_idx = np.argmin(maes)
        best_model = models[best_model_idx]
        print(f"\n=== æ¨èæ¨¡å‹ ===")
        print(f"æ ¹æ®MAEæŒ‡æ ‡ï¼Œæ¨èä½¿ç”¨: {best_model}æ¨¡å‹")
        print(f"ç†ç”±: åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æœ€ä½³ (MAE = {maes[best_model_idx]:.2f})")
    
    return {
        'arima': arima_results,
        'xgboost': xgb_results,
        'lightgbm': lgb_results,
        'comparison': comparison_df
    }

# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾æœ‰å®Œæ•´çš„æ—¶é—´åºåˆ—æ•°æ®é›†df
# model_comparison = compare_time_series_models(df, target_col='Sales', seasonal_period=7, lag_features=14)
```

## ğŸ”§ æ¨¡å‹ä¼˜åŒ–ä¸è°ƒå‚

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

def model_optimization_pipeline(X, y, problem_type='regression'):
    """æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–æµç¨‹"""
    
    print(f"=== å¼€å§‹ {problem_type} æ¨¡å‹ä¼˜åŒ– ===")
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # é€‰æ‹©æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼
    if problem_type == 'regression':
        model = RandomForestRegressor(random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        scoring = 'r2'
    else:
        model = RandomForestClassifier(random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        scoring = 'accuracy'
    
    # ç½‘æ ¼æœç´¢
    print("æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...")
    grid_search = GridSearchCV(
        model, param_grid, cv=5, scoring=scoring, 
        n_jobs=-1, verbose=1
    )
    grid_search.fit(X_train, y_train)
    
    # è¾“å‡ºæœ€ä¼˜å‚æ•°
    print(f"\n=== æœ€ä¼˜å‚æ•° ===")
    for param, value in grid_search.best_params_.items():
        print(f"  {param}: {value}")
    
    print(f"æœ€ä¼˜æ¨¡å‹å¾—åˆ†: {grid_search.best_score_:.4f}")
    
    # æµ‹è¯•é›†æ€§èƒ½
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    
    if problem_type == 'regression':
        test_score = r2_score(y_test, y_pred)
        print(f"æµ‹è¯•é›† RÂ²: {test_score:.4f}")
    else:
        test_score = accuracy_score(y_test, y_pred)
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")
    
    return {
        'best_model': best_model,
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'test_score': test_score
    }

# ä½¿ç”¨ç¤ºä¾‹
# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()
# optimized_regression = model_optimization_pipeline(X_reg, y_reg, 'regression')
# optimized_classification = model_optimization_pipeline(X_clf, y_clf, 'classification')
```

## æœºå™¨å­¦ä¹ å¢å¼º(v2.5æ–°å¢)

### LightGBM - é«˜æ•ˆæ¢¯åº¦æå‡

**ç”¨é€”**: é«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘ç®—æ³•  
**ä¼˜åŠ¿**: æ¯”XGBoostè®­ç»ƒæ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘  

```python
import lightgbm as lgb
from sklearn.model_selection import train_test_split
import pandas as pd

# å‡†å¤‡æ•°æ®
data = pd.read_csv('/data/train.csv')
X = data.drop('target', axis=1)
y = data['target']

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºæ•°æ®é›†
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# å‚æ•°è®¾ç½®ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'num_threads': 2  # é™åˆ¶çº¿ç¨‹æ•°
}

# è®­ç»ƒæ¨¡å‹
gbm = lgb.train(params, train_data, num_boost_round=100)
```

### Category Encoders - åˆ†ç±»ç‰¹å¾ç¼–ç 

**ç”¨é€”**: å„ç§åˆ†ç±»ç¼–ç æ–¹æ³•  
**ä¼˜åŠ¿**: æå‡åˆ†ç±»æ¨¡å‹æ€§èƒ½ï¼Œæ”¯æŒå¤šç§ç¼–ç ç­–ç•¥  

```python
import pandas as pd
import category_encoders as ce

# åˆ›å»ºç¤ºä¾‹æ•°æ®
df = pd.DataFrame({
    'category': ['A', 'B', 'A', 'C', 'B', 'A'],
    'value': [1, 2, 3, 4, 5, 6]
})

# ä½¿ç”¨Target Encoding
encoder = ce.TargetEncoder(cols=['category'])
df_encoded = encoder.fit_transform(df['category'], df['value'])

print(df_encoded)
```

### XGBoost - é«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘ (v2.3æ–°å¢)

**ç”¨é€”**: é«˜çº§æ¢¯åº¦æå‡æ ‘ç®—æ³•ï¼Œæ”¯æŒå›å½’ã€åˆ†ç±»ã€æ’åºä»»åŠ¡  
**ä¼˜åŠ¿**: ç²¾åº¦é«˜ï¼Œæ”¯æŒè‡ªå®šä¹‰ç›®æ ‡å‡½æ•°ï¼Œå¯è§£é‡Šæ€§å¥½  

```python
import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error

# å‡†å¤‡æ•°æ®
data = pd.read_csv('/data/train.csv')
X = data.drop('target', axis=1)
y = data['target']

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºDMatrixï¼ˆXGBoosté«˜æ•ˆæ•°æ®ç»“æ„ï¼‰
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# å‚æ•°è®¾ç½®ï¼ˆå›å½’é—®é¢˜ç¤ºä¾‹ï¼‰
params = {
    'objective': 'reg:squarederror',  # å›å½’ä»»åŠ¡
    'max_depth': 5,
    'eta': 0.1,  # å­¦ä¹ ç‡
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'tree_method': 'hist',  # å†…å­˜å‹å¥½çš„ç›´æ–¹å›¾ç®—æ³•
    'n_jobs': 2,  # 6GBå†…å­˜ä¸‹ä½¿ç”¨2ä¸ªçº¿ç¨‹
    'random_state': 42
}

# è®­ç»ƒæ¨¡å‹
num_rounds = 100
model = xgb.train(params, dtrain, num_rounds)

# é¢„æµ‹
y_pred = model.predict(dtest)

# è¯„ä¼°
if data['target'].dtype == 'object':  # åˆ†ç±»ä»»åŠ¡
    accuracy = accuracy_score(y_test, y_pred.round())
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
else:  # å›å½’ä»»åŠ¡
    mse = mean_squared_error(y_test, y_pred)
    print(f"MSE: {mse:.4f}")

# ç‰¹å¾é‡è¦æ€§
importance = model.get_score(importance_type='weight')
print("ç‰¹å¾é‡è¦æ€§:", importance)

# ä¿å­˜æ¨¡å‹
model.save_model('/data/xgboost_model.json')
```

### scikit-optimize - è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–

**ç”¨é€”**: è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–  
**ä¼˜åŠ¿**: æ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆï¼Œæ‰¾åˆ°æ›´å¥½å‚æ•°ç»„åˆ  

```python
from skopt import BayesSearchCV
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# å‡†å¤‡æ•°æ®
data = pd.read_csv('/data/train.csv')
X = data.drop('target', axis=1)
y = data['target']

# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
param_space = {
    'n_estimators': (50, 200),
    'max_depth': (3, 10),
    'min_samples_split': (2, 10),
    'min_samples_leaf': (1, 4)
}

# è´å¶æ–¯ä¼˜åŒ–æœç´¢
opt = BayesSearchCV(
    RandomForestClassifier(),
    param_space,
    n_iter=50,
    cv=5,
    n_jobs=2  # é™åˆ¶å¹¶è¡Œçº¿ç¨‹
)

opt.fit(X, y)
print(f"æœ€ä½³å‚æ•°: {opt.best_params_}")
print(f"æœ€ä½³åˆ†æ•°: {opt.best_score_:.4f}")
```

## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹

### âœ… æ¨èåšæ³•ï¼š
- ä½¿ç”¨æ ‡å‡†çš„ scikit-learn å’Œ statsmodels æ¥å£
- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœå’ŒæŒ‡æ ‡
- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨
- å¯¹æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†å’Œæ ‡å‡†åŒ–
- æ—¶é—´åºåˆ—åˆ†æä¼˜å…ˆä½¿ç”¨pmdarimaè‡ªåŠ¨é€‰æ‹©ARIMAå‚æ•°
- éçº¿æ€§å»ºæ¨¡ä¼˜å…ˆä½¿ç”¨XGBoostæˆ–LightGBM

### âŒ é¿å…çš„æ“ä½œï¼š
- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º
- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç 
- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼
- ä¸è¦å¯¹æ˜æ˜¾å­£èŠ‚æ€§æ•°æ®ä½¿ç”¨éå­£èŠ‚æ€§ARIMA

### ğŸ”§ é”™è¯¯å¤„ç†ï¼š
```python
try:
    from sklearn.ensemble import RandomForestRegressor
    # æ¨¡å‹è®­ç»ƒä»£ç 
except ImportError:
    print("scikit-learn ä¸å¯ç”¨")

try:
    import statsmodels.api as sm
    # ç»Ÿè®¡å»ºæ¨¡ä»£ç 
except ImportError:
    print("statsmodels ä¸å¯ç”¨")

try:
    import xgboost as xgb
    # XGBoostä»£ç 
except ImportError:
    print("XGBoost ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")

try:
    import pmdarima as pm
    # ARIMAä»£ç 
except ImportError:
    print("pmdarima ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
```

### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š
```python
# å¿«é€Ÿæ¨¡å‹è¯„ä¼°å‡½æ•°
def quick_model_evaluation(model, X_test, y_test, problem_type='regression'):
    """å¿«é€Ÿæ¨¡å‹è¯„ä¼°"""
    y_pred = model.predict(X_test)
    
    if problem_type == 'regression':
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        print(f"RÂ²: {r2:.4f}, RMSE: {rmse:.4f}")
    else:
        accuracy = accuracy_score(y_test, y_pred)
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    
    return y_pred

# æ—¶é—´åºåˆ—åˆ†æå¿«é€Ÿæ¨¡æ¿
def quick_time_series_analysis(series, model_type='auto_arima'):
    """å¿«é€Ÿæ—¶é—´åºåˆ—åˆ†ææ¨¡æ¿"""
    if model_type == 'auto_arima':
        from pmdarima import auto_arima
        model = auto_arima(series, seasonal=True, m=7, suppress_warnings=True)
        forecast = model.predict(n_periods=30)
    elif model_type == 'xgboost':
        # ä½¿ç”¨time_series_xgboost_analysiså‡½æ•°
        pass
    
    return model, forecast
```

**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡å’Œåˆ†æé€»è¾‘ï¼

## ğŸ“ æ²™ç›’ç¯å¢ƒæ–‡ä»¶æ“ä½œæŒ‡å—

### æ–‡ä»¶ä¸Šä¼ ï¼ˆå¿…é¡»æ­¥éª¤ï¼‰
åœ¨æ²™ç›’ä¸­è¿è¡Œä»£ç å‰ï¼Œ**å¿…é¡»å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶**ï¼š

```python
# ç¤ºä¾‹ï¼šå¦‚ä½•å¼•ç”¨å·²ä¸Šä¼ çš„æ–‡ä»¶
# å‡è®¾æ‚¨å·²ç»é€šè¿‡å‰ç«¯ç•Œé¢ä¸Šä¼ äº†ä»¥ä¸‹æ–‡ä»¶ï¼š
# - /data/train.csv      ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰
# - /data/dataset.xlsx   ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰
# - /data/sales.parquet  ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰

import pandas as pd
import os

def list_uploaded_files():
    """åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡ä»¶"""
    data_dir = '/data'
    if os.path.exists(data_dir):
        files = os.listdir(data_dir)
        print(f"å·²ä¸Šä¼ çš„æ–‡ä»¶: {files}")
        return files
    else:
        print("æ²¡æœ‰æ‰¾åˆ°/dataç›®å½•")
        return []

# åˆ—å‡ºæ–‡ä»¶
available_files = list_uploaded_files()

# è¯»å–ç‰¹å®šæ–‡ä»¶
if 'train.csv' in available_files:
    df = pd.read_csv('/data/train.csv')
    print(f"æˆåŠŸè¯»å– train.csvï¼Œå½¢çŠ¶: {df.shape}")
    
if 'dataset.xlsx' in available_files:
    df = pd.read_excel('/data/dataset.xlsx')
    print(f"æˆåŠŸè¯»å– dataset.xlsxï¼Œå½¢çŠ¶: {df.shape}")
```

### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
æ ¹æ®code_interpreter.pyï¼Œç³»ç»Ÿæ”¯æŒä»¥ä¸‹æ–‡ä»¶æ ¼å¼ï¼š
- ğŸ“Š æ•°æ®æ–‡ä»¶ï¼š`.csv`, `.xlsx`, `.xls`, `.parquet`, `.json`

### æ–‡ä»¶è¯»å–æœ€ä½³å®è·µ
```python
def safe_read_data(filename):
    """å®‰å…¨è¯»å–æ•°æ®æ–‡ä»¶ï¼Œå¸¦é”™è¯¯å¤„ç†"""
    try:
        filepath = f'/data/{filename}'
        
        # æ ¹æ®æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
        if filename.endswith('.csv'):
            df = pd.read_csv(filepath)
        elif filename.endswith('.parquet'):
            df = pd.read_parquet(filepath)
        elif filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(filepath)
        elif filename.endswith('.json'):
            df = pd.read_json(filepath)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
        
        print(f"âœ… æˆåŠŸè¯»å– {filename}")
        print(f"   è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
        print(f"   åˆ—å: {list(df.columns)}")
        
        return df
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        print("è¯·å…ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ æ–‡ä»¶")
        return None
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ£€æŸ¥å¯ç”¨çš„æ–‡ä»¶
    files = list_uploaded_files()
    if files:
        for file in files:
            print(f"å‘ç°æ–‡ä»¶: {file}")
        
        # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶
        csv_files = [f for f in files if f.endswith('.csv')]
        if csv_files:
            df = safe_read_data(csv_files[0])
            if df is not None:
                # è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ
                pass
```

### å·¥ä½œæµæ•´åˆç¤ºä¾‹
```python
# å®Œæ•´çš„MLå·¥ä½œæµï¼ŒåŒ…å«æ–‡ä»¶æ£€æŸ¥
def complete_ml_workflow_with_file_check():
    """åŒ…å«æ–‡ä»¶æ£€æŸ¥çš„å®Œæ•´MLå·¥ä½œæµ"""
    
    print("=== æœºå™¨å­¦ä¹ å·¥ä½œæµå¼€å§‹ ===")
    
    # 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶
    files = list_uploaded_files()
    if not files:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        # ä½¿ç”¨generate_sample_data()å‡½æ•°åˆ›å»ºç¤ºä¾‹æ•°æ®
        from sklearn.datasets import make_regression
        X, y = make_regression(n_samples=1000, n_features=10, random_state=42)
    else:
        print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶: {files}")
        
        # è¯»å–ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶
        data_file = files[0]
        df = safe_read_data(data_file)
        
        if df is None:
            print("æ— æ³•è¯»å–æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
            from sklearn.datasets import make_regression
            X, y = make_regression(n_samples=1000, n_features=10, random_state=42)
        else:
            # å‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡å˜é‡
            X = df.iloc[:, :-1].values
            y = df.iloc[:, -1].values
    
    # 2. æ‰§è¡ŒMLåˆ†æï¼ˆä½¿ç”¨æ–‡æ¡£ä¸­çš„å‡½æ•°ï¼‰
    results = standard_ml_pipeline(X, y, problem_type='regression')
    
    return results
```

### âš¡ å¿«é€Ÿä½¿ç”¨æ¨¡æ¿
```python
# åœ¨æ²™ç›’ä¸­è¿è¡Œæœºå™¨å­¦ä¹ åˆ†æçš„å®Œæ•´ç¤ºä¾‹
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# æ­¥éª¤1ï¼šè¯»å–æ•°æ®ï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶åï¼‰
try:
    # å¦‚æœæ‚¨ä¸Šä¼ äº†train.csv
    df = pd.read_csv('/data/train.csv')
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # æ­¥éª¤2ï¼šå‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
    X = df.drop('target_column', axis=1)  # æ›¿æ¢ä¸ºæ‚¨çš„ç›®æ ‡åˆ—å
    y = df['target_column']
    
    # æ­¥éª¤3ï¼šè®­ç»ƒæ¨¡å‹
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # æ­¥éª¤4ï¼šè¯„ä¼°
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    print(f"æ¨¡å‹æ€§èƒ½: RÂ²={r2:.4f}, RMSE={rmse:.4f}")
    
    # æ­¥éª¤5ï¼šå¯è§†åŒ–
    plt.figure(figsize=(10, 5))
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('å®é™…å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')
    plt.grid(True, alpha=0.3)
    plt.show()
    
except FileNotFoundError:
    print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶ã€‚è¯·ç¡®ä¿ï¼š")
    print("   1. å·²é€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ train.csv")
    print("   2. æ–‡ä»¶ä½äº/dataç›®å½•ä¸‹")
    print("   3. æ–‡ä»¶åæ‹¼å†™æ­£ç¡®")
    
    # æä¾›ç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡é€‰
    print("\nğŸ”§ æ­£åœ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®è¿›è¡Œåˆ†æ...")
    from sklearn.datasets import make_regression
    X, y = make_regression(n_samples=1000, n_features=5, random_state=42)
    
    # ç»§ç»­æ‰§è¡Œåˆ†æ...
```
