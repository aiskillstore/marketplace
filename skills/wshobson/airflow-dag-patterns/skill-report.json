{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T07:03:33.284Z",
    "slug": "wshobson-airflow-dag-patterns",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/data-engineering/skills/airflow-dag-patterns",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "e7da3ff014c8990cda3522d8289abb47fac7b0a7087a4c0ab5e6d78d659eecd1",
    "tree_hash": "1c88e7d57e558a07eae3f7b6b2eae42f4bfd218842a292e11bd900091bc50406"
  },
  "skill": {
    "name": "airflow-dag-patterns",
    "description": "Build production Apache Airflow DAGs with best practices for operators, sensors, testing, and deployment. Use when creating data pipelines, orchestrating workflows, or scheduling batch jobs.",
    "summary": "Build production Apache Airflow DAGs with best practices for operators, sensors, testing, and deploy...",
    "icon": "ðŸŒ€",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "data",
    "tags": [
      "airflow",
      "dag",
      "orchestration",
      "etl",
      "testing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "scripts",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill containing only Airflow patterns and example code. No executable scripts, network calls, filesystem access, or environment variable reads. All code in SKILL.md is illustrative documentation for building data pipelines.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          },
          {
            "file": "SKILL.md",
            "line_start": 332,
            "line_end": 332
          },
          {
            "file": "SKILL.md",
            "line_start": 332,
            "line_end": 332
          },
          {
            "file": "SKILL.md",
            "line_start": 521,
            "line_end": 521
          },
          {
            "file": "SKILL.md",
            "line_start": 522,
            "line_end": 522
          },
          {
            "file": "SKILL.md",
            "line_start": 523,
            "line_end": 523
          }
        ]
      },
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 227,
            "line_end": 227
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 32,
            "line_end": 45
          },
          {
            "file": "SKILL.md",
            "line_start": 45,
            "line_end": 49
          },
          {
            "file": "SKILL.md",
            "line_start": 49,
            "line_end": 93
          },
          {
            "file": "SKILL.md",
            "line_start": 93,
            "line_end": 99
          },
          {
            "file": "SKILL.md",
            "line_start": 99,
            "line_end": 155
          },
          {
            "file": "SKILL.md",
            "line_start": 155,
            "line_end": 159
          },
          {
            "file": "SKILL.md",
            "line_start": 159,
            "line_end": 228
          },
          {
            "file": "SKILL.md",
            "line_start": 228,
            "line_end": 232
          },
          {
            "file": "SKILL.md",
            "line_start": 232,
            "line_end": 285
          },
          {
            "file": "SKILL.md",
            "line_start": 285,
            "line_end": 289
          },
          {
            "file": "SKILL.md",
            "line_start": 289,
            "line_end": 349
          },
          {
            "file": "SKILL.md",
            "line_start": 349,
            "line_end": 353
          },
          {
            "file": "SKILL.md",
            "line_start": 353,
            "line_end": 428
          },
          {
            "file": "SKILL.md",
            "line_start": 428,
            "line_end": 432
          },
          {
            "file": "SKILL.md",
            "line_start": 432,
            "line_end": 475
          },
          {
            "file": "SKILL.md",
            "line_start": 475,
            "line_end": 479
          },
          {
            "file": "SKILL.md",
            "line_start": 479,
            "line_end": 501
          },
          {
            "file": "SKILL.md",
            "line_start": 501,
            "line_end": 508
          },
          {
            "file": "SKILL.md",
            "line_start": 508,
            "line_end": 513
          },
          {
            "file": "SKILL.md",
            "line_start": 513,
            "line_end": 514
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 701,
    "audit_model": "claude",
    "audited_at": "2026-01-17T07:03:33.284Z"
  },
  "content": {
    "user_title": "Build production Airflow DAGs with proven patterns",
    "value_statement": "Airflow DAGs can fail when structure and retries are not consistent. This skill provides clear patterns for design, sensors, testing, and alerts that improve pipeline reliability.",
    "seo_keywords": [
      "Apache Airflow",
      "DAG patterns",
      "ETL pipelines",
      "workflow orchestration",
      "TaskFlow API",
      "Claude",
      "Codex",
      "Claude Code",
      "data engineering",
      "Airflow testing"
    ],
    "actual_capabilities": [
      "Provide TaskFlow API pattern with extraction, transform, load, and notifications",
      "Show dynamic DAG generation from configuration objects",
      "Demonstrate branching and conditional task routing",
      "Provide sensor patterns for S3, external tasks, and custom APIs",
      "Outline error handling with callbacks and trigger rules",
      "Include unit testing patterns with DagBag assertions"
    ],
    "limitations": [
      "Does not run or validate DAGs in a live Airflow environment",
      "Does not configure cloud credentials or secrets management",
      "Examples use placeholder services and paths that require adaptation",
      "Does not generate deployment infrastructure files"
    ],
    "use_cases": [
      {
        "target_user": "Data engineer",
        "title": "Standardize ETL DAGs",
        "description": "Create consistent DAG structures, retries, and schedules for daily and hourly pipelines."
      },
      {
        "target_user": "Platform engineer",
        "title": "Add operational safeguards",
        "description": "Apply sensors, alerts, and failure callbacks to improve pipeline reliability."
      },
      {
        "target_user": "Analytics engineer",
        "title": "Test DAG integrity",
        "description": "Write unit tests to ensure DAGs load, have no cycles, and respect dependencies."
      }
    ],
    "prompt_templates": [
      {
        "title": "Start a simple DAG",
        "scenario": "New ETL pipeline with one extract task",
        "prompt": "Create a daily Airflow DAG with start, extract, and end tasks using PythonOperator and basic default_args."
      },
      {
        "title": "Use TaskFlow API",
        "scenario": "ETL with XCom passing",
        "prompt": "Draft a TaskFlow API DAG with extract, transform, and load tasks that pass data between them safely."
      },
      {
        "title": "Add sensors and waits",
        "scenario": "Upstream dependency management",
        "prompt": "Show a pattern for waiting on S3 data and an upstream DAG before processing."
      },
      {
        "title": "Harden error handling",
        "scenario": "Alerting and cleanup",
        "prompt": "Add failure callbacks, retries, and cleanup tasks that run on failure for a critical DAG."
      }
    ],
    "output_examples": [
      {
        "input": "Give me a safe pattern for a daily ETL DAG with retries, testing, and a failure callback.",
        "output": [
          "Use default_args with retries, retry_delay, and exponential backoff",
          "Define a linear start -> extract -> load -> end structure",
          "Add a task failure callback to report errors",
          "Create DagBag tests for load errors, task count, and dependencies"
        ]
      },
      {
        "input": "How do I create DAGs dynamically from configuration?",
        "output": [
          "Define a PIPELINE_CONFIGS list with name, schedule, and source fields",
          "Create a create_dag factory function that takes a config parameter",
          "Use globals()[f\"dag_{name}\"] to register each DAG dynamically",
          "Apply the same pattern for customers, orders, and products pipelines"
        ]
      },
      {
        "input": "Show me how to wait for external dependencies before running tasks.",
        "output": [
          "Use S3KeySensor to wait for files in S3 buckets",
          "Use ExternalTaskSensor to wait for upstream DAG completion",
          "Set mode='reschedule' to free workers while waiting",
          "Combine multiple sensors with dependencies before processing"
        ]
      }
    ],
    "best_practices": [
      "Keep tasks idempotent and avoid heavy logic in DAG files",
      "Use TaskFlow API and sensor reschedule mode for efficiency",
      "Add unit tests for DAG loading, structure, and cycles"
    ],
    "anti_patterns": [
      "Using depends_on_past for most tasks",
      "Hardcoding dates instead of Airflow macros",
      "Storing mutable global state in DAG files"
    ],
    "faq": [
      {
        "question": "Is this compatible with Airflow 2.x?",
        "answer": "Yes, the patterns target Airflow 2.x and include TaskFlow API examples."
      },
      {
        "question": "What are the limits of the examples?",
        "answer": "Examples are templates and require adapting paths, credentials, and environments."
      },
      {
        "question": "Can it integrate with my existing DAGs?",
        "answer": "Yes, you can apply the patterns to existing DAG files and refactor safely."
      },
      {
        "question": "Does it access or store my data?",
        "answer": "No, it provides guidance only and does not read or transmit data."
      },
      {
        "question": "What if my DAG fails to import?",
        "answer": "Use the DagBag tests to identify import errors and fix missing dependencies."
      },
      {
        "question": "How does it compare to generic coding help?",
        "answer": "It focuses on Airflow-specific patterns, sensors, testing, and operational safety."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 524
    }
  ]
}
