{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T10:22:50.318Z",
    "slug": "wshobson-spark-optimization",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/data-engineering/skills/spark-optimization",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "727dafbdd8048ccd6a0602d8eea77f5ffcdd4098fed065f2550b8f819ad4b6ec",
    "tree_hash": "3743658127c637b0f62184058d59e8ba8fd2437d2b8a95d5d2355299ee3529ab"
  },
  "skill": {
    "name": "spark-optimization",
    "description": "Optimize Apache Spark jobs with partitioning, caching, shuffle optimization, and memory tuning. Use when improving Spark performance, debugging slow jobs, or scaling data processing pipelines.",
    "summary": "Optimize Apache Spark jobs with partitioning, caching, shuffle optimization, and memory tuning. Use ...",
    "icon": "âš¡",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "data",
    "tags": [
      "spark",
      "performance",
      "optimization",
      "data-engineering"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill containing only markdown content with Apache Spark tuning guidance. No executable code, credential access, network calls, or malicious patterns detected. All 43 static findings are false positives triggered by misidentified Spark terminology.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          },
          {
            "file": "SKILL.md",
            "line_start": 413,
            "line_end": 413
          },
          {
            "file": "SKILL.md",
            "line_start": 414,
            "line_end": 414
          },
          {
            "file": "SKILL.md",
            "line_start": 415,
            "line_end": 415
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 23,
            "line_end": 31
          },
          {
            "file": "SKILL.md",
            "line_start": 31,
            "line_end": 45
          },
          {
            "file": "SKILL.md",
            "line_start": 45,
            "line_end": 73
          },
          {
            "file": "SKILL.md",
            "line_start": 73,
            "line_end": 79
          },
          {
            "file": "SKILL.md",
            "line_start": 79,
            "line_end": 104
          },
          {
            "file": "SKILL.md",
            "line_start": 104,
            "line_end": 108
          },
          {
            "file": "SKILL.md",
            "line_start": 108,
            "line_end": 169
          },
          {
            "file": "SKILL.md",
            "line_start": 169,
            "line_end": 173
          },
          {
            "file": "SKILL.md",
            "line_start": 173,
            "line_end": 210
          },
          {
            "file": "SKILL.md",
            "line_start": 210,
            "line_end": 214
          },
          {
            "file": "SKILL.md",
            "line_start": 214,
            "line_end": 245
          },
          {
            "file": "SKILL.md",
            "line_start": 245,
            "line_end": 249
          },
          {
            "file": "SKILL.md",
            "line_start": 249,
            "line_end": 276
          },
          {
            "file": "SKILL.md",
            "line_start": 276,
            "line_end": 280
          },
          {
            "file": "SKILL.md",
            "line_start": 280,
            "line_end": 305
          },
          {
            "file": "SKILL.md",
            "line_start": 305,
            "line_end": 308
          },
          {
            "file": "SKILL.md",
            "line_start": 308,
            "line_end": 312
          },
          {
            "file": "SKILL.md",
            "line_start": 312,
            "line_end": 356
          },
          {
            "file": "SKILL.md",
            "line_start": 356,
            "line_end": 360
          },
          {
            "file": "SKILL.md",
            "line_start": 360,
            "line_end": 393
          },
          {
            "file": "SKILL.md",
            "line_start": 393,
            "line_end": 409
          },
          {
            "file": "SKILL.md",
            "line_start": 409,
            "line_end": 409
          },
          {
            "file": "SKILL.md",
            "line_start": 409,
            "line_end": 409
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 590,
    "audit_model": "claude",
    "audited_at": "2026-01-17T10:22:50.318Z"
  },
  "content": {
    "user_title": "Improve Spark Performance for Large Pipelines",
    "value_statement": "Slow Spark jobs waste cluster time and delay analytics. This skill provides proven tuning patterns for partitioning, caching, joins, and memory to improve performance.",
    "seo_keywords": [
      "Spark optimization",
      "Apache Spark tuning",
      "Spark performance",
      "Spark partitioning",
      "Spark shuffle",
      "Spark caching",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Recommend partition sizing and repartition or coalesce strategies.",
      "Suggest broadcast, sort merge, bucket, and skew join handling.",
      "Outline caching and persistence levels with materialization steps.",
      "Provide executor and memory configuration guidance.",
      "Detail shuffle reduction and compression settings.",
      "Explain monitoring steps using Spark UI and explain plans."
    ],
    "limitations": [
      "Does not run Spark jobs or access your cluster.",
      "Does not estimate exact speedups or costs.",
      "Requires user provided job details and metrics.",
      "Does not validate configuration against vendor specific limits."
    ],
    "use_cases": [
      {
        "target_user": "Data engineer",
        "title": "Reduce nightly job time",
        "description": "Analyze a slow batch pipeline and get tuning steps for partitions, joins, and caching."
      },
      {
        "target_user": "Analytics engineer",
        "title": "Fix skewed joins",
        "description": "Apply AQE and salting guidance to remove long running tasks."
      },
      {
        "target_user": "Platform engineer",
        "title": "Standardize Spark configs",
        "description": "Create a baseline executor and shuffle configuration for new clusters."
      }
    ],
    "prompt_templates": [
      {
        "title": "Speed up my job",
        "scenario": "Beginner improving a slow Spark job",
        "prompt": "My Spark job takes 2 hours and uses groupBy on large tables. Suggest quick wins for partitions, caching, and joins."
      },
      {
        "title": "Partition sizing",
        "scenario": "Intermediate tuning for large input data",
        "prompt": "I process 1 TB of parquet data daily. Recommend partition counts and file sizes, and explain how to adjust shuffle partitions."
      },
      {
        "title": "Skew diagnosis",
        "scenario": "Advanced skewed join investigation",
        "prompt": "A join on customer_id has a few hot keys and long tasks. Provide AQE settings and a manual salting approach."
      },
      {
        "title": "Memory tuning",
        "scenario": "Advanced executor memory planning",
        "prompt": "We use 8g executors and see frequent spills. Propose memory, overhead, and shuffle settings with rationale."
      }
    ],
    "output_examples": [
      {
        "input": "Suggest Spark optimizations for a slow join and high shuffle spill.",
        "output": [
          "Enable AQE and skew join handling to split hot partitions.",
          "Broadcast the small dimension table to avoid shuffle on that side.",
          "Lower shuffle partition size and enable compression to reduce spill."
        ]
      },
      {
        "input": "How do I optimize partitioning for 500GB of daily data?",
        "output": [
          "Target 128-256 MB per partition for balanced parallelism.",
          "For 500GB, use roughly 2000-4000 partitions initially.",
          "Adjust based on actual task duration in Spark UI.",
          "Consider partitionBy on date columns for pruneable reads."
        ]
      },
      {
        "input": "My cache is not improving performance. What am I doing wrong?",
        "output": [
          "Ensure you call count() or an action to materialize the cache.",
          "Check that you are reusing the DataFrame multiple times.",
          "Verify the DataFrame fits in memory without excessive spilling.",
          "Use unpersist() when done to free memory for other operations."
        ]
      }
    ],
    "best_practices": [
      "Use AQE and monitor Spark UI for skew and spills.",
      "Target 128 to 256 MB partition sizes for balanced parallelism.",
      "Prefer built in functions over UDFs for better optimization."
    ],
    "anti_patterns": [
      "Collecting large datasets to the driver.",
      "Over caching multiple large DataFrames without unpersist.",
      "Using wide shuffles for simple aggregates without pre aggregation."
    ],
    "faq": [
      {
        "question": "Is this compatible with PySpark and Spark SQL?",
        "answer": "Yes. The guidance covers PySpark DataFrame and Spark SQL configurations."
      },
      {
        "question": "What are the limits of the recommendations?",
        "answer": "They are general patterns and require validation against your data size and cluster constraints."
      },
      {
        "question": "Can it integrate with Databricks or EMR?",
        "answer": "Yes. You can apply the same Spark configs and optimization steps in those platforms."
      },
      {
        "question": "Does it access my data or cluster?",
        "answer": "No. It provides guidance only and does not connect to your systems."
      },
      {
        "question": "What if performance does not improve?",
        "answer": "Provide Spark UI metrics, query plans, and data sizes to refine the recommendations."
      },
      {
        "question": "How does it compare to generic tuning advice?",
        "answer": "It focuses on Spark specific execution stages, shuffles, and memory behavior with concrete configuration examples."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 416
    }
  ]
}
