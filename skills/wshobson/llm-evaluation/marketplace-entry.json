{
  "name": "wshobson-llm-evaluation",
  "source": "./plugins/wshobson/llm-evaluation",
  "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.",
  "version": "1.0.0",
  "author": {
    "name": "wshobson"
  },
  "repository": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/llm-evaluation",
  "license": "MIT",
  "keywords": [
    "llm",
    "evaluation",
    "metrics",
    "testing",
    "benchmarking"
  ],
  "category": "research",
  "tags": [
    "llm",
    "evaluation",
    "metrics",
    "testing",
    "benchmarking"
  ]
}