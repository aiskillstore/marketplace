{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T17:03:04.838Z",
    "slug": "wshobson-rag-implementation",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/rag-implementation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "b9f8ca17aad219835e06b60f33b8d312ccfa35cdcca6c1faa3376e6521835243",
    "tree_hash": "c4b89df2fa90466a73cd247e60c78bd9443767d48aeb011e4428c369ea753328"
  },
  "skill": {
    "name": "rag-implementation",
    "description": "Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and semantic search. Use when implementing knowledge-grounded AI, building document Q&A systems, or integrating LLMs with external knowledge bases.",
    "summary": "Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and se...",
    "icon": "ðŸ“š",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "data",
    "tags": [
      "rag",
      "retrieval",
      "embeddings",
      "vector-databases",
      "semantic-search"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "filesystem",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a documentation-only skill containing Markdown guides with Python code examples. No executable scripts, no network calls, and no file access capabilities exist in the skill itself. The code examples demonstrate typical RAG patterns using LangChain APIs. No obfuscation, persistence mechanisms, or malicious patterns detected.",
    "risk_factor_evidence": [
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 70,
            "line_end": 72
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 236,
            "line_end": 236
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 604,
    "audit_model": "claude",
    "audited_at": "2026-01-04T17:03:04.838Z"
  },
  "content": {
    "user_title": "Build a grounded RAG pipeline",
    "value_statement": "You need accurate answers from private documents and changing data. This skill explains RAG components and retrieval patterns to ground responses with sources.",
    "seo_keywords": [
      "RAG",
      "retrieval augmented generation",
      "vector databases",
      "semantic search",
      "embeddings",
      "Claude",
      "Codex",
      "Claude Code",
      "document Q&A",
      "LLM apps"
    ],
    "actual_capabilities": [
      "Provides a quick start RAG pipeline example with loaders, chunking, embeddings, and retrieval.",
      "Describes vector database options and setup patterns for Pinecone, Weaviate, and Chroma.",
      "Explains retrieval strategies like hybrid search, multi-query, and contextual compression.",
      "Shows reranking approaches including cross-encoder and MMR.",
      "Outlines evaluation metrics for accuracy, retrieval quality, and groundedness."
    ],
    "limitations": [
      "Does not execute code or connect to databases.",
      "Does not ingest or index documents automatically.",
      "Examples focus on Python and LangChain style APIs.",
      "No deployment, scaling, or cost guidance."
    ],
    "use_cases": [
      {
        "target_user": "Product engineer",
        "title": "Design a RAG chatbot",
        "description": "Plan a retrieval pipeline that grounds answers with citations from internal documentation."
      },
      {
        "target_user": "Data scientist",
        "title": "Evaluate retrieval quality",
        "description": "Define metrics and test cases to measure accuracy, grounding, and retrieval quality."
      },
      {
        "target_user": "Platform team",
        "title": "Select vector storage",
        "description": "Compare vector database options and choose an approach that fits scale and deployment needs."
      }
    ],
    "prompt_templates": [
      {
        "title": "RAG basics",
        "scenario": "New project setup",
        "prompt": "Create a simple RAG plan for a document Q&A app. Include data ingestion, chunking, embeddings, vector store choice, and retrieval chain."
      },
      {
        "title": "Hybrid retrieval",
        "scenario": "Improve recall",
        "prompt": "Design a hybrid retrieval strategy using dense and BM25. Specify k values, weights, and when to rerank."
      },
      {
        "title": "Reranking plan",
        "scenario": "Increase precision",
        "prompt": "Propose a reranking approach with cross-encoders or MMR. Explain candidate size and selection criteria."
      },
      {
        "title": "Evaluation design",
        "scenario": "Quality audit",
        "prompt": "Draft an evaluation plan for a RAG system. Include accuracy, retrieval quality, groundedness metrics, and test case structure."
      }
    ],
    "output_examples": [
      {
        "input": "Outline a RAG pipeline for internal policies with citations.",
        "output": [
          "Load policy documents from a controlled folder and split into 800 token chunks",
          "Create embeddings and store them in a vector database",
          "Use hybrid retrieval and rerank the top 20 results",
          "Answer with citations that reference source files and sections"
        ]
      }
    ],
    "best_practices": [
      "Use metadata for filtering and debugging.",
      "Combine hybrid search with reranking for top results.",
      "Track retrieval metrics during evaluation."
    ],
    "anti_patterns": [
      "Indexing documents without chunk overlap.",
      "Skipping citations in user-facing answers.",
      "Using only dense retrieval for keyword-heavy queries."
    ],
    "faq": [
      {
        "question": "Which platforms does this support",
        "answer": "It works with Claude, Codex, and Claude Code prompts and is framework agnostic."
      },
      {
        "question": "What are the main limits",
        "answer": "It provides guidance only and does not run code or manage infrastructure."
      },
      {
        "question": "How do I integrate it into my app",
        "answer": "Follow the pipeline steps and map loaders, embeddings, and retrievers to your stack."
      },
      {
        "question": "Does it access my data",
        "answer": "No. It is a text guide and does not read files or send network requests."
      },
      {
        "question": "What if retrieval quality is low",
        "answer": "Adjust chunk size, embedding model, filters, and reranking weights, then re-test."
      },
      {
        "question": "How is this different from basic search",
        "answer": "It combines semantic retrieval with grounding, which improves relevance over keyword search."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
