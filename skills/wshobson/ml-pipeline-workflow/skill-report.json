{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T19:35:03.653Z",
    "slug": "wshobson-ml-pipeline-workflow",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/machine-learning-ops/skills/ml-pipeline-workflow",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "c4e340ed92e2dbd20c758e9fdd9bad0c0f0f522385a80703bf0959364681b619",
    "tree_hash": "a8f5f70485e506824ddf7e561d5a9cd897d8265572b35dc48eafd5b3854d973d"
  },
  "skill": {
    "name": "ml-pipeline-workflow",
    "description": "Build end-to-end MLOps pipelines from data preparation through model training, validation, and production deployment. Use when creating ML pipelines, implementing MLOps practices, or automating model training and deployment workflows.",
    "summary": "Complete MLOps pipeline orchestration from data preparation through model deployment and monitoring.",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "data",
    "tags": [
      "mlops",
      "machine-learning",
      "pipeline",
      "deployment",
      "data-engineering"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill contains only documentation and guidance for ML pipeline workflows with no executable code. All static findings are false positives from pattern matching on markdown file extensions and documentation examples. The skill provides templates and best practices for MLOps workflows with no security concerns.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 557,
    "audit_model": "claude",
    "audited_at": "2026-01-21T19:35:03.653Z"
  },
  "content": {
    "user_title": "Build Production ML Pipelines with End-to-End Orchestration",
    "value_statement": "Machine learning teams struggle to connect data preparation, training, validation, and deployment into reliable production workflows. This skill provides comprehensive guidance for building end-to-end MLOps pipelines with proper orchestration, monitoring, and deployment strategies.",
    "seo_keywords": [
      "Claude",
      "Codex",
      "Claude Code",
      "MLOps",
      "machine learning pipeline",
      "model deployment",
      "data engineering",
      "ML orchestration",
      "experiment tracking",
      "model validation"
    ],
    "actual_capabilities": [
      "Design end-to-end ML pipeline architectures with DAG orchestration patterns for Airflow, Dagster, and Kubeflow",
      "Implement data preparation workflows with validation, feature engineering, versioning, and lineage tracking",
      "Set up model training orchestration with hyperparameter management, experiment tracking, and distributed training patterns",
      "Create model validation frameworks with A/B testing, performance regression detection, and comparison workflows",
      "Build deployment automation with canary releases, blue-green strategies, rollback mechanisms, and monitoring",
      "Integrate with MLOps tools including MLflow, Weights and Biases, TensorBoard, and cloud ML platforms"
    ],
    "limitations": [
      "Provides architectural guidance and best practices rather than pre-built pipeline code implementations",
      "Does not include specific configuration for proprietary ML platforms or custom infrastructure setups",
      "Focuses on general MLOps patterns and may require adaptation for domain-specific ML workflows",
      "Does not cover advanced topics like federated learning, edge deployment, or specialized hardware acceleration"
    ],
    "use_cases": [
      {
        "title": "Build New ML Pipeline from Scratch",
        "description": "Design and implement a complete MLOps pipeline for a new machine learning project with data ingestion, training, validation, and deployment stages.",
        "target_user": "ML Engineers and Data Scientists starting new ML projects"
      },
      {
        "title": "Modernize Legacy ML Workflows",
        "description": "Refactor existing manual or fragmented ML processes into automated, orchestrated pipelines with proper versioning and monitoring.",
        "target_user": "Platform Engineers upgrading ML infrastructure"
      },
      {
        "title": "Implement Production Deployment Strategy",
        "description": "Set up safe model deployment workflows with canary releases, A/B testing, and automated rollback for production ML systems.",
        "target_user": "MLOps Teams managing production model deployments"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Pipeline Architecture",
        "prompt": "Help me design a simple ML pipeline for a classification model that includes data validation, training, and deployment stages. The pipeline should run on Airflow.",
        "scenario": "Starting a new ML project and need a basic pipeline structure"
      },
      {
        "title": "Data Preparation Workflow",
        "prompt": "Create a data preparation pipeline that validates input data quality, engineers features, and versions datasets for reproducibility. Include Great Expectations for validation.",
        "scenario": "Building robust data processing before model training"
      },
      {
        "title": "Model Validation Framework",
        "prompt": "Design a model validation workflow that compares new models against baselines, runs performance tests, and generates approval reports before deployment.",
        "scenario": "Ensuring model quality before production deployment"
      },
      {
        "title": "Production Deployment Strategy",
        "prompt": "Implement a canary deployment workflow for ML models with gradual traffic rollout, automated performance monitoring, and rollback triggers if metrics degrade.",
        "scenario": "Safely deploying models to production with risk mitigation"
      }
    ],
    "output_examples": [
      {
        "input": "Design a batch training pipeline for a recommendation model that retrains weekly",
        "output": "Pipeline architecture with scheduled data ingestion from production database, feature engineering with historical user interactions, distributed training on GPU cluster, validation against hold-out test set, and automated deployment to serving infrastructure if performance thresholds are met. Includes MLflow experiment tracking and model registry integration."
      },
      {
        "input": "How do I implement A/B testing for comparing two ML models in production?",
        "output": "A/B testing framework with traffic splitting between model versions, metric collection for both models, statistical significance testing, and automated winner selection based on business metrics. Implementation uses feature flags for traffic routing and real-time monitoring dashboards."
      },
      {
        "input": "What monitoring should I set up for a production ML pipeline?",
        "output": "Comprehensive monitoring strategy including data drift detection for input features, model performance metrics, prediction latency and throughput, error rates and failure modes, resource utilization, and data quality checks. Alerts configured for threshold violations with automated rollback capabilities."
      }
    ],
    "best_practices": [
      "Design pipelines with modular stages that can be tested independently and implement idempotency so re-running stages is safe without side effects.",
      "Version all artifacts including datasets, feature transformations, model code, and trained models using tools like DVC, MLflow, or custom versioning systems.",
      "Implement gradual rollout strategies starting with shadow deployments, progressing to canary releases, and maintaining automated rollback capabilities for production models."
    ],
    "anti_patterns": [
      "Avoid tightly coupling pipeline stages or hardcoding dependencies that make it difficult to test components in isolation or modify the workflow.",
      "Do not skip validation stages or deploy models directly to production without proper testing, comparison against baselines, and approval workflows.",
      "Never ignore monitoring and alerting for production models as this leads to undetected performance degradation, data drift, and model failures."
    ],
    "faq": [
      {
        "question": "What orchestration tool should I use for ML pipelines?",
        "answer": "The choice depends on your infrastructure and team preferences. Apache Airflow is widely adopted with strong community support. Dagster offers modern asset-based orchestration. Kubeflow Pipelines is ideal for Kubernetes environments. Prefect provides a developer-friendly Python-first approach. Start with what your team already uses for data workflows."
      },
      {
        "question": "How do I handle model versioning and rollback?",
        "answer": "Use a model registry like MLflow or cloud platform registries to version models with metadata. Implement blue-green or canary deployment strategies that keep previous model versions running. Set up automated health checks and rollback triggers based on performance metrics. Maintain artifacts and configurations for each version to enable quick rollback."
      },
      {
        "question": "What is the difference between batch and real-time ML pipelines?",
        "answer": "Batch pipelines process data and make predictions on a schedule with higher latency but better resource efficiency. Real-time pipelines serve predictions with low latency for individual requests but require more infrastructure. Many production systems use hybrid approaches with real-time serving backed by batch feature engineering and model updates."
      },
      {
        "question": "How do I implement data validation in ML pipelines?",
        "answer": "Use libraries like Great Expectations or TensorFlow Data Validation to define data schemas and quality checks. Validate data types, value ranges, distributions, and relationships at pipeline boundaries. Fail fast when validation fails rather than propagating bad data. Log validation results for debugging and monitoring data quality over time."
      },
      {
        "question": "What metrics should I track for ML pipeline health?",
        "answer": "Track pipeline execution time and success rates for each stage. Monitor data volumes and feature distributions for drift detection. Log model performance metrics including accuracy, precision, and recall. Measure prediction latency and throughput for serving. Set up alerts for anomalies and threshold violations."
      },
      {
        "question": "How do I test ML pipelines before production deployment?",
        "answer": "Test individual pipeline components with unit tests using sample data. Run integration tests on the full pipeline with realistic datasets. Perform canary deployments with small traffic percentages to validate production behavior. Use shadow deployments to compare new pipelines against existing ones without affecting users. Validate that rollback procedures work correctly."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 246
    }
  ]
}
