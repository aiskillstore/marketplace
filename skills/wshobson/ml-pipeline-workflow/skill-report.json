{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T16:41:48.900Z",
    "slug": "wshobson-ml-pipeline-workflow",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/machine-learning-ops/skills/ml-pipeline-workflow",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "c4e340ed92e2dbd20c758e9fdd9bad0c0f0f522385a80703bf0959364681b619",
    "tree_hash": "3019f8fb0b0538df58e1f825479f29be9f66ab7984cfb6d42fb36296d8e0fad0"
  },
  "skill": {
    "name": "ml-pipeline-workflow",
    "description": "Build end-to-end MLOps pipelines from data preparation through model training, validation, and production deployment. Use when creating ML pipelines, implementing MLOps practices, or automating model training and deployment workflows.",
    "summary": "Build end-to-end MLOps pipelines from data preparation through model training, validation, and produ...",
    "icon": "ðŸ§ª",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "devops",
    "tags": [
      "mlops",
      "pipelines",
      "deployment",
      "orchestration"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Only documentation files are present in this skill. There is no executable code, scripting, or network behavior to audit.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 463,
    "audit_model": "claude",
    "audited_at": "2026-01-04T16:41:48.900Z"
  },
  "content": {
    "user_title": "Design end-to-end ML pipelines",
    "value_statement": "Teams struggle to connect data, training, validation, and deployment into one reliable flow. This skill provides a clear pipeline structure and best practices.",
    "seo_keywords": [
      "mlops pipeline",
      "ml workflow orchestration",
      "model training automation",
      "model validation",
      "model deployment",
      "Airflow Dagster Kubeflow",
      "Claude",
      "Codex",
      "Claude Code",
      "ML pipeline design"
    ],
    "actual_capabilities": [
      "Outline pipeline stages from ingestion to deployment",
      "Recommend DAG orchestration patterns and dependencies",
      "Describe data validation and feature engineering steps",
      "Define training workflows with experiment tracking",
      "Explain validation metrics and regression checks",
      "Suggest deployment strategies like canary and blue green"
    ],
    "limitations": [
      "Does not run pipelines or execute code",
      "Does not connect to cloud services or registries",
      "Does not generate full infrastructure configurations",
      "Requires user provided data sources and requirements"
    ],
    "use_cases": [
      {
        "target_user": "Data engineer",
        "title": "Plan data preparation flow",
        "description": "Map ingestion, validation, and feature engineering steps for a new ML pipeline."
      },
      {
        "target_user": "ML engineer",
        "title": "Design training and validation",
        "description": "Structure training jobs, metrics, and approval gates before deployment."
      },
      {
        "target_user": "MLOps engineer",
        "title": "Prepare deployment strategy",
        "description": "Choose rollout patterns, monitoring needs, and rollback criteria for models."
      }
    ],
    "prompt_templates": [
      {
        "title": "Start a simple pipeline",
        "scenario": "New project with batch data",
        "prompt": "Create a basic ML pipeline plan from ingestion to deployment for a batch dataset. Include stages, inputs, and outputs."
      },
      {
        "title": "Add validation gates",
        "scenario": "Pipeline needs quality checks",
        "prompt": "Extend the pipeline with data validation and model evaluation gates. List metrics and pass fail criteria."
      },
      {
        "title": "Design an Airflow DAG",
        "scenario": "Orchestration with Airflow",
        "prompt": "Propose an Airflow DAG structure for the pipeline, including dependencies and retry policies."
      },
      {
        "title": "Plan multi model rollout",
        "scenario": "Multiple models in production",
        "prompt": "Design a rollout plan with canary testing, A/B validation, and rollback triggers for multiple models."
      }
    ],
    "output_examples": [
      {
        "input": "Draft an end to end pipeline for a churn model with validation and deployment.",
        "output": [
          "Stages: ingest, validate, feature engineering, train, evaluate, deploy",
          "Validation: baseline comparison, drift checks, and performance thresholds",
          "Deployment: canary release with rollback on regression",
          "Monitoring: data drift and latency alerts"
        ]
      }
    ],
    "best_practices": [
      "Define clear stage boundaries and inputs",
      "Track data and model versions consistently",
      "Design rollback and monitoring before launch"
    ],
    "anti_patterns": [
      "Skipping validation before deployment",
      "Mixing training and serving infrastructure",
      "Ignoring data lineage and versioning"
    ],
    "faq": [
      {
        "question": "Is it compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes, the guidance works across Claude, Codex, and Claude Code."
      },
      {
        "question": "What are the main limits?",
        "answer": "It provides guidance only and does not run pipelines or generate full configs."
      },
      {
        "question": "Can it integrate with Airflow or Kubeflow?",
        "answer": "It explains how to structure workflows for tools like Airflow and Kubeflow."
      },
      {
        "question": "Does it access or store my data?",
        "answer": "No, it does not access files, services, or user data."
      },
      {
        "question": "How do I troubleshoot pipeline failures?",
        "answer": "Check stage logs, validate inputs at boundaries, and test components in isolation."
      },
      {
        "question": "How does it compare to a deployment skill?",
        "answer": "This skill covers the full pipeline, while deployment skills focus on serving and rollout."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
