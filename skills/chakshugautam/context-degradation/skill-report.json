{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T21:02:28.177Z",
    "slug": "chakshugautam-context-degradation",
    "source_url": "https://github.com/ChakshuGautam/games/tree/main/.claude/skills/context-degradation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "8afbe608dd3ed3e1bedcb1202d766baf626a4df8d9bfab28f6dd1bcd68ff9ad2",
    "tree_hash": "ad0506d45e37c844491a6a41c3b9cbe1f1efc7edd92315d43eada2ff2680d16d"
  },
  "skill": {
    "name": "context-degradation",
    "description": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle issues\", \"debug agent failures\", \"understand context poisoning\", or mentions context degradation, attention patterns, context clash, context confusion, or agent performance degradation. Provides patterns for recognizing and mitigating context failures.",
    "summary": "This skill should be used when the user asks to \"diagnose context problems\", \"fix lost-in-middle iss...",
    "icon": "üîç",
    "version": "1.0.0",
    "author": "ChakshuGautam",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "context-engineering",
      "llm-performance",
      "agent-debugging",
      "attention-mechanisms"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "external_commands",
      "network",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is an educational documentation skill with no executable code that modifies files, makes network calls, or accesses sensitive data. The Python utility module contains only in-memory text processing functions for analyzing context strings. All 59 static findings are false positives - the scanner incorrectly flagged documentation patterns and metadata as security risks.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "references/patterns.md",
            "line_start": 11,
            "line_end": 39
          },
          {
            "file": "references/patterns.md",
            "line_start": 39,
            "line_end": 45
          },
          {
            "file": "references/patterns.md",
            "line_start": 45,
            "line_end": 79
          },
          {
            "file": "references/patterns.md",
            "line_start": 79,
            "line_end": 87
          },
          {
            "file": "references/patterns.md",
            "line_start": 87,
            "line_end": 118
          },
          {
            "file": "references/patterns.md",
            "line_start": 118,
            "line_end": 124
          },
          {
            "file": "references/patterns.md",
            "line_start": 124,
            "line_end": 153
          },
          {
            "file": "references/patterns.md",
            "line_start": 153,
            "line_end": 161
          },
          {
            "file": "references/patterns.md",
            "line_start": 161,
            "line_end": 192
          },
          {
            "file": "references/patterns.md",
            "line_start": 192,
            "line_end": 200
          },
          {
            "file": "references/patterns.md",
            "line_start": 200,
            "line_end": 255
          },
          {
            "file": "references/patterns.md",
            "line_start": 255,
            "line_end": 261
          },
          {
            "file": "references/patterns.md",
            "line_start": 261,
            "line_end": 269
          },
          {
            "file": "references/patterns.md",
            "line_start": 269,
            "line_end": 277
          },
          {
            "file": "references/patterns.md",
            "line_start": 277,
            "line_end": 313
          },
          {
            "file": "SKILL.md",
            "line_start": 162,
            "line_end": 169
          },
          {
            "file": "SKILL.md",
            "line_start": 169,
            "line_end": 172
          },
          {
            "file": "SKILL.md",
            "line_start": 172,
            "line_end": 188
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 1170,
    "audit_model": "claude",
    "audited_at": "2026-01-16T21:02:28.177Z"
  },
  "content": {
    "user_title": "Diagnose context degradation patterns",
    "value_statement": "LLM agents fail unpredictably as context grows. This skill helps you identify and fix context degradation issues like lost-in-middle effects, poisoning, and attention collapse so your agents stay reliable at scale.",
    "seo_keywords": [
      "context degradation",
      "lost in middle",
      "Claude context",
      "LLM attention patterns",
      "context poisoning",
      "agent debugging",
      "context engineering",
      "Claude Code",
      "long context",
      "attention degradation"
    ],
    "actual_capabilities": [
      "Diagnose lost-in-middle effects where critical information gets ignored",
      "Detect context poisoning from hallucinations and error accumulation",
      "Identify context distraction from irrelevant information",
      "Analyze context clash from contradictory information",
      "Provide model-specific degradation thresholds",
      "Recommend architectural patterns for mitigation"
    ],
    "limitations": [
      "Does not access actual model attention weights (uses simulation)",
      "Cannot modify your running agent or system state",
      "Cannot detect all hallucination patterns reliably",
      "Only provides analysis and recommendations, not automatic fixes"
    ],
    "use_cases": [
      {
        "target_user": "Agent developers",
        "title": "Debug agent failures",
        "description": "Investigate why your agent produces incorrect outputs during long conversations."
      },
      {
        "target_user": "ML engineers",
        "title": "Design robust systems",
        "description": "Architect context management strategies that prevent degradation in production systems."
      },
      {
        "target_user": "AI researchers",
        "title": "Understand attention patterns",
        "description": "Learn how context length affects model behavior and design experiments accordingly."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick diagnosis",
        "scenario": "Agent is failing mid-conversation",
        "prompt": "My agent is producing incorrect outputs after 20+ conversation turns. Diagnose potential context degradation issues."
      },
      {
        "title": "Specific pattern",
        "scenario": "Suspect lost-in-middle",
        "prompt": "I placed critical instructions in the middle of context but the agent ignores them. Explain the lost-in-middle effect."
      },
      {
        "title": "Architectural design",
        "scenario": "Building long-context system",
        "prompt": "Design context management patterns to handle 100K+ tokens without degradation."
      },
      {
        "title": "Poisoning recovery",
        "scenario": "Context contains errors",
        "prompt": "My agent context contains hallucinated information that keeps propagating. How do I detect and recover from context poisoning?"
      }
    ],
    "output_examples": [
      {
        "input": "My Claude agent ignores important instructions placed in the middle of long conversations",
        "output": [
          "This is likely the lost-in-middle effect. Research shows models have U-shaped attention: high at beginning and end, low in middle.",
          "Critical items in degraded regions have 10-40% lower recall accuracy.",
          "Move critical information to the first 10% or last 10% of context.",
          "Use explicit markers like [CRITICAL] or section headers to draw attention.",
          "Consider splitting conversations before degradation threshold (typically 60K-100K tokens for Claude)."
        ]
      },
      {
        "input": "My agent is making inconsistent decisions after processing multiple retrieved documents",
        "output": [
          "This suggests context clash from contradictory information in retrieved sources.",
          "Multi-source retrieval often introduces conflicting facts that derail reasoning.",
          "Implement version filtering to exclude outdated information.",
          "Add explicit conflict markers and ask for clarification when contradictions appear.",
          "Consider priority rules that establish which source takes precedence."
        ]
      }
    ],
    "best_practices": [
      "Place critical information at the start or end of context, never buried in the middle.",
      "Monitor context length and implement compaction triggers before reaching degradation thresholds.",
      "Validate retrieved documents before adding them to context to prevent poisoning.",
      "Use version filtering to exclude outdated information that causes context clash."
    ],
    "anti_patterns": [
      "Adding all available information to context hoping more is better.",
      "Placing critical instructions in the middle of long documents.",
      "Assuming larger context windows eliminate degradation problems.",
      "Ignoring error accumulation that can poison context over time."
    ],
    "faq": [
      {
        "question": "Does this skill work with Claude Code?",
        "answer": "Yes, this skill works with Claude Code. The patterns apply to all major LLMs including Claude, GPT, and Gemini."
      },
      {
        "question": "What context lengths cause degradation?",
        "answer": "Degradation typically begins around 60K-100K tokens for Claude models. Severe degradation appears around 150K-200K tokens depending on task complexity."
      },
      {
        "question": "Can I integrate this with my agent?",
        "answer": "The skill provides patterns and analysis. The Python module can be imported into your codebase for programmatic context health checking."
      },
      {
        "question": "Does this access my data?",
        "answer": "No. The skill only processes text strings you provide. No files are read, no network calls are made, no data leaves your system."
      },
      {
        "question": "How does lost-in-middle differ from forgetting?",
        "answer": "Lost-in-middle is about attention allocation, not memory. The model attends less to middle content regardless of its importance."
      },
      {
        "question": "How does this compare to context-fundamentals?",
        "answer": "context-degradation builds on context-fundamentals by focusing specifically on failure patterns and mitigation strategies."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "patterns.md",
          "type": "file",
          "path": "references/patterns.md",
          "lines": 315
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "degradation_detector.py",
          "type": "file",
          "path": "scripts/degradation_detector.py",
          "lines": 420
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 232
    }
  ]
}
