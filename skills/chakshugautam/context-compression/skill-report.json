{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T20:58:51.928Z",
    "slug": "chakshugautam-context-compression",
    "source_url": "https://github.com/ChakshuGautam/games/tree/main/.claude/skills/context-compression",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "bc398950888ae73df9651f44f2fb51acea9409362d736592c771e38d236022ae",
    "tree_hash": "9d330c84a3e33b3b8def69c29417a0f1485695ae90649c3bf17a4b8622cf5881"
  },
  "skill": {
    "name": "context-compression",
    "description": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\", \"implement compaction\", \"reduce token usage\", or mentions context compression, structured summarization, tokens-per-task optimization, or long-running agent sessions exceeding context limits.",
    "summary": "This skill should be used when the user asks to \"compress context\", \"summarize conversation history\"...",
    "icon": "üóúÔ∏è",
    "version": "1.1.0",
    "author": "ChakshuGautam",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "context-compression",
      "token-optimization",
      "ai-agents",
      "conversation-summarization",
      "evaluation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "external_commands",
      "network",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 57 static findings are false positives. The 'external_commands' flags are markdown backticks for formatting. 'Weak cryptographic algorithm' flags are mentions of 'GPT-5.2' as example model names. 'System reconnaissance' flags are technical terms in documentation. 'Windows SAM database' is a regex pattern for error extraction, not credential theft. This is pure documentation with a stub Python implementation containing no actual API calls, network access, or file system operations outside its directory.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "references/evaluation-framework.md",
            "line_start": 12,
            "line_end": 16
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 16,
            "line_end": 28
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 28,
            "line_end": 32
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 32,
            "line_end": 44
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 44,
            "line_end": 48
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 48,
            "line_end": 60
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 60,
            "line_end": 64
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 64,
            "line_end": 121
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 121,
            "line_end": 131
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 131,
            "line_end": 135
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 135,
            "line_end": 143
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 143,
            "line_end": 147
          },
          {
            "file": "references/evaluation-framework.md",
            "line_start": 147,
            "line_end": 166
          },
          {
            "file": "scripts/compression_evaluator.py",
            "line_start": 358,
            "line_end": 367
          },
          {
            "file": "SKILL.md",
            "line_start": 55,
            "line_end": 76
          },
          {
            "file": "SKILL.md",
            "line_start": 76,
            "line_end": 187
          },
          {
            "file": "SKILL.md",
            "line_start": 187,
            "line_end": 208
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 1354,
    "audit_model": "claude",
    "audited_at": "2026-01-16T20:58:51.928Z"
  },
  "content": {
    "user_title": "Compress AI Agent Context Efficiently",
    "value_statement": "Long AI agent sessions quickly exceed context windows, causing failures. This skill provides proven compression strategies that preserve critical information while reducing tokens by 98%+.",
    "seo_keywords": [
      "context compression",
      "AI agent optimization",
      "token reduction",
      "Claude context window",
      "conversation summarization",
      "Codex compression",
      "Claude Code optimization",
      "agent memory management",
      "structured summarization",
      "token efficiency"
    ],
    "actual_capabilities": [
      "Implements anchored iterative summarization with 98.6% compression ratio",
      "Provides probe-based evaluation framework for compression quality",
      "Generates structured summaries with explicit file tracking sections",
      "Evaluates compression across 6 dimensions: accuracy, context, artifacts, completeness, continuity, instructions",
      "Includes Python implementation for compression evaluation"
    ],
    "limitations": [
      "Python evaluator uses stub implementation - needs actual LLM API integration",
      "Artifact trail tracking is universally weak across all compression methods",
      "Requires manual integration into agent workflows",
      "Evaluation framework needs ground truth data for optimal results"
    ],
    "use_cases": [
      {
        "target_user": "AI agent developers",
        "title": "Build Context-Aware Agents",
        "description": "Implement compression in agents that handle long coding sessions or document analysis tasks exceeding context limits."
      },
      {
        "target_user": "ML engineers",
        "title": "Evaluate Compression Quality",
        "description": "Use probe-based evaluation to measure how well different compression methods preserve task-critical information."
      },
      {
        "target_user": "DevOps teams",
        "title": "Optimize Production Agents",
        "description": "Deploy structured summarization to reduce token costs while maintaining agent performance on complex tasks."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Compression",
        "scenario": "Agent session hitting context limits",
        "prompt": "Compress this conversation history using structured summarization. Include sections for: session intent, files modified, decisions made, current state, and next steps."
      },
      {
        "title": "Evaluation Setup",
        "scenario": "Testing compression quality",
        "prompt": "Generate probe questions for this compressed context to test if key information was preserved: file paths, error messages, decisions, and next steps."
      },
      {
        "title": "Iterative Update",
        "scenario": "Updating existing summary",
        "prompt": "Merge this new conversation span into the existing structured summary. Update only the relevant sections while preserving information from previous compression cycles."
      },
      {
        "title": "Production Integration",
        "scenario": "Implementing in agent workflow",
        "prompt": "Design a compression trigger strategy for a coding agent that activates at 75% context utilization, using anchored iterative summarization with file tracking."
      }
    ],
    "output_examples": [
      {
        "input": "Compress a debugging session where we fixed a 401 error by updating Redis configuration",
        "output": [
          "## Session Intent",
          "Debug 401 Unauthorized error on /api/auth/login endpoint",
          "",
          "## Files Modified",
          "- config/redis.ts: Fixed connection pooling configuration",
          "- services/session.service.ts: Added retry logic for transient failures",
          "",
          "## Decisions Made",
          "- Using Redis connection pool instead of per-request connections",
          "- Retry logic with exponential backoff for transient failures",
          "",
          "## Current State",
          "14 tests passing, 2 failing (mock setup issues)",
          "",
          "## Next Steps",
          "1. Fix remaining test failures",
          "2. Run full test suite",
          "3. Deploy to staging"
        ]
      },
      {
        "input": "Evaluate if compression preserved key information",
        "output": [
          "Accuracy: 4.2/5.0 - Most technical details preserved correctly",
          "Context Awareness: 4.0/5.0 - Session state understood",
          "Artifact Trail: 2.5/5.0 - Some file tracking lost (expected weakness)",
          "Completeness: 4.4/5.0 - All question aspects addressed",
          "Continuity: 3.8/5.0 - Can continue with minimal re-fetching",
          "Recommendation: Compression quality acceptable for production use"
        ]
      }
    ],
    "best_practices": [
      "Trigger compression at 70-80% context utilization to balance quality and token savings",
      "Use structured summaries with explicit sections to force preservation of critical information",
      "Monitor tokens-per-task metric rather than tokens-per-request to measure true efficiency"
    ],
    "anti_patterns": [
      "Don't use aggressive compression that loses file paths and error messages - causes expensive re-fetching",
      "Avoid regenerating entire summaries on each compression - use incremental merging instead",
      "Don't rely solely on lexical metrics like ROUGE - use functional evaluation with probes"
    ],
    "faq": [
      {
        "question": "Which compression method should I use?",
        "answer": "Use anchored iterative summarization for coding tasks - it provides best quality retention with 98.6% compression ratio."
      },
      {
        "question": "How do I know if compression is working?",
        "answer": "Use probe-based evaluation: ask specific questions about files, errors, and decisions to verify critical information was preserved."
      },
      {
        "question": "Can I use this with any AI model?",
        "answer": "Yes, the strategies are model-agnostic. The evaluation framework works with any LLM that can act as a judge."
      },
      {
        "question": "Why is artifact tracking so weak?",
        "answer": "File tracking is inherently difficult for general summarization. Consider implementing separate artifact indexing for critical file operations."
      },
      {
        "question": "How much token savings can I expect?",
        "answer": "98-99% compression is typical. Structured methods retain 0.7% more tokens but provide 0.35 points better quality scores."
      },
      {
        "question": "When should I compress vs start a new session?",
        "answer": "Compress when you need continuity. Start fresh when switching to unrelated tasks or when compression quality drops below 3.5/5.0."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "evaluation-framework.md",
          "type": "file",
          "path": "references/evaluation-framework.md",
          "lines": 214
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "compression_evaluator.py",
          "type": "file",
          "path": "scripts/compression_evaluator.py",
          "lines": 659
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 266
    }
  ]
}
