{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-23T01:58:32.077Z",
    "slug": "enactprotocol-enact-firecrawl",
    "source_url": "https://github.com/EnactProtocol/enact/tree/main/examples/tools/firecrawl",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "241fa325f4a7b857e930a28dba3ec1e2e2d76169f9611ae1a2adc88706f7d4d2",
    "tree_hash": "bbf9f153fb8f60f487d7b3c2da99d2d9927eedb46ad9e1b46c96c5d1587cf6fc"
  },
  "skill": {
    "name": "enact/firecrawl",
    "description": "Scrape, crawl, search, and extract structured data from websites using Firecrawl API - converts web pages to LLM-ready markdown",
    "summary": "Web scraping tool that converts websites into clean markdown and structured data using the Firecrawl API",
    "icon": "üï∏Ô∏è",
    "version": "1.2.1",
    "author": "EnactProtocol",
    "license": "MIT",
    "tags": [
      "web-scraping",
      "crawling",
      "markdown",
      "llm",
      "data-extraction",
      "search",
      "structured-data"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "env_access"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate web scraping tool that makes authenticated API calls to Firecrawl. Static analysis flagged expected patterns (network requests + environment variable access for API credentials) as suspicious, but these represent standard API client behavior. No malicious intent or dangerous code patterns were found.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "firecrawl.py",
            "line_start": 10,
            "line_end": 10
          },
          {
            "file": "firecrawl.py",
            "line_start": 23,
            "line_end": 36
          },
          {
            "file": "firecrawl.py",
            "line_start": 42,
            "line_end": 56
          },
          {
            "file": "firecrawl.py",
            "line_start": 69,
            "line_end": 73
          },
          {
            "file": "firecrawl.py",
            "line_start": 92,
            "line_end": 101
          },
          {
            "file": "firecrawl.py",
            "line_start": 106,
            "line_end": 118
          },
          {
            "file": "firecrawl.py",
            "line_start": 137,
            "line_end": 145
          },
          {
            "file": "firecrawl.py",
            "line_start": 159,
            "line_end": 163
          },
          {
            "file": "firecrawl.py",
            "line_start": 230,
            "line_end": 236
          }
        ]
      },
      {
        "factor": "env_access",
        "evidence": [
          {
            "file": "firecrawl.py",
            "line_start": 13,
            "line_end": 18
          },
          {
            "file": "firecrawl.py",
            "line_start": 195,
            "line_end": 199
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 475,
    "audit_model": "claude",
    "audited_at": "2026-01-23T01:58:32.077Z"
  },
  "content": {
    "user_title": "Scrape websites as markdown",
    "value_statement": "Web scraping is time-consuming and often breaks due to anti-bot measures. This skill uses the Firecrawl API to reliably convert websites into clean, LLM-ready markdown with structured data extraction.",
    "seo_keywords": [
      "web scraping",
      "crawl websites",
      "extract website content",
      "markdown converter",
      "llm content extraction",
      "structured data from web",
      "web scraping api",
      "Claude Code",
      "Claude",
      "Codex"
    ],
    "actual_capabilities": [
      "Scrape single URLs and convert to markdown, HTML, or screenshots",
      "Crawl entire websites and discover all accessible pages",
      "Map website structures to list all URLs without scraping content",
      "Search the web and return scraped results for any query",
      "Extract structured data from pages using AI prompts or JSON schemas"
    ],
    "limitations": [
      "Requires a Firecrawl API key for authentication",
      "Cannot bypass paywalls or access members-only content",
      "Rate limited by Firecrawl API quotas and pricing tiers",
      "Dynamic JavaScript-heavy sites may have extended processing times"
    ],
    "use_cases": [
      {
        "title": "Research and data collection",
        "description": "Gather information from multiple web sources for research projects, competitor analysis, or market research. Convert documentation, articles, and product pages into clean markdown for AI processing.",
        "target_user": "Researchers and analysts"
      },
      {
        "title": "Documentation ingestion",
        "description": "Scrape and crawl technical documentation sites to create offline knowledge bases. Perfect for building context for AI assistants or archiving documentation.",
        "target_user": "Developers and technical writers"
      },
      {
        "title": "Content aggregation",
        "description": "Extract structured data from websites like news headlines, pricing tables, product listings, or event calendars. Use natural language prompts or JSON schemas for precise extraction.",
        "target_user": "Data engineers and content managers"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic page scrape",
        "prompt": "Scrape the content from ${url} and return it as clean markdown.",
        "scenario": "When you need content from a single webpage"
      },
      {
        "title": "Crawl documentation site",
        "prompt": "Crawl ${url} with a limit of ${limit} pages. Extract all content as markdown.",
        "scenario": "When you need to gather documentation from an entire site"
      },
      {
        "title": "Extract structured data",
        "prompt": "Extract structured data from ${url} using this schema: ${schema}. Prompt: ${prompt}",
        "scenario": "When you need specific data in a structured format"
      },
      {
        "title": "Web search",
        "prompt": "Search the web for \"${query}\" and return the top ${limit} results with full scraped content.",
        "scenario": "When you need to find and gather information from multiple sources"
      }
    ],
    "output_examples": [
      {
        "input": "https://example.com/blog post",
        "output": "Blog post extracted with clean markdown including headers, paragraphs, and links."
      },
      {
        "input": "https://news.ycombinator.com with extract action",
        "output": "List of top 5 news headlines with URLs and point counts in structured format."
      }
    ],
    "best_practices": [
      "Set the FIRECRAWL_API_KEY environment variable before use to authenticate with the Firecrawl service",
      "Use the extract action with JSON schemas for consistent, structured data extraction from similar pages",
      "Start with the map action to discover website structure before running expensive crawl operations"
    ],
    "anti_patterns": [
      "Do not use for scraping personal or private data without authorization",
      "Avoid excessive crawling that could strain target websites - respect rate limits and robots.txt",
      "Do not rely on screenshots alone for data extraction - markdown and structured extraction are more reliable"
    ],
    "faq": [
      {
        "question": "What is the FIRECRAWL_API_KEY and where do I get it?",
        "answer": "The FIRECRAWL_API_KEY is your authentication token for the Firecrawl service. Get it by signing up at firecrawl.dev and copying your API key from the dashboard."
      },
      {
        "question": "What is the difference between scrape, crawl, map, search, and extract actions?",
        "answer": "Scrape gets content from one URL. Crawl discovers and scrapes all pages on a site. Map lists URLs without scraping content. Search queries the web and returns results. Extract pulls structured data using AI."
      },
      {
        "question": "What output formats are supported?",
        "answer": "The scrape and crawl actions support markdown, HTML, links, and screenshot formats. Use the formats parameter to specify comma-separated output types."
      },
      {
        "question": "How many pages can I crawl?",
        "answer": "The crawl action has a configurable limit parameter. The Firecrawl API also has rate limits based on your subscription tier."
      },
      {
        "question": "Can I extract data into a specific format?",
        "answer": "Yes. Use the extract action with a JSON schema to define the data structure you want. The AI will extract matching fields from the page content."
      },
      {
        "question": "Does this work on JavaScript-heavy single-page applications?",
        "answer": "Yes. Firecrawl handles JavaScript rendering and can extract content from dynamic SPAs, though processing may take longer than static pages."
      }
    ]
  },
  "file_structure": [
    {
      "name": "firecrawl.py",
      "type": "file",
      "path": "firecrawl.py",
      "lines": 250
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 225
    }
  ]
}
