{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T14:01:12.512Z",
    "slug": "89jobrien-ai-ethics",
    "source_url": "https://github.com/89jobrien/steve/tree/main/steve/skills/ai-ethics",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "a71373e2f0b7ed7c7b3b1697c164c171a2076d90a6cefb8fbbf3ef7b7769ad3f",
    "tree_hash": "ddc08f25b2899b929c8e3210d706cd7796dfd2de956c3558b38c9258ec0d903b"
  },
  "skill": {
    "name": "ai-ethics",
    "description": "Responsible AI development and ethical considerations. Use when evaluating AI bias, implementing fairness measures, conducting ethical assessments, or ensuring AI systems align with human values.",
    "summary": "Responsible AI development and ethical considerations. Use when evaluating AI bias, implementing fai...",
    "icon": "üõ°Ô∏è",
    "version": "1.0.1",
    "author": "Joseph OBrien",
    "license": "UNLICENSED",
    "category": "security",
    "tags": [
      "bias-detection",
      "fairness",
      "governance",
      "compliance",
      "responsible-ai"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure prompt-based documentation skill with no code execution capabilities. The SKILL.md file contains only informational content about AI ethics principles, bias detection, governance frameworks, and regulatory compliance. No network calls, filesystem access, or command execution patterns were detected in the actual content.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 201,
            "line_end": 201
          },
          {
            "file": "SKILL.md",
            "line_start": 202,
            "line_end": 202
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 386,
    "audit_model": "claude",
    "audited_at": "2026-01-16T14:01:12.512Z"
  },
  "content": {
    "user_title": "Apply AI ethics principles to development",
    "value_statement": "AI systems can perpetuate bias and cause harm without proper ethical safeguards. This skill provides comprehensive guidance for implementing fairness measures, detecting bias, ensuring regulatory compliance, and building trustworthy AI systems.",
    "seo_keywords": [
      "AI ethics",
      "bias detection",
      "fairness",
      "AI governance",
      "regulatory compliance",
      "Claude AI",
      "Codex",
      "Claude Code",
      "responsible AI",
      "explainability"
    ],
    "actual_capabilities": [
      "Evaluate AI models for bias across different demographic groups",
      "Implement fairness metrics including demographic parity and equalized odds",
      "Conduct ethical impact assessments for AI systems",
      "Ensure compliance with EU AI Act and similar regulations",
      "Design human-in-the-loop oversight patterns",
      "Create model cards and transparency documentation"
    ],
    "limitations": [
      "Does not execute code or access external systems",
      "Provides guidance only, not automated bias detection tools",
      "Does not replace human ethics review boards",
      "References external frameworks but does not provide legal advice"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Build Fair Models",
        "description": "Apply bias detection and mitigation techniques during model development lifecycle to prevent discriminatory outcomes."
      },
      {
        "target_user": "Product Managers",
        "title": "Assess AI Impact",
        "description": "Conduct ethical impact assessments before deploying AI features to identify potential harms to users and stakeholders."
      },
      {
        "target_user": "Compliance Officers",
        "title": "Meet Regulatory Requirements",
        "description": "Document AI systems according to EU AI Act requirements and prepare for audits and certification processes."
      }
    ],
    "prompt_templates": [
      {
        "title": "Check for Bias",
        "scenario": "Evaluate a model for bias",
        "prompt": "Use the AI Ethics skill to evaluate this AI model for potential bias. What types of bias should I look for? What fairness metrics apply?"
      },
      {
        "title": "Assess Fairness",
        "scenario": "Measure fairness across groups",
        "prompt": "Apply the AI Ethics skill to assess fairness for this classification model. How do I calculate demographic parity and equalized odds across protected groups?"
      },
      {
        "title": "Design Oversight",
        "scenario": "Create human oversight systems",
        "prompt": "Use the AI Ethics skill to design a human-in-the-loop oversight system for high-stakes AI decisions. What patterns and escalation paths should I implement?"
      },
      {
        "title": "Ensure Compliance",
        "scenario": "Meet AI regulation standards",
        "prompt": "Apply the AI Ethics skill to ensure this AI system complies with the EU AI Act. What documentation and risk assessment requirements apply?"
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate this hiring AI model for potential bias and recommend mitigation strategies",
        "output": [
          "Bias Types to Check: Historical bias from past hiring decisions, representation bias if training data underrepresents certain groups, measurement bias from proxy variables like ZIP code",
          "Fairness Metrics to Apply: Calculate demographic parity across gender, equalized odds for false positive and negative rates, predictive parity for precision",
          "Recommended Mitigation: Pre-processing with resampling and reweighting, in-processing with fairness constraints in loss function, post-processing with threshold adjustment per group",
          "Documentation Required: Model card with purpose, training data description, performance by subgroup, limitations, and version history"
        ]
      },
      {
        "input": "How do I design human oversight for an AI system that makes medical diagnoses?",
        "output": [
          "Use Human-in-the-Loop pattern for high-stakes decisions like medical diagnosis confirmation",
          "Implement clear escalation paths for low-confidence predictions requiring human review",
          "Set confidence thresholds to determine when AI can automate versus when human review is mandatory",
          "Maintain audit trails of all AI recommendations and human overrides for compliance"
        ]
      },
      {
        "input": "What documentation do I need for EU AI Act compliance?",
        "output": [
          "Risk category classification documentation showing why the system falls in its risk level",
          "Data provenance and lineage records showing training data sources and processing",
          "Model training documentation including architecture, hyperparameters, and validation results",
          "Performance metrics by demographic subgroup to demonstrate fairness across protected groups"
        ]
      }
    ],
    "best_practices": [
      "Apply bias detection throughout the ML lifecycle from data collection to model deployment and monitoring",
      "Use multiple fairness metrics as different metrics can conflict and no single metric captures all fairness aspects",
      "Document all ethical considerations, limitations, and stakeholder impact assessments in model cards"
    ],
    "anti_patterns": [
      "Assuming high overall accuracy means the model is fair across all demographic groups",
      "Using a single protected attribute for fairness checks without considering intersectionality",
      "Deploying high-stakes AI systems without human oversight or override capabilities"
    ],
    "faq": [
      {
        "question": "Does this skill work with all AI frameworks?",
        "answer": "Yes. The ethics principles apply universally regardless of the ML framework or model type being used."
      },
      {
        "question": "What fairness metrics does the skill recommend?",
        "answer": "It covers demographic parity, equalized odds, predictive parity, and individual fairness with counterfactual analysis."
      },
      {
        "question": "How does this integrate with other skills?",
        "answer": "Use with machine-learning for model development, testing for bias testing, and documentation for model cards."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "This is a prompt-based skill with no code execution. No data leaves your system and no external calls are made."
      },
      {
        "question": "Why is my model showing high accuracy but failing fairness checks?",
        "answer": "High overall accuracy can mask disparate impact. Check performance metrics separately for each demographic group."
      },
      {
        "question": "How is this different from general AI guidelines?",
        "answer": "This skill provides specific, actionable frameworks for bias mitigation, governance, and regulatory compliance."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 209
    }
  ]
}
