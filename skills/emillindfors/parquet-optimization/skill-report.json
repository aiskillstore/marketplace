{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-23T02:28:02.568Z",
    "slug": "emillindfors-parquet-optimization",
    "source_url": "https://github.com/EmilLindfors/claude-marketplace/tree/main/plugins/rust-data-engineering/skills/parquet-optimization",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "1f4a44045a563c0a760b3e158ba15e5577559582bab283b94d13ec269bd974f1",
    "tree_hash": "538d58528d2d02bbd931b1de4f7dccf58fee0ce0f8ee3e958a3ab9b8d24a17d7"
  },
  "skill": {
    "name": "parquet-optimization",
    "description": "Proactively analyzes Parquet file operations and suggests optimization improvements for compression, encoding, row group sizing, and statistics. Activates when users are reading or writing Parquet files or discussing Parquet performance.",
    "summary": "Proactively analyzes Parquet file operations and suggests optimization improvements for compression, encoding, row group sizing, and statistics.",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "EmilLindfors",
    "license": "MIT",
    "tags": [
      "parquet",
      "data-engineering",
      "compression",
      "performance",
      "data-lake"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All static findings are false positives. The 37 'external_commands' detections are markdown code formatting backticks in SKILL.md (a documentation file). The 3 'weak cryptographic algorithm' detections are misclassifications of compression algorithms (ZSTD, Snappy) as cryptographic algorithms. This is a documentation-only skill with no executable code.",
    "risk_factor_evidence": [
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 15,
            "line_end": 15
          },
          {
            "file": "SKILL.md",
            "line_start": 28,
            "line_end": 28
          },
          {
            "file": "SKILL.md",
            "line_start": 30,
            "line_end": 30
          },
          {
            "file": "SKILL.md",
            "line_start": 33,
            "line_end": 42
          },
          {
            "file": "SKILL.md",
            "line_start": 42,
            "line_end": 47
          },
          {
            "file": "SKILL.md",
            "line_start": 47,
            "line_end": 50
          },
          {
            "file": "SKILL.md",
            "line_start": 50,
            "line_end": 59
          },
          {
            "file": "SKILL.md",
            "line_start": 59,
            "line_end": 62
          },
          {
            "file": "SKILL.md",
            "line_start": 62,
            "line_end": 67
          },
          {
            "file": "SKILL.md",
            "line_start": 67,
            "line_end": 75
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [
      {
        "title": "Weak Cryptographic Algorithm Misclassification",
        "description": "Static analyzer flagged ZSTD and Snappy as weak cryptographic algorithms. These are actually compression algorithms used in Parquet file optimization.",
        "locations": [
          {
            "file": "SKILL.md",
            "line_start": 3,
            "line_end": 3
          },
          {
            "file": "SKILL.md",
            "line_start": 41,
            "line_end": 41
          },
          {
            "file": "SKILL.md",
            "line_start": 231,
            "line_end": 231
          }
        ],
        "verdict": "FALSE_POSITIVE",
        "confidence": 0.1,
        "confidence_reasoning": "ZSTD (Zstandard) and Snappy are compression algorithms, not cryptographic algorithms. This is a documentation file discussing Parquet optimization."
      }
    ],
    "medium_findings": [
      {
        "title": "Markdown Backtick Code Formatting Misclassified as External Commands",
        "description": "Static analyzer flagged markdown inline code syntax (backticks) as Ruby/shell backtick execution. SKILL.md is a documentation file containing code examples.",
        "locations": [
          {
            "file": "SKILL.md",
            "line_start": 15,
            "line_end": 15
          },
          {
            "file": "SKILL.md",
            "line_start": 28,
            "line_end": 28
          },
          {
            "file": "SKILL.md",
            "line_start": 30,
            "line_end": 30
          }
        ],
        "verdict": "FALSE_POSITIVE",
        "confidence": 0.05,
        "confidence_reasoning": "Backticks are markdown syntax for inline code formatting. No actual shell or Ruby execution exists in this documentation file."
      }
    ],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 303,
    "audit_model": "claude",
    "audited_at": "2026-01-23T02:28:02.568Z",
    "risk_factors": []
  },
  "content": {
    "user_title": "Optimize Parquet Files for Performance",
    "value_statement": "Parquet files can become slow and bloated without proper optimization. This skill proactively analyzes Parquet operations and suggests improvements for compression, encoding, row group sizing, and statistics to maximize query performance and minimize storage costs.",
    "seo_keywords": [
      "Parquet optimization",
      "Parquet compression",
      "data lake optimization",
      "columnar storage",
      "Apache Parquet",
      "Claude Code",
      "Parquet performance",
      "row group tuning"
    ],
    "actual_capabilities": [
      "Analyzes Parquet writing code and suggests optimal compression settings (ZSTD, Snappy)",
      "Recommends row group sizing for efficient S3 scanning and predicate pushdown",
      "Identifies missing statistics configuration for query optimization",
      "Suggests column-specific encoding (dictionary encoding for low-cardinality columns)",
      "Detects inefficient reading patterns (full file scans, missing projection)",
      "Provides streaming recommendations for large dataset handling"
    ],
    "limitations": [
      "Focuses on Rust arrow/parquet crate patterns; other languages may need different advice",
      "Cannot modify code directly; provides suggestions for user to implement",
      "Does not analyze existing Parquet file metadata or provide file-specific tuning",
      "Does not cover Parquet schema design or data modeling best practices"
    ],
    "use_cases": [
      {
        "title": "Data Engineer Optimizing Production Pipeline",
        "description": "A data engineer is building a pipeline to write millions of records to S3 daily. The skill suggests ZSTD compression, 100MB row groups, and statistics enabled for optimal query performance.",
        "target_user": "Data engineers building ETL pipelines and data lakes"
      },
      {
        "title": "Analyst Improving Query Speed",
        "description": "An analyst notices slow Parquet queries. The skill identifies missing column projection and batch size tuning to reduce scan costs and improve response times.",
        "target_user": "Data analysts working with large Parquet datasets"
      },
      {
        "title": "Developer Debugging Performance Issues",
        "description": "A developer sees OOM errors when reading large Parquet files. The skill recommends streaming patterns, row group filtering, and memory-conscious batch sizing.",
        "target_user": "Software engineers integrating Parquet in applications"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Parquet Writing Help",
        "prompt": "I'm writing Parquet files using the Rust arrow parquet crate. What settings should I use for production?",
        "scenario": "User wants guidance on optimal Parquet writer configuration"
      },
      {
        "title": "Compression Selection",
        "prompt": "Should I use Snappy or ZSTD for my Parquet files? The data is accessed frequently for analytics queries.",
        "scenario": "User needs help choosing compression algorithm"
      },
      {
        "title": "Row Group Optimization",
        "prompt": "My Parquet queries are slow even though I have a powerful cluster. What Parquet-level settings could improve predicate pushdown?",
        "scenario": "User experiencing performance issues despite adequate compute"
      },
      {
        "title": "Large File Streaming",
        "prompt": "I need to process 50GB Parquet files without running out of memory. What patterns should I use?",
        "scenario": "User processing very large files with memory constraints"
      }
    ],
    "output_examples": [
      {
        "input": "I'm writing Parquet files with AsyncArrowWriter but didn't set any special properties. What am I missing?",
        "output": "I notice you're using AsyncArrowWriter without explicit WriterProperties. For production workloads, consider adding:\n\n- Compression: ZSTD(3) provides 3-4x compression with low CPU overhead\n- Row Group Size: 100MB uncompressed (100_000_000 rows) enables better predicate pushdown\n- Statistics: EnabledStatistics::Page allows DataFusion to skip irrelevant row groups\n\nThese settings typically improve query performance by 2-5x while reducing storage costs."
      },
      {
        "input": "My Parquet queries scan the entire file even when I only need 3 columns. How do I fix this?",
        "output": "You're missing column projection. Use ProjectionMask to read only the columns you need:\n\n```rust\nlet projection = ProjectionMask::roots(&schema, vec![0, 2, 5]);\nlet builder = ParquetRecordBatchStreamBuilder::new(reader)\n    .await?\n    .with_projection(projection);\n```\n\nThis can provide 10x+ speedup for wide tables by eliminating unnecessary column reads."
      }
    ],
    "best_practices": [
      "Use ZSTD(3) compression as the default for production data lakes - it balances compression ratio with CPU overhead",
      "Target 100MB-1GB row groups to optimize S3 scanning and predicate pushdown effectiveness",
      "Enable page-level statistics to allow query engines to skip irrelevant row groups during scans"
    ],
    "anti_patterns": [
      "Using default compression settings - always specify ZSTD or Snappy based on access patterns",
      "Writing many small Parquet files (<10MB) - this creates excessive metadata overhead and slows queries",
      "Collecting all batches into memory when reading large files - use streaming patterns instead"
    ],
    "faq": [
      {
        "question": "What compression should I use for frequently accessed data?",
        "answer": "Use Snappy for hot data. It provides fast decompression (2-3x compression) which is ideal for real-time analytics workloads where query latency matters more than storage savings."
      },
      {
        "question": "How do I choose the right row group size?",
        "answer": "Target 100MB-1GB uncompressed per row group. Smaller groups increase metadata overhead; larger groups prevent effective predicate pushdown. Start with 100_000_000 rows and adjust based on your data characteristics."
      },
      {
        "question": "What is dictionary encoding and when should I use it?",
        "answer": "Dictionary encoding stores unique values once and references them by index. Use it for low-cardinality columns (like status, category, country codes) to achieve 5-10x compression on suitable columns."
      },
      {
        "question": "Should I use Parquet 1.0 or 2.0?",
        "answer": "Use Parquet 2.0 for new projects. It supports longer strings, better type extensions, and more efficient encoding. The arrow parquet crate defaults to 2.0."
      },
      {
        "question": "How do I handle Parquet files on cloud storage with retries?",
        "answer": "Configure retry logic on your object store client. Most S3 clients support retry configuration with max_retries and timeout settings to handle transient network issues."
      },
      {
        "question": "What's the difference between streaming and collecting batches?",
        "answer": "Streaming processes batches incrementally as they arrive, keeping memory usage constant. Collecting loads all batches into memory first, which can cause OOM errors with large files. Always stream for files over a few GB."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 303
    }
  ]
}
