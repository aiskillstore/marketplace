{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-23T01:58:57.736Z",
    "slug": "emillindfors-data-lake-architect",
    "source_url": "https://github.com/EmilLindfors/claude-marketplace/tree/main/plugins/rust-data-engineering/skills/data-lake-architect",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "ac8185c03b59406bfb53cbc496984f0c8e5c516db72056641b3a9a21c8807482",
    "tree_hash": "fb1146fe83f4a58798edc8033a09ddfa03eddf05e938be61f5f455415f05d76f"
  },
  "skill": {
    "name": "data-lake-architect",
    "description": "Provides architectural guidance for data lake design including partitioning strategies, storage layout, schema design, and lakehouse patterns. Activates when users discuss data lake architecture, partitioning, or large-scale data organization.",
    "summary": "Expert guidance for data lake and lakehouse architecture including partitioning, storage layout, and schema design patterns.",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "EmilLindfors",
    "license": "MIT",
    "tags": [
      "data-architecture",
      "data-lake",
      "lakehouse",
      "partitioning",
      "parquet"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 58 static findings are false positives. This is a documentation-only skill file containing markdown guidance. The flagged 'backticks' are markdown code delimiters, 'weak cryptographic algorithm' alerts misidentify hash partitioning examples as crypto code, and 'system reconnaissance' flags are benign data distribution patterns. No executable code or security risks present.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 551,
    "audit_model": "claude",
    "audited_at": "2026-01-23T01:58:57.736Z",
    "risk_factors": []
  },
  "content": {
    "user_title": "Design scalable data lake architecture",
    "value_statement": "Data lakes require careful architectural decisions around partitioning, storage tiers, and schema design that impact performance and cost. This skill provides expert guidance on building scalable lakehouse architectures using modern patterns and best practices.",
    "seo_keywords": [
      "data lake architecture",
      "lakehouse design",
      "data partitioning",
      "parquet storage",
      "iceberg table format",
      "cloud storage",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Recommends partitioning strategies (time-based, hash, multi-dimensional) based on query patterns",
      "Designs three-tier storage architecture (raw, processed, curated layers)",
      "Guides schema design for analytical workloads including denormalization and nested structures",
      "Compares table formats (Parquet vs Iceberg) and helps select based on requirements",
      "Advises on compression, file sizing, and retention policies for cost optimization",
      "Provides Rust code examples for implementing data lake patterns"
    ],
    "limitations": [
      "Does not execute code or interact with actual data lake systems",
      "Does not generate infrastructure-as-code or deployment scripts",
      "Does not perform performance benchmarking or cost analysis on real systems",
      "Does not integrate with specific cloud providers or data platforms"
    ],
    "use_cases": [
      {
        "title": "Designing a new data lake from scratch",
        "description": "Architects building a data lake for the first time need guidance on folder structure, partitioning, and data tiers to avoid costly redesigns later.",
        "target_user": "Data architects and engineers new to data lake design"
      },
      {
        "title": "Optimizing existing data lake performance",
        "description": "Teams experiencing slow queries or high costs receive recommendations for partitioning improvements, file sizing, and compression settings.",
        "target_user": "Data engineers troubleshooting performance issues"
      },
      {
        "title": "Choosing between lakehouse formats",
        "description": "Organizations evaluating Iceberg, Delta Lake, or Hudi need clear guidance on trade-offs and selection criteria for their specific use case.",
        "target_user": "Technical leads evaluating data platform options"
      }
    ],
    "prompt_templates": [
      {
        "title": "New data lake setup",
        "prompt": "I am setting up a data lake for [use case: e.g., clickstream analytics]. We expect [volume] data per [time period] and need to support [query type] queries. What storage layout and partitioning strategy would you recommend?",
        "scenario": "Designing architecture for a new data lake from scratch"
      },
      {
        "title": "Partitioning strategy review",
        "description": "Current queries are slow because of too many small files. The issue stems from inefficient partitioning that creates unnecessary metadata overhead and fragmented storage.",
        "prompt": "Our data lake has performance issues with [specific query pattern]. We currently partition by [current strategy] and see [symptom: e.g., millions of small files, slow metadata operations]. How should we restructure our partitioning?",
        "scenario": "Troubleshooting partitioning problems in an existing data lake"
      },
      {
        "title": "Table format selection",
        "description": "Choosing the right table format is critical for schema evolution, ACID support, and operational complexity.",
        "prompt": "We need to choose a table format for [workload type]. Our requirements are: [requirement 1], [requirement 2], [requirement 3]. Should we use raw Parquet or Iceberg? What are the trade-offs?",
        "scenario": "Evaluating table format options for a new data platform"
      },
      {
        "title": "Cost optimization review",
        "description": "Storage costs escalate without proper tiering, compression, and retention policies in place.",
        "prompt": "Our data lake storage costs are [metric]. We have [volume] of [data types] spanning [time periods]. Recommend a tiered storage and retention strategy to reduce costs while maintaining query performance.",
        "scenario": "Optimizing data lake storage costs"
      }
    ],
    "output_examples": [
      {
        "input": "We have 500GB of clickstream data per day and need to support queries filtering by date and event type. How should we partition?",
        "output": "For high-volume clickstream data, I recommend Hive-style date partitioning with event_type as a secondary partition:\n\n```\nevents/\nâ”œâ”€â”€ year=2024/\nâ”‚   â”œâ”€â”€ month=01/\nâ”‚   â”‚   â”œâ”€â”€ day=15/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ event_type=click/\nâ”‚   â”‚   â”‚   â”‚   â””â”€â”€ part-*.parquet\nâ”‚   â”‚   â”‚   â”œâ”€â”€ event_type=view/\nâ”‚   â”‚   â”‚   â”‚   â””â”€â”€ part-*.parquet\nâ”‚   â”‚   â”‚   â””â”€â”€ event_type=purchase/\nâ”‚   â”‚   â”‚       â””â”€â”€ part-*.parquet\n```\n\nThis enables efficient partition pruning for your date filters while supporting event_type filtering. Target 100MB-1GB per file after compression."
      },
      {
        "input": "Should we use raw Parquet or Apache Iceberg for our new data platform?",
        "output": "Choose based on your requirements:\n\n**Use raw Parquet when:**\n- Append-only workloads\n- Stable schema\n- Simple single-writer use cases\n- Cost-sensitive (fewer metadata files)\n\n**Use Iceberg when:**\n- Schema evolves frequently\n- Need ACID transactions\n- Multiple concurrent writers\n- Updates/deletes required\n- Time travel for debugging needed\n\nBased on your mention of schema changes and multiple services writing, Iceberg provides better operational characteristics despite higher metadata overhead."
      }
    ],
    "best_practices": [
      "Use a three-tier architecture (raw, processed, curated) to separate concerns and enable reprocessing from source data",
      "Target 100MB-1GB per file and partition by query patterns, not by high-cardinality dimensions like user_id",
      "Implement tiered storage (hot/warm/cold) with appropriate compression (ZSTD levels) to balance performance and cost"
    ],
    "anti_patterns": [
      "Partitioning by high-cardinality dimensions like user_id, which creates millions of tiny partitions",
      "Skipping compression for cloud storage, which increases I/O costs and network transfer costs",
      "Using raw Parquet for workloads requiring frequent schema changes or ACID transactions"
    ],
    "faq": [
      {
        "question": "What is the difference between a data lake and a data lakehouse?",
        "answer": "A data lake stores raw data in various formats. A lakehouse adds structured table formats (like Iceberg) on top, providing ACID transactions, schema enforcement, and time travel while maintaining the flexibility of a data lake."
      },
      {
        "question": "How many partitions should my data lake have?",
        "answer": "Target fewer than 10,000 partitions for good performance. Signs of too many partitions include slow metadata operations, many empty partitions, and files smaller than 10MB. Reduce granularity or implement compaction if exceeded."
      },
      {
        "question": "What compression format should I use for my data lake?",
        "answer": "ZSTD is recommended for most use cases. Use ZSTD(3) for hot data (fast compression/decompression), ZSTD(6) for warm data (balanced), and ZSTD(9) for cold archival data (maximum compression)."
      },
      {
        "question": "How do I handle schema evolution in my data lake?",
        "answer": "Use Iceberg for native schema evolution support (add, rename, delete columns safely). For Parquet, make new fields optional so old readers ignore them. Avoid breaking changes by versioning schemas when possible."
      },
      {
        "question": "Should I partition by date or use another strategy?",
        "answer": "Time-series data (events, logs, metrics) should use date partitioning. For data without natural time dimensions, consider hash partitioning to ensure even distribution. Multi-dimensional partitioning works when queries filter on consistent low-cardinality dimensions."
      },
      {
        "question": "What file size should I target for my Parquet files?",
        "answer": "Target 100MB-1GB per file (compressed). Smaller files cause excessive metadata overhead and slow queries. Larger files reduce parallelism. Implement compaction jobs to merge small files from streaming ingestion."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 551
    }
  ]
}
