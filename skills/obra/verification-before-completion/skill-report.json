{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T18:37:25.348Z",
    "slug": "obra-verification-before-completion",
    "source_url": "https://github.com/obra/superpowers/tree/main/skills/verification-before-completion",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "963f6b63d58d88e029181126528a1c720f76c4f8cfaec3532ece14795aef5b77",
    "tree_hash": "ba2b31349bf06828ca4332affb98907624f4056f9cb0fe5d2ad932cf9ea466aa"
  },
  "skill": {
    "name": "verification-before-completion",
    "description": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always",
    "summary": "Require verification evidence before claiming completion, passing, or success",
    "icon": "âœ“",
    "version": "1.0.0",
    "author": "obra",
    "license": "MIT",
    "category": "security",
    "tags": [
      "verification",
      "quality",
      "trust",
      "best-practices",
      "testing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill contains ONLY behavioral instruction documentation (SKILL.md) with no executable code. All 48 static findings are false positives: markdown code block delimiters were misidentified as Ruby/shell backticks, YAML frontmatter field names were flagged as cryptographic content, and verification guidelines were misclassified as reconnaissance patterns. No external commands, network operations, filesystem access, or code execution capabilities exist. This is purely documentation that instructs AI agents to run verification commands themselves.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 896,
    "audit_model": "claude",
    "audited_at": "2026-01-21T18:37:25.348Z",
    "risk_factors": [],
    "risk_factor_evidence": []
  },
  "content": {
    "user_title": "Verify before claiming completion",
    "value_statement": "AI agents often claim work is complete without actual verification. This skill enforces evidence-based completion claims by requiring verification command output before any success assertions. It prevents false completion reports and builds trust through proof.",
    "seo_keywords": [
      "verification before completion",
      "AI agent quality control",
      "completion claims verification",
      "evidence-based reporting",
      "trust in AI agents",
      "Claude Code best practices",
      "testing verification",
      "code review guidelines",
      "agent honesty",
      "verification workflow"
    ],
    "actual_capabilities": [
      "Prevents claims of completion without running verification commands",
      "Requires actual test output showing 0 failures before claiming tests pass",
      "Blocks expressions of satisfaction like 'Great!' or 'Done!' before verification",
      "Verifies agent delegation results independently before trusting reports",
      "Enforces red-green cycles for regression test verification"
    ],
    "limitations": [
      "Requires the agent to have access to verification commands and their output",
      "Does not execute verification itself - instructs the agent to run commands",
      "Cannot prevent users from ignoring verification results"
    ],
    "use_cases": [
      {
        "title": "Prevent false completion reports",
        "description": "Stop AI agents from claiming tests pass or bugs fixed without running the actual verification commands.",
        "target_user": "Development teams using AI coding assistants"
      },
      {
        "title": "Build trust in AI contributions",
        "description": "Ensure AI agents provide verification evidence before PRs, reducing rework from false completion claims.",
        "target_user": "Code reviewers"
      },
      {
        "title": "Enforce verification discipline",
        "description": "Apply consistent verification standards across all AI agent work before accepting completion.",
        "target_user": "Quality assurance teams"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic verification",
        "scenario": "Agent claims work is complete",
        "prompt": "Before claiming this work is complete: 1) What command proves completion? 2) Run it now. 3) Show the output. 4) Only then state the status with evidence."
      },
      {
        "title": "Test verification",
        "scenario": "Agent says tests should pass",
        "prompt": "Do not claim tests pass. Run the test command now. Report the actual output including failure counts. Show exit code if available."
      },
      {
        "title": "Build verification",
        "scenario": "Agent claims build succeeds",
        "prompt": "Run the build command and show the full output. Do not rely on linter passing or logs looking good. Report actual build status."
      },
      {
        "title": "Agent delegation",
        "scenario": "Sub-agent reports success",
        "prompt": "The agent reports success. Before trusting this: Check the VCS diff for actual changes. Run relevant verification commands. Report what you actually verified."
      }
    ],
    "output_examples": [
      {
        "input": "The tests should pass now",
        "output": [
          "Before claiming tests pass, I need to run verification.",
          "Running: npm test",
          "Output: 34/34 tests pass, 0 failures",
          "Result: All tests pass with verified output"
        ]
      },
      {
        "input": "The bug is fixed",
        "output": [
          "Before claiming the bug is fixed:",
          "Tested the original symptom scenario",
          "Result: Bug symptom no longer occurs",
          "Verified with reproduction steps - issue resolved"
        ]
      },
      {
        "input": "Build succeeded",
        "output": [
          "Running: ./build.sh",
          "Exit code: 0",
          "Build completed successfully",
          "Compilation verified - no errors"
        ]
      }
    ],
    "best_practices": [
      "Always run verification commands fresh - never trust previous runs or cached results",
      "Report actual output including failure counts and exit codes, not interpretations",
      "Apply verification to agent delegation - check VCS diffs and run independent verification"
    ],
    "anti_patterns": [
      "Using words like 'should', 'probably', or 'seems to' instead of verified facts",
      "Expressing satisfaction ('Great!', 'Perfect!', 'Done!') before running verification",
      "Trusting agent success reports without independent verification of actual results"
    ],
    "faq": [
      {
        "question": "Does this skill execute verification commands?",
        "answer": "No. This skill instructs the agent to run verification commands and requires evidence before claims. The agent must execute commands."
      },
      {
        "question": "Can I skip verification for simple tasks?",
        "answer": "No. The rule applies to all completion claims without exception, regardless of task complexity."
      },
      {
        "question": "What if verification commands are unavailable?",
        "answer": "State that verification could not be performed and do not claim completion. Report the actual status with this limitation."
      },
      {
        "question": "Does this work with agent delegation?",
        "answer": "Yes. When a sub-agent reports success, verify independently by checking VCS diffs and running verification commands."
      },
      {
        "question": "How does this handle partial verification?",
        "answer": "Partial verification proves nothing. Full verification of the specific claim is required before any completion assertion."
      },
      {
        "question": "What about re-running recent verification?",
        "answer": "Run verification fresh each time. Previous results, even from minutes ago, do not satisfy the verification requirement."
      }
    ]
  },
  "file_structure": [
    {
      "name": "EVALUATION.json",
      "type": "file",
      "path": "EVALUATION.json",
      "lines": 251
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 140
    }
  ]
}
