{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T16:35:24.362Z",
    "slug": "asmayaseen-streaming-llm-responses",
    "source_url": "https://github.com/Asmayaseen/hackathon-2/tree/main/.claude/skills/streaming-llm-responses",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "bca971a64e4da9a89a3a0e6ccd7dfd34afce5df2a96f6e17bac8ff52f35fe9e3",
    "tree_hash": "2a3fd0a7cb338520bed79642b30a9673588c1c2b9654b90ff1d7e1df48f6b6ae"
  },
  "skill": {
    "name": "streaming-llm-responses",
    "description": "Implement real-time streaming UI patterns for AI chat applications. Use when adding response\nlifecycle handlers, progress indicators, client effects, or thread state synchronization.\nCovers onResponseStart/End, onEffect, ProgressUpdateEvent, and client tools.\nNOT when building basic chat without real-time feedback.\n",
    "summary": "Implement real-time streaming UI patterns for AI chat applications. Use when adding response\nlifecyc...",
    "icon": "ðŸ“¡",
    "version": "1.0.0",
    "author": "Asmayaseen",
    "license": "UNLICENSED",
    "category": "coding",
    "tags": [
      "streaming",
      "chat-ui",
      "real-time",
      "react",
      "typescript"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation skill containing only markdown files with code examples. The only executable code is scripts/verify.py which performs a simple file-existence check. All 73 static findings are FALSE POSITIVES: the analyzer misidentified markdown code block delimiters as Ruby backticks, HTTP imports as network calls, and metadata URLs as hardcoded endpoints. No network calls, file writes, credential access, or command execution exists.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/verify.py",
            "line_start": 1,
            "line_end": 21
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 780,
    "audit_model": "claude",
    "audited_at": "2026-01-16T16:35:24.362Z"
  },
  "content": {
    "user_title": "Stream real-time AI chat responses",
    "value_statement": "Users struggle to build responsive chat interfaces that update in real-time as AI responses stream token by token. This skill provides code patterns for lifecycle handlers, progress indicators, and client-side effects to create smooth streaming experiences.",
    "seo_keywords": [
      "Claude streaming responses",
      "AI chat UI patterns",
      "real-time chat development",
      "streaming LLM interface",
      "ChatKit implementation",
      "Claude Code chat",
      "progress indicators AI",
      "client effects streaming",
      "onResponseStart handler",
      "streaming UI patterns"
    ],
    "actual_capabilities": [
      "Implement response lifecycle handlers (onResponseStart, onResponseEnd)",
      "Add real-time progress update events during AI processing",
      "Send fire-and-forget client effects from server to UI",
      "Define client tools for AI to query browser state",
      "Handle thread lifecycle events for persistence",
      "Lock UI during streaming to prevent race conditions"
    ],
    "limitations": [
      "Does not include backend server implementation",
      "Requires a compatible streaming library like useChatKit",
      "Does not handle authentication or user management",
      "Frontend code examples are TypeScript/React specific"
    ],
    "use_cases": [
      {
        "target_user": "Frontend developers",
        "title": "Build streaming chat UI",
        "description": "Create responsive chat interfaces with progress indicators and live updates as AI responses stream."
      },
      {
        "target_user": "AI application architects",
        "title": "Design real-time feedback",
        "description": "Architect communication patterns between AI agents and client UIs using effects and client tools."
      },
      {
        "target_user": "Full-stack engineers",
        "title": "Sync thread state",
        "description": "Implement thread lifecycle management with client and server coordination for persistence."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic streaming setup",
        "scenario": "Getting started with streaming responses",
        "prompt": "Show me how to set up streaming response handlers in a React chat component. Include onResponseStart and onResponseEnd callbacks."
      },
      {
        "title": "Progress indicators",
        "scenario": "Adding progress during long operations",
        "prompt": "How do I use ProgressUpdateEvent to show status messages during AI processing? Give a Python backend example."
      },
      {
        "title": "Client effects",
        "scenario": "Updating UI from server",
        "prompt": "Explain client effects vs client tools. Show how to send a fire-and-forget effect from the server to update the UI."
      },
      {
        "title": "Client tools",
        "scenario": "AI querying browser state",
        "prompt": "How can the AI read client-side state like selected items or map viewport? Show the full pattern for client tools."
      }
    ],
    "output_examples": [
      {
        "input": "How do I lock the UI while the AI is responding and show a loading indicator?",
        "output": [
          "Use onResponseStart to lock UI and show loading state",
          "Use onResponseEnd to unlock UI and hide loading indicator",
          "Always handle onError to unlock UI on failures",
          "Example: setIsResponding(true) + lockInteraction() in onResponseStart"
        ]
      },
      {
        "input": "What is the difference between client effects and client tools?",
        "output": [
          "Client effects: server-to-client fire-and-forget messages that update UI",
          "Client tools: server-to-client-to-server requests that return values",
          "Effects are one-way; tools are two-way with expected return values",
          "Use effects for notifications, tools for state queries"
        ]
      }
    ],
    "best_practices": [
      "Always unlock the UI in error handlers to prevent frozen interfaces",
      "Keep effect handlers lightweight and fire-and-forget for performance",
      "Use requestAnimationFrame for DOM updates in onEffect callbacks"
    ],
    "anti_patterns": [
      "Not locking UI during response leads to race conditions and double-submissions",
      "Blocking in effects causes streaming to pause waiting for UI updates",
      "Heavy computation in onEffect handlers degrades streaming performance"
    ],
    "faq": [
      {
        "question": "What frameworks does this skill support?",
        "answer": "The examples use React with TypeScript. Patterns apply to any frontend framework with equivalent state management."
      },
      {
        "question": "What is the maximum streaming message length?",
        "answer": "Message length limits depend on your backend API. The skill focuses on UI patterns, not backend constraints."
      },
      {
        "question": "How do I integrate with existing chat components?",
        "answer": "Add lifecycle handlers to your existing chat component configuration. Effects and tools integrate alongside current handlers."
      },
      {
        "question": "Is user data sent to external servers?",
        "answer": "The skill provides patterns only. Data handling depends on your API configuration. No data leaves your configured endpoints."
      },
      {
        "question": "Why is my progress indicator not showing?",
        "answer": "Ensure ProgressUpdateEvent is yielded during async processing, not just at the start and end. Backend must emit events."
      },
      {
        "question": "How does this compare to native streaming APIs?",
        "answer": "This skill provides higher-level patterns. Native streaming (Server-Sent Events) handles transport; these patterns handle UI state."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "streaming-patterns.md",
          "type": "file",
          "path": "references/streaming-patterns.md",
          "lines": 210
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "verify.py",
          "type": "file",
          "path": "scripts/verify.py",
          "lines": 21
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 337
    }
  ]
}
