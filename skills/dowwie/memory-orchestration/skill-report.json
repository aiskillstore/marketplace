{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T04:33:00.683Z",
    "slug": "dowwie-memory-orchestration",
    "source_url": "https://github.com/Dowwie/agent_framework_study/tree/main/.claude/skills/memory-orchestration",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "9a7e43f57652dd57a71b00b03cf306e43cee9f08021f210b50db57563ad9e73f",
    "tree_hash": "82d1acec3fe0e298b38143d4c105a7f445a337de6c080e3854c12a6a7f50fc44"
  },
  "skill": {
    "name": "memory-orchestration",
    "description": "Analyze context management, memory systems, and state continuity in agent frameworks. Use when (1) understanding how prompts are assembled, (2) evaluating eviction policies for context overflow, (3) mapping memory tiers (short-term/long-term), (4) analyzing token budget management, or (5) comparing context strategies across frameworks.",
    "summary": "Analyze context management, memory systems, and state continuity in agent frameworks. Use when (1) u...",
    "icon": "ðŸ§ ",
    "version": "1.0.0",
    "author": "Dowwie",
    "license": "MIT",
    "category": "research",
    "tags": [
      "context",
      "memory",
      "token-budget",
      "state-management"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation skill containing only educational content about memory orchestration concepts. All 49 static findings are false positives: the analyzer misinterpreted memory management code snippets as cryptographic operations and markdown backticks as shell execution. No executable code, file access, network calls, or command execution capabilities exist.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 21,
            "line_end": 49
          },
          {
            "file": "SKILL.md",
            "line_start": 49,
            "line_end": 54
          },
          {
            "file": "SKILL.md",
            "line_start": 54,
            "line_end": 74
          },
          {
            "file": "SKILL.md",
            "line_start": 74,
            "line_end": 77
          },
          {
            "file": "SKILL.md",
            "line_start": 77,
            "line_end": 83
          },
          {
            "file": "SKILL.md",
            "line_start": 83,
            "line_end": 86
          },
          {
            "file": "SKILL.md",
            "line_start": 86,
            "line_end": 95
          },
          {
            "file": "SKILL.md",
            "line_start": 95,
            "line_end": 101
          },
          {
            "file": "SKILL.md",
            "line_start": 101,
            "line_end": 105
          },
          {
            "file": "SKILL.md",
            "line_start": 105,
            "line_end": 112
          },
          {
            "file": "SKILL.md",
            "line_start": 112,
            "line_end": 123
          },
          {
            "file": "SKILL.md",
            "line_start": 123,
            "line_end": 130
          },
          {
            "file": "SKILL.md",
            "line_start": 130,
            "line_end": 144
          },
          {
            "file": "SKILL.md",
            "line_start": 144,
            "line_end": 151
          },
          {
            "file": "SKILL.md",
            "line_start": 151,
            "line_end": 162
          },
          {
            "file": "SKILL.md",
            "line_start": 162,
            "line_end": 169
          },
          {
            "file": "SKILL.md",
            "line_start": 169,
            "line_end": 188
          },
          {
            "file": "SKILL.md",
            "line_start": 188,
            "line_end": 195
          },
          {
            "file": "SKILL.md",
            "line_start": 195,
            "line_end": 217
          },
          {
            "file": "SKILL.md",
            "line_start": 217,
            "line_end": 221
          },
          {
            "file": "SKILL.md",
            "line_start": 221,
            "line_end": 235
          },
          {
            "file": "SKILL.md",
            "line_start": 235,
            "line_end": 243
          },
          {
            "file": "SKILL.md",
            "line_start": 243,
            "line_end": 244
          },
          {
            "file": "SKILL.md",
            "line_start": 244,
            "line_end": 250
          },
          {
            "file": "SKILL.md",
            "line_start": 250,
            "line_end": 265
          },
          {
            "file": "SKILL.md",
            "line_start": 265,
            "line_end": 269
          },
          {
            "file": "SKILL.md",
            "line_start": 269,
            "line_end": 275
          },
          {
            "file": "SKILL.md",
            "line_start": 275,
            "line_end": 280
          },
          {
            "file": "SKILL.md",
            "line_start": 280,
            "line_end": 294
          },
          {
            "file": "SKILL.md",
            "line_start": 294,
            "line_end": 298
          },
          {
            "file": "SKILL.md",
            "line_start": 298,
            "line_end": 299
          },
          {
            "file": "SKILL.md",
            "line_start": 299,
            "line_end": 300
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 474,
    "audit_model": "claude",
    "audited_at": "2026-01-17T04:33:00.683Z"
  },
  "content": {
    "user_title": "Analyze context memory systems in agent frameworks",
    "value_statement": "Agent frameworks struggle with context overflow and memory management. This skill provides systematic analysis of context assembly, eviction policies, and token budgeting strategies across different implementations.",
    "seo_keywords": [
      "memory orchestration",
      "context management",
      "agent frameworks",
      "token budget",
      "Claude",
      "Claude Code",
      "context overflow",
      "memory tiers",
      "eviction policies",
      "prompt assembly"
    ],
    "actual_capabilities": [
      "Trace how prompts are assembled from system prompts, context, tools, history, and user input",
      "Identify and evaluate eviction policies including FIFO, sliding window, summarization, and vector store swapping",
      "Map memory tiers from working memory to persistent database storage",
      "Analyze token counting strategies and budget allocation approaches"
    ],
    "limitations": [
      "Does not modify or execute code in target frameworks",
      "Requires codebase-mapping skill to identify memory-related files first",
      "Cannot access live running systems or their memory states"
    ],
    "use_cases": [
      {
        "target_user": "AI Researchers",
        "title": "Compare memory strategies",
        "description": "Evaluate how different agent frameworks handle context overflow and memory persistence"
      },
      {
        "target_user": "Framework Developers",
        "title": "Design memory systems",
        "description": "Learn patterns for implementing token budgeting and memory tier management"
      },
      {
        "target_user": "Technical Auditors",
        "title": "Audit context handling",
        "description": "Analyze prompt assembly order and memory tier mapping in existing implementations"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Framework Analysis",
        "scenario": "Analyze memory system",
        "prompt": "Analyze the memory orchestration patterns in [framework/repository]. Identify the context assembly order, eviction policy used, and memory tier structure."
      },
      {
        "title": "Compare Eviction Policies",
        "scenario": "Compare memory strategies",
        "prompt": "Compare the eviction policies across [framework1] and [framework2]. Which approach is more effective for long conversations?"
      },
      {
        "title": "Map Memory Tiers",
        "scenario": "Map tier architecture",
        "prompt": "Map the memory tiers in [framework]. How does data move from working memory to persistent storage? What triggers tier promotions?"
      },
      {
        "title": "Token Budget Analysis",
        "scenario": "Analyze token allocation",
        "prompt": "Analyze the token management approach in [framework]. How are budgets allocated across system, tools, history, and input? What happens on overflow?"
      }
    ],
    "output_examples": [
      {
        "input": "Analyze the memory orchestration in the agent framework at /src/agents",
        "output": [
          "Context Assembly: System â†’ Memory â†’ Tools â†’ History â†’ Input",
          "Eviction Policy: Sliding window with 2000 token limit",
          "Memory Tiers: Working (prompt), Session (RAM), Persistent (ChromaDB)",
          "Token Counting: Uses tiktoken for exact counting"
        ]
      },
      {
        "input": "Compare memory strategies between LangChain and AutoGPT",
        "output": [
          "LangChain: Message history with token-aware truncation",
          "AutoGPT: Persistent memory with vector store retrieval",
          "Key Difference: LangChain favors recency, AutoGPT favors semantic relevance",
          "Trade-off: Latency vs context preservation"
        ]
      },
      {
        "input": "Map memory tiers in our custom agent implementation",
        "output": [
          "Tier 1 (Working): In-prompt context window (~8K tokens)",
          "Tier 2 (Session): Redis cache for full history",
          "Tier 3 (Persistent): PostgreSQL with vector embeddings",
          "Promotion Trigger: Session end or explicit save"
        ]
      }
    ],
    "best_practices": [
      "Always run codebase-mapping first to identify memory-related files before detailed analysis",
      "Use the output template to standardize analysis reports across different frameworks",
      "Combine with control-loop-extraction to understand scratchpad memory usage patterns"
    ],
    "anti_patterns": [
      "Analyzing without first identifying memory files (use codebase-mapping skill)",
      "Assuming all frameworks use the same context assembly order or eviction policy",
      "Ignoring token counting accuracy differences between estimation methods"
    ],
    "faq": [
      {
        "question": "Which AI tools support this skill?",
        "answer": "This skill works with Claude, Codex, and Claude Code. Each tool has different context window limits that affect memory strategy analysis."
      },
      {
        "question": "What are the context window limits?",
        "answer": "Context limits vary by model. Claude Code supports up to 200K tokens. Smaller windows require more aggressive eviction policies."
      },
      {
        "question": "How does this integrate with other skills?",
        "answer": "Run codebase-mapping first to find memory files. Feed findings into comparative-matrix for strategy comparisons. Combine with control-loop-extraction for scratchpad analysis."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "Yes. This skill only reads and analyzes code locally. No data is sent externally or persisted beyond the analysis session."
      },
      {
        "question": "Why does memory management matter?",
        "answer": "Poor memory handling causes context loss, repeated instructions, and degraded performance in long conversations. Understanding patterns helps build more capable agents."
      },
      {
        "question": "How is this different from RAG?",
        "answer": "RAG retrieves external documents. Memory orchestration manages conversation context and state continuity within the agent processing loop."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 301
    }
  ]
}
