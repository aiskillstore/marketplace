{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-17T01:52:37.197Z",
    "slug": "dcjanus-fetch-url",
    "source_url": "https://github.com/DCjanus/prompts/tree/master/skills/fetch-url",
    "source_ref": "master",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "80536e9611e9ec0234ebbd2b676723ae86e944c8c1b552ee83aab0d4daeff77d",
    "tree_hash": "e3fe57ca613e135db58884519a7aa05acb4e9ff7b2f21a5b6953f98ba31750ea"
  },
  "skill": {
    "name": "fetch-url",
    "description": "Ê∏≤ÊüìÁΩëÈ°µ URLÔºåÂéªÂô™ÊèêÂèñÊ≠£ÊñáÂπ∂ËæìÂá∫‰∏∫ MarkdownÔºàÈªòËÆ§ÔºâÊàñÂÖ∂‰ªñÊ†ºÂºè/ÂéüÂßã HTMLÔºå‰ª•ÂáèÂ∞ë Token„ÄÇ",
    "summary": "Ê∏≤ÊüìÁΩëÈ°µ URLÔºåÂéªÂô™ÊèêÂèñÊ≠£ÊñáÂπ∂ËæìÂá∫‰∏∫ MarkdownÔºàÈªòËÆ§ÔºâÊàñÂÖ∂‰ªñÊ†ºÂºè/ÂéüÂßã HTMLÔºå‰ª•ÂáèÂ∞ë Token„ÄÇ",
    "icon": "üîó",
    "version": "1.0.0",
    "author": "DCjanus",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "web",
      "content extraction",
      "markdown",
      "documentation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a legitimate web scraping skill that renders URLs using Playwright and extracts content via trafilatura. All detected patterns are consistent with the stated purpose. The network requests (page.goto) are core functionality for web scraping. The browser path detection (lines 33-52) is a performance optimization, not reconnaissance. All 10 HIGH severity 'weak cryptographic algorithm' findings are false positives - no cryptographic code exists in the codebase. The MEDIUM 'external_commands' findings are documentation examples, not actual command execution. Input validation (line 104) restricts URLs to http/https only.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 1,
            "line_end": 146
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 56,
            "line_end": 70
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 137,
            "line_end": 138
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 59,
            "line_end": 63
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 411,
    "audit_model": "claude",
    "audited_at": "2026-01-17T01:52:37.197Z"
  },
  "content": {
    "user_title": "Fetch and render web page content",
    "value_statement": "Web pages contain heavy HTML that wastes tokens. This skill renders URLs with a headless browser, extracts clean text, and outputs as Markdown or other formats. Perfect for turning documentation and articles into efficient AI input.",
    "seo_keywords": [
      "fetch url",
      "web scraping",
      "markdown converter",
      "content extraction",
      "Claude",
      "Codex",
      "Claude Code",
      "token reduction",
      "HTML to Markdown",
      "web page renderer"
    ],
    "actual_capabilities": [
      "Renders URLs using headless Chromium browser via Playwright",
      "Extracts clean text content using trafilatura library",
      "Outputs in 8 formats: markdown, HTML, JSON, CSV, TXT, XML, XML-TEI, raw HTML",
      "Auto-detects local browser installations to avoid downloads",
      "Configurable timeout and browser path options",
      "Writes output directly to file or stdout"
    ],
    "limitations": [
      "Only supports http and https URL schemes",
      "Requires Chromium-based browser (local or Playwright-installed)",
      "Cannot render pages requiring authentication or login",
      "JavaScript-heavy pages may have reduced content"
    ],
    "use_cases": [
      {
        "target_user": "Documentation writers",
        "title": "Convert online docs",
        "description": "Turn web documentation into local Markdown files for offline access and cleaner editing."
      },
      {
        "target_user": "AI practitioners",
        "title": "Optimize AI context",
        "description": "Convert articles and guides to Markdown to maximize useful content within token limits."
      },
      {
        "target_user": "Researchers",
        "title": "Archive web content",
        "description": "Extract and save content from multiple URLs in structured formats for analysis."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic fetch",
        "scenario": "Get page content",
        "prompt": "Use fetch-url to render https://example.com/docs/getting-started and output as markdown"
      },
      {
        "title": "Save to file",
        "scenario": "Archive page content",
        "prompt": "Use fetch-url to extract https://api.example.com/docs --output ./api-docs.md --output-format markdown"
      },
      {
        "title": "Custom format",
        "scenario": "Get raw HTML",
        "prompt": "Use fetch-url to fetch https://blog.example.com/post --output-format raw-html --timeout-ms 30000"
      },
      {
        "title": "Multiple pages",
        "scenario": "Batch extraction",
        "prompt": "Extract these three URLs as JSON: docs page, API reference, and changelog. Save each to output files."
      }
    ],
    "output_examples": [
      {
        "input": "Fetch the Python documentation homepage as markdown",
        "output": [
          "# Python Documentation",
          "",
          "## Getting Started",
          "",
          "Python is a programming language that lets you work quickly...",
          "[Saved output to stdout - 2.3KB of Markdown content]",
          "Extracted 15 headings, 84 paragraphs, and 23 links from https://docs.python.org/3/"
        ]
      },
      {
        "input": "Convert a blog post to structured JSON",
        "output": [
          "{",
          "  \"title\": \"My Blog Post\",",
          "  \"author\": \"Jane Smith\",",
          "  \"date\": \"2024-01-15\",",
          "  \"content\": \"The article body goes here...\"",
          "}",
          "[Saved to output file]",
          "Extracted structured data from https://blog.example.com/post"
        ]
      },
      {
        "input": "Extract API documentation to text file",
        "output": [
          "API Reference Documentation",
          "===========================",
          "",
          "## Authentication",
          "",
          "All API requests require...",
          "[Saved to ./api-reference.txt - 15KB]",
          "Extracted 42 endpoints, 156 parameters, and 89 examples"
        ]
      }
    ],
    "best_practices": [
      "Use --timeout-ms flag for slow-loading pages to avoid hanging",
      "Specify --browser-path if you have Chrome/Edge installed locally",
      "Use --output-format raw-html when you need unmodified HTML"
    ],
    "anti_patterns": [
      "Do not try to fetch local file URLs (file:// scheme not supported)",
      "Avoid fetching pages behind login walls without proper authentication",
      "Do not use extremely long timeouts for unresponsive servers"
    ],
    "faq": [
      {
        "question": "Which browsers are supported?",
        "answer": "Any Chromium-based browser: Chrome, Edge, Brave, Chromium, or Playwright's bundled browser."
      },
      {
        "question": "What happens to JavaScript content?",
        "answer": "Playwright renders JavaScript, but dynamically loaded content may be missing if it loads after networkidle."
      },
      {
        "question": "How is this different from curl or wget?",
        "answer": "This tool uses a real browser to render JavaScript and extracts clean text, not raw HTML source."
      },
      {
        "question": "Is my browsing data saved?",
        "answer": "No persistent data. Each run creates a fresh browser context that is closed after extraction."
      },
      {
        "question": "Why does it need a browser?",
        "answer": "Modern web pages use JavaScript to load content. A browser renderer captures the final rendered page."
      },
      {
        "question": "Can this access authenticated content?",
        "answer": "Not directly. The tool runs headless without login credentials. Authenticated content requires separate handling."
      }
    ]
  },
  "file_structure": [
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "fetch_url.py",
          "type": "file",
          "path": "scripts/fetch_url.py",
          "lines": 146
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 28
    }
  ]
}
