{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-21T15:33:37.355Z",
    "slug": "ancplua-epistemic-checkpoint",
    "source_url": "https://github.com/ANcpLua/ancplua-claude-plugins/tree/main/plugins/metacognitive-guard/skills/epistemic-checkpoint",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "46caada05b6102ac056c990555dd845b6f688820e9319d3984a20f5b6e098f62",
    "tree_hash": "2285754bdb730d9f8c4c25e95bedbd6794799d4311520fc9a750a27bee516494"
  },
  "skill": {
    "name": "epistemic-checkpoint",
    "description": "Force verification before answering questions involving versions, dates, status, or \"current\" state. Prevents hallucinations at the reasoning level by checking assertions.yaml and WebSearch before forming beliefs. Triggers on software versions, release status, dates, and package versions.",
    "summary": "Metacognitive guard that prevents AI hallucinations by forcing verification of factual claims about software versions, release status, and dates before forming conclusions.",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "ANcpLua",
    "license": "MIT",
    "tags": [
      "hallucination-prevention",
      "fact-verification",
      "metacognition",
      "software-versions",
      "accuracy"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "medium",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All 21 static findings are false positives. The skill is a legitimate metacognitive guard for hallucination prevention. Network access (WebSearch) is controlled and intentional for fact verification. All \"external_commands\" detections are documentation examples in markdown, not actual shell execution.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 37,
            "line_end": 40
          },
          {
            "file": "SKILL.md",
            "line_start": 40,
            "line_end": 46
          },
          {
            "file": "SKILL.md",
            "line_start": 46,
            "line_end": 49
          },
          {
            "file": "SKILL.md",
            "line_start": 49,
            "line_end": 79
          },
          {
            "file": "SKILL.md",
            "line_start": 79,
            "line_end": 81
          },
          {
            "file": "SKILL.md",
            "line_start": 81,
            "line_end": 85
          },
          {
            "file": "SKILL.md",
            "line_start": 85,
            "line_end": 89
          },
          {
            "file": "SKILL.md",
            "line_start": 89,
            "line_end": 93
          },
          {
            "file": "SKILL.md",
            "line_start": 93,
            "line_end": 95
          },
          {
            "file": "SKILL.md",
            "line_start": 95,
            "line_end": 99
          },
          {
            "file": "SKILL.md",
            "line_start": 99,
            "line_end": 103
          },
          {
            "file": "SKILL.md",
            "line_start": 103,
            "line_end": 118
          },
          {
            "file": "SKILL.md",
            "line_start": 118,
            "line_end": 127
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 420,
    "audit_model": "claude",
    "audited_at": "2026-01-21T15:33:37.355Z",
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "content": {
    "user_title": "Verify facts before answering",
    "value_statement": "AI assistants often produce incorrect answers about software versions, release dates, and current states because their training data is stale. This skill forces verification at the reasoning level using assertions.yaml and WebSearch, preventing the root cause of hallucinations.",
    "seo_keywords": [
      "Claude",
      "Codex",
      "Claude Code",
      "hallucination prevention",
      "fact verification",
      "software version check",
      "accuracy",
      "metacognitive",
      "release status",
      "version validation"
    ],
    "actual_capabilities": [
      "Detects questions about software versions, release status, dates, and current state",
      "Checks local assertions.yaml for verified ground truth before answering",
      "Uses WebSearch to verify facts from official documentation sources",
      "Forces verification before forming beliefs or making claims",
      "Provides structured verification output with confidence levels",
      "Prevents reasoning-level hallucinations by requiring source verification"
    ],
    "limitations": [
      "Only triggers on configured topics (software versions, dates, release status)",
      "Depends on assertions.yaml being populated with known values",
      "WebSearch results depend on source availability and freshness",
      "Does not verify user-provided claims without additional verification"
    ],
    "use_cases": [
      {
        "title": "Software version verification",
        "description": "When asked about .NET, React, Node, Python, or other software versions, this skill forces verification of the actual current version and release status before answering.",
        "target_user": "Developers working with multiple software ecosystems"
      },
      {
        "title": "Release status checking",
        "description": "Before claiming something is preview, LTS, GA, deprecated, or in beta, the skill verifies the actual release status from official sources.",
        "target_user": "Technical writers and architects"
      },
      {
        "title": "Date-sensitive fact validation",
        "description": "When answering questions that involve dates after the AI training cutoff, this skill ensures facts are verified against current information.",
        "target_user": "Researchers and support engineers"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic version check",
        "prompt": "What is the current LTS version of Node.js and when was it released?",
        "scenario": "Asking about current software versions"
      },
      {
        "title": "Release status verification",
        "prompt": "Is React 19 still in beta or has it been released as stable?",
        "scenario": "Checking release status of a software package"
      },
      {
        "title": "Multiple version comparison",
        "prompt": "Compare the latest Python 3.x version with the previous release. What are the key differences?",
        "scenario": "Comparing multiple software versions"
      },
      {
        "title": "Deprecation checking",
        "prompt": "Is the use of the old authentication method deprecated? What should we use instead?",
        "scenario": "Verifying API or feature deprecation status"
      }
    ],
    "output_examples": [
      {
        "input": "What is the latest .NET version?",
        "output": "Epistemic Checkpoint\n\nClaim to verify: '.NET 10 is still in preview'\nSource checked: WebSearch\nVerified fact: '.NET 10 LTS was released November 2025 and is now the stable, supported version'\nConfidence: high\n\nProceeding with verified baseline..."
      },
      {
        "input": "Is Python 3.12 still the latest version?",
        "output": "Epistemic Checkpoint\n\nClaim to verify: 'Python 3.12 is the latest version'\nSource checked: assertions.yaml and WebSearch\nVerified fact: 'Python 3.13 was released October 2024 and is now the current stable version'\nConfidence: high\n\nProceeding with verified baseline..."
      },
      {
        "input": "When was React 19 released?",
        "output": "Epistemic Checkpoint\n\nClaim to verify: 'React 19 beta release date'\nSource checked: WebSearch\nVerified fact: 'React 19 was released as stable in December 2024'\nConfidence: high\n\nProceeding with verified baseline..."
      }
    ],
    "best_practices": [
      "Enable this skill for projects that span multiple software ecosystems to catch version-related hallucinations",
      "Populate assertions.yaml with known versions and facts specific to your project",
      "Trust the verification process even when you feel confident about your knowledge"
    ],
    "anti_patterns": [
      "Assuming your training knowledge is current for version and release questions",
      "Accepting user-provided version information without independent verification",
      "Skipping verification for 'simple factual questions'"
    ],
    "faq": [
      {
        "question": "What triggers this skill to activate?",
        "answer": "The skill activates when questions involve software versions, release status, 'current' or 'latest' references, dates after training cutoff, package versions, or API deprecations."
      },
      {
        "question": "What is assertions.yaml and why is it important?",
        "answer": "assertions.yaml is a local file containing verified ground truth about your project's dependencies and configurations. The skill checks this file first before using WebSearch."
      },
      {
        "question": "Does this skill make network requests?",
        "answer": "Yes, the skill uses WebSearch to verify facts from official documentation sources when the information is not found in assertions.yaml. This is intentional for accuracy."
      },
      {
        "question": "What sources does WebSearch use?",
        "answer": "The skill prefers official sources like microsoft.com/dotnet for .NET, nodejs.org for Node, python.org for Python, and similar authoritative documentation sites."
      },
      {
        "question": "How does this prevent hallucinations?",
        "answer": "Most AI hallucinations come from stale training data about versions and release status. By forcing verification before forming beliefs, this skill prevents the root cause of incorrect answers."
      },
      {
        "question": "Can I override the verification step?",
        "answer": "The skill is designed to enforce verification. Forcing overrides would defeat the purpose of hallucination prevention, but you can skip by not using triggering keywords."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 128
    }
  ]
}
