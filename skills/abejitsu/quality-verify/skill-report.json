{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-16T14:57:00.619Z",
    "slug": "abejitsu-quality-verify",
    "source_url": "https://github.com/AbeJitsu/Game-Settings-Panel/tree/main/.claude/skills/quality-verify",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "3.0.0",
    "source_type": "community",
    "content_hash": "00e2d4e0b08aeba474ba79e94559a55d26f150c6d1f0d261881884ba5bc7aa45",
    "tree_hash": "003026744776f2498f5356274e5983d6785609fb79c4d5ccf3e6abcd854744ab"
  },
  "skill": {
    "name": "quality-verify",
    "description": "Verify the final deliverable meets all quality criteria before delivery. Use as the final validation step to ensure the output meets the user's quality standards across all 6 dimensions.",
    "summary": "Verify the final deliverable meets all quality criteria before delivery. Use as the final validation...",
    "icon": "âœ…",
    "version": "1.0.0",
    "author": "AbeJitsu",
    "license": "MIT",
    "category": "documentation",
    "tags": [
      "quality assurance",
      "validation",
      "delivery",
      "standards",
      "scoring"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill is a pure documentation/prompt file with no executable code, no file system access, no network calls, and no system interactions. All 22 static findings are false positives caused by the scanner misinterpreting markdown syntax, hash identifiers, and documentation text as security-relevant patterns. The human review correctly identified this as safe.",
    "risk_factor_evidence": [
      {
        "factor": "network",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "skill-report.json",
            "line_start": 6,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "SKILL.md",
            "line_start": 61,
            "line_end": 74
          },
          {
            "file": "SKILL.md",
            "line_start": 74,
            "line_end": 90
          },
          {
            "file": "SKILL.md",
            "line_start": 90,
            "line_end": 101
          },
          {
            "file": "SKILL.md",
            "line_start": 101,
            "line_end": 103
          },
          {
            "file": "SKILL.md",
            "line_start": 103,
            "line_end": 107
          },
          {
            "file": "SKILL.md",
            "line_start": 107,
            "line_end": 133
          },
          {
            "file": "SKILL.md",
            "line_start": 133,
            "line_end": 187
          },
          {
            "file": "SKILL.md",
            "line_start": 187,
            "line_end": 191
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 377,
    "audit_model": "claude",
    "audited_at": "2026-01-16T14:57:00.619Z"
  },
  "content": {
    "user_title": "Validate quality before delivery",
    "value_statement": "This skill provides final validation that deliverables meet all quality standards. It evaluates across 6 dimensions and produces a quality score with actionable recommendations. Ensures nothing goes to users without meeting your standards.",
    "seo_keywords": [
      "quality verification",
      "deliverable validation",
      "quality scoring",
      "Claude Code skill",
      "code quality check",
      "delivery standards",
      "quality gates",
      "completeness check",
      "consistency validation",
      "security review"
    ],
    "actual_capabilities": [
      "Scores deliverables across 6 quality dimensions",
      "Calculates overall quality score with weighted averages",
      "Identifies critical, major, and minor issues",
      "Provides actionable recommendations for improvements",
      "Compares against user-defined quality standards",
      "Determines if deliverable is ready to ship"
    ],
    "limitations": [
      "Does not execute code or run tests",
      "Does not access files outside the current context",
      "Does not modify or fix issues automatically",
      "Requires the AI to evaluate based on provided criteria"
    ],
    "use_cases": [
      {
        "target_user": "Development teams",
        "title": "Pre-delivery code review",
        "description": "Validate code, documentation, or features before shipping to ensure they meet team standards."
      },
      {
        "target_user": "Technical writers",
        "title": "Documentation quality gate",
        "description": "Check documentation for completeness, accuracy, and consistency before publication."
      },
      {
        "target_user": "QA engineers",
        "title": "Quality assessment scoring",
        "description": "Apply consistent quality criteria across deliverables with objective scoring and reporting."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic quality check",
        "scenario": "Quick deliverable validation",
        "prompt": "Use quality-verify to check if my current deliverable meets quality standards. Report the score and any issues found."
      },
      {
        "title": "Detailed dimension review",
        "scenario": "Comprehensive quality assessment",
        "prompt": "Run a full quality verification across all 6 dimensions: completeness, correctness, consistency, performance, security, and maintainability."
      },
      {
        "title": "Standard-based validation",
        "scenario": "Custom standards compliance",
        "prompt": "Verify my deliverable against our team quality standards. Load the standards from StandardsRepository and score accordingly."
      },
      {
        "title": "Ship decision support",
        "scenario": "Go/no-go delivery decision",
        "prompt": "Give me a quality score with clear recommendations. Should I deliver as-is, make minor fixes, or do major revision?"
      }
    ],
    "output_examples": [
      {
        "input": "Use quality-verify to check my React component deliverable",
        "output": [
          "Quality Score: 94/100 - Ready to Deliver",
          "Dimension Scores: Completeness 95, Correctness 90, Consistency 88, Performance 85, Security 90, Maintainability 95",
          "Issues Found: None",
          "Summary: Excellent quality across all dimensions. Component is production-ready."
        ]
      },
      {
        "input": "Verify our API documentation against quality standards",
        "output": [
          "Quality Score: 82/100 - Minor fixes recommended",
          "Dimension Scores: Completeness 90, Correctness 85, Consistency 80, Security 90, Maintainability 65",
          "Issues Found: Missing examples for error responses",
          "Recommendation: Add error response examples before delivery"
        ]
      },
      {
        "input": "Check if my feature implementation is ready to ship",
        "output": [
          "Quality Score: 78/100 - Minor issues found",
          "Dimension Scores: Completeness 85, Correctness 80, Consistency 75, Performance 70, Security 80, Maintainability 75",
          "Issues Found: Performance bottleneck in data fetching, Missing inline comments",
          "Summary: Good quality with fixable issues. Recommend resolving before delivery."
        ]
      }
    ],
    "best_practices": [
      "Use this skill as the final step before delivery to catch issues",
      "Review dimension scores to identify specific areas for improvement",
      "Apply the scoring algorithm consistently across all deliverables"
    ],
    "anti_patterns": [
      "Skipping quality verification when under time pressure",
      "Ignoring minor issues when score is above 80",
      "Not loading project-specific standards for context"
    ],
    "faq": [
      {
        "question": "Which AI tools support this skill?",
        "answer": "This skill works with Claude, Codex, and Claude Code. All three tools can invoke quality verification."
      },
      {
        "question": "What is the maximum quality score?",
        "answer": "The maximum score is 100. Scores above 85 indicate delivery-ready quality with no critical issues."
      },
      {
        "question": "How does it integrate with my project standards?",
        "answer": "The skill uses StandardsRepository to load your project-specific quality criteria and validates against those standards."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "Yes. This is a prompt-based skill with no file access, network calls, or code execution. Your data never leaves the context."
      },
      {
        "question": "What if my score is between 60-79?",
        "answer": "Scores in this range indicate acceptable quality with minor issues. The skill recommends fixing issues before delivery."
      },
      {
        "question": "How is this different from code review tools?",
        "answer": "This skill provides subjective quality assessment across 6 dimensions. It complements automated tools by evaluating completeness and maintainability."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md",
      "lines": 200
    }
  ]
}
