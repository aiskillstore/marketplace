{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T13:10:26.851Z",
    "slug": "dcjanus-fetch-url",
    "source_url": "https://github.com/DCjanus/prompts/tree/master/skills/fetch-url",
    "source_ref": "master",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "80536e9611e9ec0234ebbd2b676723ae86e944c8c1b552ee83aab0d4daeff77d",
    "tree_hash": "49e60d7b0a7dc7c5bb44de3582861afd35b14cef1a1499d9bf9d33e2b4f0a035"
  },
  "skill": {
    "name": "fetch-url",
    "description": "Ê∏≤ÊüìÁΩëÈ°µ URLÔºåÂéªÂô™ÊèêÂèñÊ≠£ÊñáÂπ∂ËæìÂá∫‰∏∫ MarkdownÔºàÈªòËÆ§ÔºâÊàñÂÖ∂‰ªñÊ†ºÂºè/ÂéüÂßã HTMLÔºå‰ª•ÂáèÂ∞ë Token„ÄÇ",
    "summary": "Ê∏≤ÊüìÁΩëÈ°µ URLÔºåÂéªÂô™ÊèêÂèñÊ≠£ÊñáÂπ∂ËæìÂá∫‰∏∫ MarkdownÔºàÈªòËÆ§ÔºâÊàñÂÖ∂‰ªñÊ†ºÂºè/ÂéüÂßã HTMLÔºå‰ª•ÂáèÂ∞ë Token„ÄÇ",
    "icon": "üîó",
    "version": "1.0.0",
    "author": "DCjanus",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "web",
      "content extraction",
      "markdown",
      "documentation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill renders web URLs and extracts content using Playwright and trafilatura. All capabilities (network requests, file writes) are necessary for the stated purpose. Code is clean with no obfuscation or suspicious patterns. Input validation prevents non-http(s) URLs. No credential access or persistence mechanisms.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 1,
            "line_end": 146
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 56,
            "line_end": 70
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 137,
            "line_end": 138
          }
        ]
      },
      {
        "factor": "external_commands",
        "evidence": [
          {
            "file": "scripts/fetch_url.py",
            "line_start": 59,
            "line_end": 63
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 146,
    "audit_model": "claude",
    "audited_at": "2026-01-10T13:10:26.850Z"
  },
  "content": {
    "user_title": "Fetch and render web page content",
    "value_statement": "Web pages contain heavy HTML that wastes tokens. This skill renders URLs with a headless browser, extracts clean text, and outputs as Markdown or other formats. Perfect for turning documentation and articles into efficient AI input.",
    "seo_keywords": [
      "fetch url",
      "web scraping",
      "markdown converter",
      "content extraction",
      "Claude",
      "Codex",
      "Claude Code",
      "token reduction",
      "HTML to Markdown",
      "web page renderer"
    ],
    "actual_capabilities": [
      "Renders URLs using headless Chromium browser via Playwright",
      "Extracts clean text content using trafilatura library",
      "Outputs in 8 formats: markdown, HTML, JSON, CSV, TXT, XML, XML-TEI, raw HTML",
      "Auto-detects local browser installations to avoid downloads",
      "Configurable timeout and browser path options",
      "Writes output directly to file or stdout"
    ],
    "limitations": [
      "Only supports http and https URL schemes",
      "Requires Chromium-based browser (local or Playwright-installed)",
      "Cannot render pages requiring authentication or login",
      "JavaScript-heavy pages may have reduced content"
    ],
    "use_cases": [
      {
        "target_user": "Documentation writers",
        "title": "Convert online docs",
        "description": "Turn web documentation into local Markdown files for offline access and cleaner editing."
      },
      {
        "target_user": "AI practitioners",
        "title": "Optimize AI context",
        "description": "Convert articles and guides to Markdown to maximize useful content within token limits."
      },
      {
        "target_user": "Researchers",
        "title": "Archive web content",
        "description": "Extract and save content from multiple URLs in structured formats for analysis."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic fetch",
        "scenario": "Get page content",
        "prompt": "Use fetch-url to render https://example.com/docs/getting-started and output as markdown"
      },
      {
        "title": "Save to file",
        "scenario": "Archive page content",
        "prompt": "Use fetch-url to extract https://api.example.com/docs --output ./api-docs.md --output-format markdown"
      },
      {
        "title": "Custom format",
        "scenario": "Get raw HTML",
        "prompt": "Use fetch-url to fetch https://blog.example.com/post --output-format raw-html --timeout-ms 30000"
      },
      {
        "title": "Multiple pages",
        "scenario": "Batch extraction",
        "prompt": "Extract these three URLs as JSON: docs page, API reference, and changelog. Save each to output files."
      }
    ],
    "output_examples": [
      {
        "input": "Fetch the Python documentation homepage as markdown",
        "output": [
          "# Python Documentation",
          "",
          "## Getting Started",
          "",
          "Python is a programming language that lets you work quickly...",
          "[Saved output to stdout - 2.3KB of Markdown content]",
          "Extracted 15 headings, 84 paragraphs, and 23 links from https://docs.python.org/3/"
        ]
      }
    ],
    "best_practices": [
      "Use --timeout-ms flag for slow-loading pages to avoid hanging",
      "Specify --browser-path if you have Chrome/Edge installed locally",
      "Use --output-format raw-html when you need unmodified HTML"
    ],
    "anti_patterns": [
      "Do not try to fetch local file URLs (file:// scheme not supported)",
      "Avoid fetching pages behind login walls without proper authentication",
      "Do not use extremely long timeouts for unresponsive servers"
    ],
    "faq": [
      {
        "question": "Which browsers are supported?",
        "answer": "Any Chromium-based browser: Chrome, Edge, Brave, Chromium, or Playwright's bundled browser."
      },
      {
        "question": "What happens to JavaScript content?",
        "answer": "Playwright renders JavaScript, but dynamically loaded content may be missing if it loads after networkidle."
      },
      {
        "question": "How is this different from curl or wget?",
        "answer": "This tool uses a real browser to render JavaScript and extracts clean text, not raw HTML source."
      },
      {
        "question": "Is my browsing data saved?",
        "answer": "No persistent data. Each run creates a fresh browser context that is closed after extraction."
      },
      {
        "question": "Why does it need a browser?",
        "answer": "Modern web pages use JavaScript to load content. A browser renderer captures the final rendered page."
      },
      {
        "question": "Can this access authenticated content?",
        "answer": "Not directly. The tool runs headless without login credentials. Authenticated content requires separate handling."
      }
    ]
  },
  "file_structure": [
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "fetch_url.py",
          "type": "file",
          "path": "scripts/fetch_url.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
