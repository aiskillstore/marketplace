{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T13:06:32.991Z",
    "slug": "cycleaddict-verification-before-completion",
    "source_url": "https://github.com/Cycleaddict/generic-superpowers/tree/main/skills/verification-before-completion",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "963f6b63d58d88e029181126528a1c720f76c4f8cfaec3532ece14795aef5b77",
    "tree_hash": "a47ef4ef5b04c2bc29852a290adc9789c5480fde2db005d4f45b021355653408"
  },
  "skill": {
    "name": "verification-before-completion",
    "description": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always",
    "summary": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - req...",
    "icon": "üõ°Ô∏è",
    "version": "1.0.0",
    "author": "Cycleaddict",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "verification",
      "quality-assurance",
      "workflow",
      "best-practices",
      "testing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure prompt-based skill containing only behavioral guidelines. No executable code, scripts, network calls, file system access, or command execution capabilities. This is a YAML/markdown document instructing AI agents to verify work before claiming completion.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 140,
    "audit_model": "claude",
    "audited_at": "2026-01-10T13:06:32.991Z"
  },
  "content": {
    "user_title": "Verify work before claiming completion",
    "value_statement": "AI agents often claim completion without proof. This skill enforces a verification gate that requires running actual verification commands and examining output before making any success claims. Prevents false completion reports and broken code.",
    "seo_keywords": [
      "verification before completion",
      "AI quality assurance",
      "Claude Code verification",
      "code verification workflow",
      "test verification skill",
      "prevent false completion",
      "evidence-based claims",
      "CLAUDE verification",
      "Codex verification",
      "CLAUDE Code quality control"
    ],
    "actual_capabilities": [
      "Blocks completion claims without running verification commands",
      "Requires examining full command output before claiming success",
      "Prevents partial verification shortcuts",
      "Enforces red-green cycle verification for regression tests",
      "Validates actual changes match claimed work"
    ],
    "limitations": [
      "Does not execute verification commands itself",
      "Cannot verify build environments or tool availability",
      "Does not store verification history across sessions",
      "Requires the AI agent to identify and run appropriate verification commands"
    ],
    "use_cases": [
      {
        "target_user": "Software developers",
        "title": "Ensure PRs are verified",
        "description": "Apply before committing or creating pull requests to verify tests pass and builds succeed."
      },
      {
        "target_user": "AI developers",
        "title": "Prevent false success reports",
        "description": "Stop AI agents from claiming work is complete without actual verification evidence."
      },
      {
        "target_user": "Code reviewers",
        "title": "Verify requirements are met",
        "description": "Check that all requirements are verified against actual output before considering tasks complete."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic verification",
        "scenario": "Before claiming tests pass",
        "prompt": "Run the full test suite and report the actual pass/fail count. Do not claim completion until you show verified output."
      },
      {
        "title": "Build verification",
        "scenario": "Before claiming build success",
        "prompt": "Execute the build command and show the exit code. Only claim build success after seeing exit 0."
      },
      {
        "title": "Regression test",
        "scenario": "Verify regression test works",
        "prompt": "Run the red-green cycle: write test, verify it fails, apply fix, verify it passes, revert and confirm failure returns, restore fix and confirm success."
      },
      {
        "title": "Agent delegation",
        "scenario": "Verify delegated work",
        "prompt": "Do not trust agent reports. Check the VCS diff and independently verify all changes work as claimed before reporting completion."
      }
    ],
    "output_examples": [
      {
        "input": "I've fixed the bug and all tests pass",
        "output": [
          "Before claiming completion, I need to verify:",
          "1. Running test suite...",
          "2. Result: 34/34 tests pass, 0 failures",
          "3. Exit code: 0",
          "VERIFIED: All tests pass with fresh verification evidence"
        ]
      }
    ],
    "best_practices": [
      "Always run verification commands fresh, never rely on previous runs",
      "Examine complete output including exit codes and failure counts",
      "Never use words like should, probably, or seems before verification",
      "Verify independently when agents report success on your behalf"
    ],
    "anti_patterns": [
      "Claiming completion based on partial verification",
      "Trusting agent success reports without independent verification",
      "Using confidence or assumptions as substitutes for evidence",
      "Skipping verification when tired or wanting work to finish"
    ],
    "faq": [
      {
        "question": "Which AI platforms support this skill?",
        "answer": "This skill works with Claude, Codex, and Claude Code. Apply it whenever the AI might claim work is complete."
      },
      {
        "question": "What verification commands should I run?",
        "answer": "Run the actual command for your claim: npm test for tests, go build for builds, pytest for Python tests, or your project's verification scripts."
      },
      {
        "question": "How does this integrate with my workflow?",
        "answer": "Activate before committing, creating PRs, or reporting task completion. The skill prompts for verification evidence first."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "Yes. This is a prompt-only skill. It does not access files, execute commands, or transmit data. It only guides AI behavior."
      },
      {
        "question": "What if verification fails?",
        "answer": "Report actual status with evidence. The skill requires stating the real result, not hiding failures. This enables proper fixes."
      },
      {
        "question": "How is this different from other testing skills?",
        "answer": "This skill does not run tests. It enforces verification discipline before making any completion claim. It complements testing skills."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
