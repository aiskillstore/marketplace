{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T04:15:10.178Z",
    "slug": "K-Dense-AI-shap",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/shap",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "shap",
    "description": "Model interpretability and explainability using SHAP (SHapley Additive exPlanations). Use this skill when explaining machine learning model predictions, computing feature importance, generating SHAP plots (waterfall, beeswarm, bar, scatter, force, heatmap), debugging models, analyzing model bias or fairness, comparing models, or implementing explainable AI. Works with tree-based models (XGBoost, LightGBM, Random Forest), deep learning (TensorFlow, PyTorch), linear models, and any black-box model.",
    "summary": "Model interpretability and explainability using SHAP (SHapley Additive exPlanations). Use this skill...",
    "icon": "üîç",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "data",
    "tags": [
      "shap",
      "explainability",
      "feature-importance",
      "model-interpretability",
      "machine-learning"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No suspicious code patterns or data access behaviors detected. Files are documentation only and match the stated SHAP explainability scope.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 5,
    "total_lines": 2463,
    "audit_model": "codex",
    "audited_at": "2026-01-02T04:15:10.177Z"
  },
  "content": {
    "user_title": "Explain models with SHAP insights",
    "value_statement": "Understand which features drive predictions and why outcomes change. This skill guides SHAP explainers, plots, and interpretation for reliable insights.",
    "seo_keywords": [
      "SHAP explanations",
      "model interpretability",
      "feature importance",
      "SHAP plots",
      "XGBoost explainability",
      "Claude",
      "Codex",
      "Claude Code",
      "model debugging",
      "fairness analysis"
    ],
    "actual_capabilities": [
      "Select the right SHAP explainer for model type",
      "Compute and interpret SHAP values and baselines",
      "Create SHAP plots such as beeswarm, bar, waterfall, and scatter",
      "Run model debugging workflows with error analysis",
      "Compare feature importance across models and cohorts",
      "Perform fairness and bias analysis with SHAP"
    ],
    "limitations": [
      "Does not run models or install dependencies for you",
      "Explainers for non tree models can be slow or approximate",
      "Results depend on representative background data selection",
      "Does not provide causal conclusions from SHAP values"
    ],
    "use_cases": [
      {
        "target_user": "Data scientist",
        "title": "Explain feature impact",
        "description": "Use SHAP plots to show global importance and explain individual predictions."
      },
      {
        "target_user": "ML engineer",
        "title": "Debug model behavior",
        "description": "Investigate errors, leakage, and unexpected feature influence."
      },
      {
        "target_user": "Risk analyst",
        "title": "Assess fairness",
        "description": "Compare SHAP impacts across protected groups and find proxies."
      }
    ],
    "prompt_templates": [
      {
        "title": "Pick an explainer",
        "scenario": "Unsure which SHAP explainer to use",
        "prompt": "I have a random forest classifier and tabular data. Which SHAP explainer should I use and why? Provide a short setup guide."
      },
      {
        "title": "Global importance",
        "scenario": "Need a global feature ranking",
        "prompt": "Show me how to compute SHAP values and produce a beeswarm and bar plot for my XGBoost model. List key interpretation tips."
      },
      {
        "title": "Local explanation",
        "scenario": "Explain one prediction",
        "prompt": "Help me explain a single prediction with a waterfall plot. Include how to interpret base value and SHAP signs."
      },
      {
        "title": "Fairness check",
        "scenario": "Compare groups for bias",
        "prompt": "Describe a SHAP workflow to compare feature importance across demographic groups and detect proxy features."
      }
    ],
    "output_examples": [
      {
        "input": "Explain the top drivers for this model and how to visualize them.",
        "output": [
          "Use TreeExplainer for the trained tree model",
          "Compute SHAP values on a test subset",
          "Plot a beeswarm for global importance",
          "Use a waterfall plot for one representative prediction"
        ]
      }
    ],
    "best_practices": [
      "Start with global plots before local explanations",
      "Use representative background data and document the choice",
      "Validate explanations against domain knowledge"
    ],
    "anti_patterns": [
      "Using KernelExplainer for tree models without need",
      "Interpreting SHAP values as causal effects",
      "Explaining an entire massive dataset without sampling"
    ],
    "faq": [
      {
        "question": "Is this compatible with my model type?",
        "answer": "Yes, it covers tree, linear, deep learning, and black box models with the correct explainer."
      },
      {
        "question": "What are the limits for large datasets?",
        "answer": "Use sampling or batching, and avoid slow explainers when a specialized one exists."
      },
      {
        "question": "Can I integrate this into an API?",
        "answer": "Yes, follow the production workflow to cache explainers and return top features."
      },
      {
        "question": "Does it access my data or keys?",
        "answer": "No, the skill documentation contains no code that reads files or credentials."
      },
      {
        "question": "My plots do not render, what should I do?",
        "answer": "Check your matplotlib backend and use show or save flags as described."
      },
      {
        "question": "How does this compare to permutation importance?",
        "answer": "SHAP provides local and global explanations with additive consistency, while permutation is global only."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "explainers.md",
          "type": "file",
          "path": "references/explainers.md"
        },
        {
          "name": "plots.md",
          "type": "file",
          "path": "references/plots.md"
        },
        {
          "name": "theory.md",
          "type": "file",
          "path": "references/theory.md"
        },
        {
          "name": "workflows.md",
          "type": "file",
          "path": "references/workflows.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}