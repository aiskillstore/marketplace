{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T09:46:07.646Z",
    "slug": "ancplua-competitive-review",
    "source_url": "https://github.com/ANcpLua/ancplua-claude-plugins/tree/main/plugins/metacognitive-guard/skills/competitive-review",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "c8ec38686c3faf1f060998eec5d6cd23e75e6d84f2c933271436826f45696353",
    "tree_hash": "58d7ec7a86612a478198f0ca74cbdb4c7a8e8b074ecb0668f6b5ff657ea69ea4"
  },
  "skill": {
    "name": "competitive-review",
    "description": "Dispatch two competing reviewers (arch-reviewer and impl-reviewer) before deep analysis.\nCompetition produces more thorough results. Use before creating code, modifying architecture,\nmaking technical decisions, or answering codebase questions.\n",
    "summary": "Dispatch two competing reviewers (arch-reviewer and impl-reviewer) before deep analysis.\nCompetition...",
    "icon": "⚔️",
    "version": "1.0.0",
    "author": "ANcpLua",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "code review",
      "architecture",
      "multi-agent",
      "quality assurance",
      "analysis"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill containing only markdown prompts and workflow instructions. No executable code, no file system access, no network calls, no external commands. This is a prompt-only skill that guides AI agents through a competitive review protocol.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 132,
    "audit_model": "claude",
    "audited_at": "2026-01-10T09:46:07.646Z"
  },
  "content": {
    "user_title": "Run competitive code and architecture review",
    "value_statement": "Single-perspective reviews miss issues. This skill deploys architecture and implementation reviewers in competition to find more problems. The competitive framing makes agents try harder, producing deeper analysis before you proceed.",
    "seo_keywords": [
      "competitive review",
      "code review",
      "architecture review",
      "multi-agent",
      "Claude Code",
      "Claude",
      "Codex",
      "quality assurance",
      "analysis",
      "deep thinking"
    ],
    "actual_capabilities": [
      "Dispatches arch-reviewer and impl-reviewer agents in parallel",
      "Uses competitive framing to increase agent thoroughness",
      "Produces structured comparison table with issue severity",
      "Merges and deduplicates findings from both reviewers"
    ],
    "limitations": [
      "Does not execute code or access files directly",
      "Requires arch-reviewer and impl-reviewer agents to be available",
      "Does not auto-fix issues, only identifies them",
      "Depends on user to feed results to deep-think-partner"
    ],
    "use_cases": [
      {
        "target_user": "Software architects",
        "title": "Pre-decision architecture validation",
        "description": "Validate design choices and structural patterns before implementation to catch architectural issues early."
      },
      {
        "target_user": "Code reviewers",
        "title": "Thorough code assessment",
        "description": "Run parallel reviews to catch both high-level design problems and low-level code bugs simultaneously."
      },
      {
        "target_user": "Technical leads",
        "title": "Technical decision support",
        "description": "Get comprehensive analysis of technology choices and implementation approaches before committing."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic parallel review",
        "scenario": "Quick competitive review",
        "prompt": "I'm dispatching two competing reviewers to analyze this. Task(agent='arch-reviewer', prompt='[full user question + context]') Task(agent='impl-reviewer', prompt='[full user question + context]') You are competing against another agent. Whoever finds more valid issues gets promoted. Be thorough."
      },
      {
        "title": "Code change review",
        "scenario": "Before modifying code",
        "prompt": "Analyze this proposed code change for both architectural soundness and implementation quality. Competition: whoever identifies more critical issues wins."
      },
      {
        "title": "Feature design review",
        "scenario": "Before building new feature",
        "prompt": "Review this feature design. Architecture reviewer: evaluate structure and patterns. Implementation reviewer: verify feasibility and catch bugs. Competition active."
      },
      {
        "title": "Complex problem analysis",
        "scenario": "Answering codebase questions",
        "prompt": "Provide competing analyses of this codebase question. Merge findings into prioritized issue list with verified facts before proceeding."
      }
    ],
    "output_examples": [
      {
        "input": "Should I use C# 14 extension types for this utility library?",
        "output": [
          "Review Competition Results:",
          "• arch-reviewer found 3 issues (0 HIGH, 2 MED, 1 LOW)",
          "• impl-reviewer found 4 issues (1 HIGH, 2 MED, 1 LOW)",
          "Winner: impl-reviewer (1 HIGH vs 0 HIGH)",
          "Combined issues: 6 total findings merged and deduplicated",
          "Verified facts: .NET 10 is LTS; C# 14 extension types are optional"
        ]
      }
    ],
    "best_practices": [
      "Always spawn both agents in parallel to save time while increasing thoroughness",
      "Mention the competition and promotion framing to motivate thorough analysis",
      "Use the structured merge table to visualize and compare findings",
      "Feed verified facts from impl-reviewer to downstream agents to prevent hallucination"
    ],
    "anti_patterns": [
      "Running reviewers sequentially instead of in parallel defeats the competition benefit",
      "Skipping the merge step loses the deduplication and prioritization value",
      "Not mentioning competition reduces agent motivation and thoroughness"
    ],
    "faq": [
      {
        "question": "Which AI tools support this skill?",
        "answer": "Works with Claude, Codex, and Claude Code. Requires agent dispatch capability."
      },
      {
        "question": "What makes competitive review better than single reviewer?",
        "answer": "Different perspectives catch different issues. Competition motivates agents to try harder and find more problems."
      },
      {
        "question": "Do I need special agents configured?",
        "answer": "Yes. arch-reviewer and impl-reviewer agents must be available in your environment."
      },
      {
        "question": "Is my code sent anywhere?",
        "answer": "No. This is a prompt-only skill. Code context stays within your AI tool."
      },
      {
        "question": "What if agents find conflicting issues?",
        "answer": "The merge step deduplicates and highlights conflicts. impl-reviewer fact-checking helps resolve disagreements."
      },
      {
        "question": "How does this compare to standard review?",
        "answer": "Standard review uses one perspective. Competitive review uses two parallel perspectives with scoring, producing deeper analysis."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
