{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T17:00:22.243Z",
    "slug": "wshobson-prompt-engineering-patterns",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/prompt-engineering-patterns",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "d668f45ddecd0893ea61a31a55e466ea7074b0531962302ace2dfc8e07936dd0",
    "tree_hash": "324e58405dc211baf5d71b6ebd520bbba73bf8903b5114786b212e59ef5b6044"
  },
  "skill": {
    "name": "prompt-engineering-patterns",
    "description": "Master advanced prompt engineering techniques to maximize LLM performance, reliability, and controllability in production. Use when optimizing prompts, improving LLM outputs, or designing production prompt templates.",
    "summary": "Master advanced prompt engineering techniques to maximize LLM performance, reliability, and controll...",
    "icon": "üìù",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "research",
    "tags": [
      "prompting",
      "llm",
      "few-shot",
      "templates",
      "optimization"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "A documentation-focused skill with a Python utility script for prompt optimization. Contains no network calls, no sensitive filesystem access, and no external command execution. The optimize-prompt.py script is a local testing utility with a mock LLM client for demonstration.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 12,
    "total_lines": 4042,
    "audit_model": "claude",
    "audited_at": "2026-01-04T17:00:22.243Z"
  },
  "content": {
    "user_title": "Master Prompt Engineering for Better AI Results",
    "value_statement": "LLMs produce inconsistent results with poorly crafted prompts. This skill provides battle-tested patterns and templates for chain-of-thought reasoning, few-shot learning, and systematic prompt optimization to improve output quality.",
    "seo_keywords": [
      "Claude prompt engineering",
      "ChatGPT prompting",
      "LLM optimization",
      "few-shot examples",
      "chain-of-thought",
      "prompt templates",
      "Claude Code",
      "OpenAI prompting",
      "AI instruction design",
      "prompt patterns"
    ],
    "actual_capabilities": [
      "Apply chain-of-thought prompting for step-by-step reasoning tasks",
      "Build effective few-shot learning systems with example selection strategies",
      "Create reusable prompt templates with variable interpolation",
      "Implement A/B testing workflows to measure and improve prompt performance",
      "Design system prompts that constrain model behavior and output format"
    ],
    "limitations": [
      "Requires understanding of your specific LLM API and integration",
      "Does not provide hosted prompt execution or API access",
      "Effectiveness varies based on the underlying model capabilities",
      "Templates require customization for your specific use case"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Optimize Production Prompts",
        "description": "Systematically test and refine prompts for production LLM applications using A/B testing frameworks."
      },
      {
        "target_user": "Content Developers",
        "title": "Build Template Libraries",
        "description": "Create reusable prompt templates with variable interpolation for consistent content generation."
      },
      {
        "target_user": "Research Teams",
        "title": "Apply Advanced Techniques",
        "description": "Implement chain-of-thought and self-consistency patterns for complex reasoning tasks."
      }
    ],
    "prompt_templates": [
      {
        "title": "Simple Classification",
        "scenario": "Basic text categorization",
        "prompt": "Classify this text into one of these categories: Positive, Negative, Neutral.\n\nText: {text}\n\nCategory:"
      },
      {
        "title": "Few-Shot Extraction",
        "scenario": "Structured data extraction",
        "prompt": "Extract information in JSON format.\n\nExample:\nText: Apple CEO Tim Cook announced new iPhone.\nOutput: {\"persons\":[\"Tim Cook\"],\"organizations\":[\"Apple\"],\"products\":[\"iPhone\"]}\n\nText: {text}\n\nOutput:"
      },
      {
        "title": "Chain-of-Thought",
        "scenario": "Math reasoning",
        "prompt": "Solve this step by step.\n\nProblem: {problem}\n\nStep 1: Identify what we know\nStep 2: Determine the approach\nStep 3: Calculate\nStep 4: Verify\n\nAnswer:"
      },
      {
        "title": "Self-Consistency",
        "scenario": "Multiple reasoning paths",
        "prompt": "Solve this problem three different ways. Then identify which answer appears most frequently.\n\nProblem: {problem}\n\nApproach 1:\nResult:\n\nApproach 2:\nResult:\n\nApproach 3:\nResult:\n\nFinal Answer (most common):"
      }
    ],
    "output_examples": [
      {
        "input": "Write a prompt to summarize customer feedback",
        "output": [
          "Start with the system role: You are a professional analyst.",
          "Add specific constraints: Summarize in 3 bullet points.",
          "Include examples: Show input-output pairs for feedback categories.",
          "Define format: Use consistent structure for each summary."
        ]
      }
    ],
    "best_practices": [
      "Be specific about format, length, and style requirements rather than relying on implied instructions",
      "Use few-shot examples to demonstrate the exact output format you need, especially for structured data",
      "Test prompts on edge cases and diverse inputs before deploying to production"
    ],
    "anti_patterns": [
      "Overloading prompts with too many examples, causing token limits to reduce space for actual input",
      "Using vague instructions like be helpful or be accurate that different models interpret differently",
      "Skipping verification steps for factual or logical outputs that require validation"
    ],
    "faq": [
      {
        "question": "Which LLMs work with these patterns?",
        "answer": "Patterns work with Claude, GPT-4, Claude Code, and most instruction-tuned models. Some techniques like chain-of-thought work best on reasoning-capable models."
      },
      {
        "question": "What is the optimal number of few-shot examples?",
        "answer": "Most tasks perform well with 3 to 5 examples. More examples can dilute focus and consume token budget. Test different counts for your specific use case."
      },
      {
        "question": "How do I integrate with my existing codebase?",
        "answer": "The skill provides template systems and Python utilities. Adapt the PromptTemplate classes to your LLM client. The optimize-prompt.py script shows a testing workflow you can extend."
      },
      {
        "question": "Is my data sent anywhere?",
        "answer": "No. This skill runs locally. The reference materials and utility scripts operate entirely on your machine. No external network calls are made by any component."
      },
      {
        "question": "Why do my prompts work differently across models?",
        "answer": "Models have different training and capabilities. Test and adjust templates per model. Chain-of-thought works better on reasoning models. Some models need more explicit format instructions."
      },
      {
        "question": "How does this compare to other prompting skills?",
        "answer": "This skill focuses on production-ready patterns with systematic optimization workflows. It covers template systems, A/B testing, and evaluation metrics for real-world deployment."
      }
    ]
  },
  "file_structure": [
    {
      "name": "assets",
      "type": "dir",
      "path": "assets",
      "children": [
        {
          "name": "few-shot-examples.json",
          "type": "file",
          "path": "assets/few-shot-examples.json"
        },
        {
          "name": "prompt-template-library.md",
          "type": "file",
          "path": "assets/prompt-template-library.md"
        }
      ]
    },
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "chain-of-thought.md",
          "type": "file",
          "path": "references/chain-of-thought.md"
        },
        {
          "name": "few-shot-learning.md",
          "type": "file",
          "path": "references/few-shot-learning.md"
        },
        {
          "name": "prompt-optimization.md",
          "type": "file",
          "path": "references/prompt-optimization.md"
        },
        {
          "name": "prompt-templates.md",
          "type": "file",
          "path": "references/prompt-templates.md"
        },
        {
          "name": "system-prompts.md",
          "type": "file",
          "path": "references/system-prompts.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "optimize-prompt.py",
          "type": "file",
          "path": "scripts/optimize-prompt.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
