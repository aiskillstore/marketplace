{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T06:19:51.331Z",
    "slug": "wshobson-prompt-engineering-patterns",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/prompt-engineering-patterns",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "d668f45ddecd0893ea61a31a55e466ea7074b0531962302ace2dfc8e07936dd0",
    "tree_hash": "010dcd685e606b27b0e5470eb3ad1b2b17455aa09d744be40f298b07ed53c41a"
  },
  "skill": {
    "name": "prompt-engineering-patterns",
    "description": "Master advanced prompt engineering techniques to maximize LLM performance, reliability, and controllability in production. Use when optimizing prompts, improving LLM outputs, or designing production prompt templates.",
    "summary": "Master advanced prompt engineering techniques to maximize LLM performance, reliability, and controll...",
    "icon": "ðŸ§ ",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "research",
    "tags": [
      "prompting",
      "llm",
      "few-shot",
      "templates",
      "optimization"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill is primarily documentation and example assets with a single local Python script for prompt evaluation. The script runs locally, writes results to a user-specified JSON file, and does not include network calls, environment access, or external command execution.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/optimize-prompt.py",
            "line_start": 1,
            "line_end": 6
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/optimize-prompt.py",
            "line_start": 227,
            "line_end": 230
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 11,
    "total_lines": 2935,
    "audit_model": "codex",
    "audited_at": "2026-01-04T06:19:51.331Z"
  },
  "content": {
    "user_title": "Improve prompts with proven patterns",
    "value_statement": "Inconsistent prompt outputs slow delivery and reduce trust in LLM features. This skill provides patterns, templates, and optimization guidance to improve quality, consistency, and control.",
    "seo_keywords": [
      "prompt engineering",
      "few-shot learning",
      "chain-of-thought",
      "prompt templates",
      "prompt optimization",
      "system prompts",
      "LLM evaluation",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Provide a prompt template library for classification, extraction, generation, and transformation tasks.",
      "Supply few-shot example datasets in JSON for common tasks like sentiment, extraction, and summarization.",
      "Explain few-shot selection strategies such as semantic similarity, diversity, and error-based selection.",
      "Describe chain-of-thought, self-consistency, and least-to-most prompting patterns.",
      "Include a Python script to evaluate prompts with metrics and A/B comparisons, then export results to JSON."
    ],
    "limitations": [
      "No built-in LLM client or API keys are included.",
      "Optimization script requires user-provided test cases and a client with a complete method.",
      "Token counting uses simple word splits and may not match model tokenizers.",
      "Templates and examples are generic and require domain adaptation."
    ],
    "use_cases": [
      {
        "target_user": "ML engineer",
        "title": "Stabilize production prompts",
        "description": "Apply few-shot selection and verification patterns to reduce output variance in deployed LLM features."
      },
      {
        "target_user": "Product manager",
        "title": "Define assistant behavior",
        "description": "Draft system prompts and output formats for new assistants with consistent tone and constraints."
      },
      {
        "target_user": "Support operations lead",
        "title": "Standardize support replies",
        "description": "Create templates for intent detection and response drafts aligned with team policies."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic summary",
        "scenario": "Summarize a short article",
        "prompt": "Summarize the article in three bullet points that capture the main findings."
      },
      {
        "title": "Constrained classification",
        "scenario": "Label customer tickets",
        "prompt": "Classify the message into one label: billing, shipping, or technical. Return only the label."
      },
      {
        "title": "Few-shot extraction",
        "scenario": "Extract entities from text",
        "prompt": "Extract persons and organizations.\nExample: Text: Alice joined Acme. Output: persons: Alice; organizations: Acme.\nText: {text}\nOutput:"
      },
      {
        "title": "Validated answer",
        "scenario": "Answer with verification",
        "prompt": "Answer the question using only the provided context. Then verify each claim against the context and revise any unsupported statement."
      }
    ],
    "output_examples": [
      {
        "input": "Create a system prompt for a finance analyst who must cite sources.",
        "output": [
          "Role: Finance analyst focused on accuracy and clarity",
          "Use only provided context and cite a source for each claim",
          "State gaps when data is missing",
          "Output sections: summary, analysis, sources"
        ]
      }
    ],
    "best_practices": [
      "Start simple and add constraints only when needed.",
      "Keep example formats consistent across the prompt.",
      "Test prompts on diverse inputs before deployment."
    ],
    "anti_patterns": [
      "Adding many examples without relevance checks.",
      "Mixing output formats within the same prompt.",
      "Skipping verification on high risk tasks."
    ],
    "faq": [
      {
        "question": "Which platforms does this skill support?",
        "answer": "It is model agnostic and works with Claude, Codex, Claude Code, and other LLM APIs."
      },
      {
        "question": "What are the main limitations?",
        "answer": "It provides patterns and examples only and does not include domain data, API keys, or automated evaluation services."
      },
      {
        "question": "How do I use the optimization script?",
        "answer": "Provide a client with a complete method and a test suite, then run the Python script locally."
      },
      {
        "question": "Does it send my data anywhere?",
        "answer": "No. The materials are local files and the script only processes inputs you provide."
      },
      {
        "question": "What if outputs are inconsistent?",
        "answer": "Tighten constraints, add relevant examples, and compare variants with A/B testing."
      },
      {
        "question": "How is this different from a template library?",
        "answer": "It combines templates with selection strategies and optimization workflows, not just static prompts."
      }
    ]
  },
  "file_structure": [
    {
      "name": "assets",
      "type": "dir",
      "path": "assets",
      "children": [
        {
          "name": "few-shot-examples.json",
          "type": "file",
          "path": "assets/few-shot-examples.json"
        },
        {
          "name": "prompt-template-library.md",
          "type": "file",
          "path": "assets/prompt-template-library.md"
        }
      ]
    },
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "chain-of-thought.md",
          "type": "file",
          "path": "references/chain-of-thought.md"
        },
        {
          "name": "few-shot-learning.md",
          "type": "file",
          "path": "references/few-shot-learning.md"
        },
        {
          "name": "prompt-optimization.md",
          "type": "file",
          "path": "references/prompt-optimization.md"
        },
        {
          "name": "prompt-templates.md",
          "type": "file",
          "path": "references/prompt-templates.md"
        },
        {
          "name": "system-prompts.md",
          "type": "file",
          "path": "references/system-prompts.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "optimize-prompt.py",
          "type": "file",
          "path": "scripts/optimize-prompt.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
