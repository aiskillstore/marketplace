{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T01:02:40.210Z",
    "slug": "wshobson-rag-implementation",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/rag-implementation",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "b9f8ca17aad219835e06b60f33b8d312ccfa35cdcca6c1faa3376e6521835243",
    "tree_hash": "c7162721efff5ced9e78049e7ffb64167b95d62aec07d5535e78ea4c058e323b"
  },
  "skill": {
    "name": "rag-implementation",
    "description": "Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and semantic search. Use when implementing knowledge-grounded AI, building document Q&A systems, or integrating LLMs with external knowledge bases.",
    "summary": "Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and se...",
    "icon": "ðŸ“š",
    "version": "1.0.0",
    "license": "MIT",
    "category": "data",
    "tags": [
      "rag",
      "retrieval",
      "embeddings",
      "vector-databases",
      "semantic-search"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill content is instructional guidance and code examples only. No file access, network calls, or execution patterns are present.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 403,
    "audit_model": "codex",
    "audited_at": "2026-01-02T01:02:40.210Z"
  },
  "content": {
    "user_title": "Build a grounded RAG pipeline",
    "value_statement": "You need reliable answers from private or changing documents. This skill explains RAG components and patterns to ground responses with retrieved context.",
    "seo_keywords": [
      "RAG",
      "retrieval augmented generation",
      "vector databases",
      "semantic search",
      "embeddings",
      "Claude",
      "Codex",
      "Claude Code",
      "LLM applications",
      "document Q&A"
    ],
    "actual_capabilities": [
      "Explains vector database options and setup patterns",
      "Shows chunking strategies and embedding model choices",
      "Describes retrieval approaches including hybrid and multi query",
      "Covers reranking methods and contextual compression",
      "Outlines evaluation metrics for accuracy and grounding"
    ],
    "limitations": [
      "Does not run code or provision databases",
      "Does not ingest or index your documents automatically",
      "Examples focus on Python and LangChain style APIs",
      "No deployment, scaling, or cost modeling guidance"
    ],
    "use_cases": [
      {
        "target_user": "Product engineer",
        "title": "Add RAG to a chatbot",
        "description": "Design a retrieval pipeline that grounds answers with citations from internal documentation."
      },
      {
        "target_user": "Data scientist",
        "title": "Evaluate retrieval quality",
        "description": "Define metrics and test cases to measure accuracy, grounding, and retrieval quality."
      },
      {
        "target_user": "Platform team",
        "title": "Select vector storage",
        "description": "Compare vector database options and choose an approach that fits scale and deployment needs."
      }
    ],
    "prompt_templates": [
      {
        "title": "RAG quick start",
        "scenario": "New project kickoff",
        "prompt": "Create a simple RAG plan for a document Q&A app. Include chunking, embeddings, vector store, and a retrieval chain."
      },
      {
        "title": "Hybrid retrieval plan",
        "scenario": "Improve recall",
        "prompt": "Design a hybrid retrieval setup using dense and BM25. Include weights, k values, and reranking suggestions."
      },
      {
        "title": "Chunking strategy",
        "scenario": "Large documents",
        "prompt": "Recommend a chunking approach for long technical manuals. Include size, overlap, and splitters to try."
      },
      {
        "title": "RAG evaluation",
        "scenario": "Quality review",
        "prompt": "Propose an evaluation plan with accuracy, retrieval quality, and groundedness metrics. Include simple test case structure."
      }
    ],
    "output_examples": [
      {
        "input": "Outline a RAG pipeline for internal docs with citations.",
        "output": [
          "Select Chroma for local testing and plan a production vector store later",
          "Chunk documents at 800 tokens with 15 percent overlap",
          "Use a hybrid retriever with dense and BM25, then rerank top 20",
          "Return citations with source paths and page numbers"
        ]
      }
    ],
    "best_practices": [
      "Use metadata for filtering and debugging",
      "Combine hybrid search with reranking for top results",
      "Track retrieval metrics during evaluation"
    ],
    "anti_patterns": [
      "Indexing documents without chunk overlap",
      "Skipping citations in user facing answers",
      "Using only dense retrieval for keyword heavy queries"
    ],
    "faq": [
      {
        "question": "Which tools does this work with",
        "answer": "It is framework agnostic and provides examples aligned with LangChain style APIs."
      },
      {
        "question": "What are the main limits",
        "answer": "It provides guidance only and does not execute code or manage infrastructure."
      },
      {
        "question": "How do I integrate this into my app",
        "answer": "Map the steps to your stack and follow the pipeline order from ingestion to retrieval and answer."
      },
      {
        "question": "Is my data safe",
        "answer": "The skill does not access files or send network requests."
      },
      {
        "question": "What if retrieval quality is poor",
        "answer": "Adjust chunking, embedding choice, metadata filters, and reranking weights."
      },
      {
        "question": "How does this compare to basic search",
        "answer": "RAG adds semantic retrieval and grounding, which improves relevance over keyword only search."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
