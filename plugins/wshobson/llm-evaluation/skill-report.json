{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T05:23:52.435Z",
    "slug": "wshobson-llm-evaluation",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/llm-evaluation",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "c332ab07734534754a8153ac6e940a22431e86ba7c15eea2230a8352be5f38f2",
    "tree_hash": "be1e7fef7ce4e498b87ab37623405607f692e5d7b1d812b72a5fd5a646224afe"
  },
  "skill": {
    "name": "llm-evaluation",
    "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.",
    "summary": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human fe...",
    "icon": "ðŸ§ª",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "research",
    "tags": [
      "llm",
      "evaluation",
      "metrics",
      "benchmarking",
      "testing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill is documentation only and contains no executable files or tooling. No data access, network activity, or command execution is present. Risk is minimal.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 670,
    "audit_model": "codex",
    "audited_at": "2026-01-04T05:23:52.435Z"
  },
  "content": {
    "user_title": "Build reliable LLM evaluation plans",
    "value_statement": "You need consistent ways to measure LLM quality and regressions. This skill provides metrics, human review guidance, and testing frameworks.",
    "seo_keywords": [
      "LLM evaluation",
      "automated metrics",
      "human evaluation",
      "LLM judge",
      "A/B testing",
      "benchmarking",
      "Claude",
      "Codex",
      "Claude Code",
      "regression testing"
    ],
    "actual_capabilities": [
      "Lists automated metrics for generation, classification, and retrieval tasks",
      "Defines human evaluation dimensions and annotation forms",
      "Provides LLM-as-judge prompts for pointwise and pairwise scoring",
      "Describes A/B testing with significance and effect sizes",
      "Explains regression detection using baselines",
      "Outlines benchmarking workflows and result aggregation"
    ],
    "limitations": [
      "No runnable evaluation scripts are included in this directory",
      "External libraries are required for example metric implementations",
      "No datasets or benchmarks are bundled with the skill",
      "No tool specific integration steps are provided"
    ],
    "use_cases": [
      {
        "target_user": "ML engineer",
        "title": "Regression gate in CI",
        "description": "Design an evaluation checklist and thresholds to block model changes that reduce quality."
      },
      {
        "target_user": "Product manager",
        "title": "Model comparison brief",
        "description": "Compare two model options using human ratings and automated scores for a decision memo."
      },
      {
        "target_user": "Researcher",
        "title": "Benchmark study plan",
        "description": "Create a benchmarking plan with datasets, metrics, and reporting structure."
      }
    ],
    "prompt_templates": [
      {
        "title": "Starter evaluation plan",
        "scenario": "New chatbot feature",
        "prompt": "Create a basic evaluation plan with 3 automated metrics and 2 human criteria for a customer support chatbot."
      },
      {
        "title": "Metric selection guide",
        "scenario": "Summarization task",
        "prompt": "Recommend metrics for summarization, explain what each captures, and note one limitation per metric."
      },
      {
        "title": "LLM judge prompt",
        "scenario": "Two prompt variants",
        "prompt": "Draft a pairwise LLM judge prompt to compare response A and B for accuracy, helpfulness, and clarity."
      },
      {
        "title": "A/B test analysis",
        "scenario": "Production experiment",
        "prompt": "Describe a statistical testing plan for A/B evaluation, including sample size guidance and effect size reporting."
      }
    ],
    "output_examples": [
      {
        "input": "Propose an evaluation plan for a RAG assistant.",
        "output": [
          "Automated metrics: MRR, NDCG, Precision at K",
          "Human ratings: accuracy, relevance, helpfulness",
          "LLM judge: pairwise comparison for final answers",
          "Regression rule: fail if accuracy drops more than 5 percent"
        ]
      }
    ],
    "best_practices": [
      "Use multiple metrics and human review together",
      "Test with representative and diverse data",
      "Track baselines and statistical significance"
    ],
    "anti_patterns": [
      "Relying on a single metric",
      "Testing on training data",
      "Ignoring variance in small samples"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude and Codex?",
        "answer": "Yes, the guidance is model agnostic and applies to Claude, Codex, Claude Code, and other LLMs."
      },
      {
        "question": "What are the limits of this skill?",
        "answer": "It provides guidance and examples but no executable evaluation pipeline in this directory."
      },
      {
        "question": "How do I integrate with my stack?",
        "answer": "Map the metrics and workflows to your existing evaluation or CI tools."
      },
      {
        "question": "Does it access or store my data?",
        "answer": "No, it is static documentation and does not read or transmit data."
      },
      {
        "question": "What if scores are unstable?",
        "answer": "Increase sample size, review variance, and add human validation before decisions."
      },
      {
        "question": "How is this different from a benchmark list?",
        "answer": "It combines metrics, human review, and testing strategy rather than only listing benchmarks."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
