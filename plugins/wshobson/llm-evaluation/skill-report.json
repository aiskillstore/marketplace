{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T00:39:50.445Z",
    "slug": "wshobson-llm-evaluation",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/llm-evaluation",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "llm-evaluation",
    "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.",
    "summary": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human fe...",
    "icon": "ðŸ§ª",
    "version": "1.0.0",
    "license": "MIT",
    "category": "research",
    "tags": [
      "llm",
      "evaluation",
      "metrics",
      "testing",
      "benchmarking"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill is documentation only and contains no executable logic. No data access, exfiltration, or malicious execution patterns were found.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 471,
    "audit_model": "codex",
    "audited_at": "2026-01-02T00:39:50.444Z"
  },
  "content": {
    "user_title": "Build LLM evaluation plans",
    "value_statement": "You need reliable ways to measure LLM quality and regressions. This skill shows structured metrics, human review, and testing frameworks.",
    "seo_keywords": [
      "LLM evaluation",
      "automated metrics",
      "human review",
      "A/B testing",
      "benchmarking",
      "Claude",
      "Codex",
      "Claude Code",
      "regression testing",
      "LLM judge"
    ],
    "actual_capabilities": [
      "Lists automated metrics for text generation, classification, and retrieval tasks",
      "Defines human evaluation dimensions and annotation guidance",
      "Provides LLM as judge patterns for pointwise and pairwise scoring",
      "Describes A/B testing with statistical significance and effect sizes",
      "Explains regression detection using baseline comparisons",
      "Outlines benchmarking workflows and aggregation methods"
    ],
    "limitations": [
      "No runnable evaluation scripts are included in this directory",
      "External libraries are required for example metric implementations",
      "No datasets or benchmarks are bundled with the skill",
      "Integration with specific tools is not provided"
    ],
    "use_cases": [
      {
        "target_user": "ML engineer",
        "title": "Regression gate in CI",
        "description": "Design a regression checklist and metrics to block model changes that reduce quality."
      },
      {
        "target_user": "Product manager",
        "title": "Model comparison brief",
        "description": "Compare two model options using human ratings and automated scores."
      },
      {
        "target_user": "Researcher",
        "title": "Benchmark study plan",
        "description": "Create a benchmarking plan with datasets, metrics, and reporting structure."
      }
    ],
    "prompt_templates": [
      {
        "title": "Start evaluation plan",
        "scenario": "New LLM feature release",
        "prompt": "Create a simple evaluation plan with 3 automated metrics and 2 human criteria for a chatbot feature."
      },
      {
        "title": "Metric selection",
        "scenario": "Summarization task",
        "prompt": "Recommend metrics for summarization, explain what each captures, and give a short note on limitations."
      },
      {
        "title": "LLM judge setup",
        "scenario": "Two prompt variants",
        "prompt": "Draft a pairwise LLM judge prompt to compare response A and B for accuracy, helpfulness, and clarity."
      },
      {
        "title": "A/B test analysis",
        "scenario": "Production experiment",
        "prompt": "Describe a statistical testing plan for A/B evaluation, including sample size guidance and effect size reporting."
      }
    ],
    "output_examples": [
      {
        "input": "Propose an evaluation plan for a RAG assistant.",
        "output": [
          "Automated metrics: MRR, NDCG, Precision@K",
          "Human ratings: accuracy, relevance, helpfulness",
          "LLM judge: pairwise comparison for final answers",
          "Regression rule: fail if accuracy drops more than 5 percent"
        ]
      }
    ],
    "best_practices": [
      "Use multiple metrics and human review together",
      "Test with representative and diverse data",
      "Track baselines and statistical significance"
    ],
    "anti_patterns": [
      "Relying on a single metric",
      "Testing on training data",
      "Ignoring variance in small samples"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude and Codex?",
        "answer": "Yes, the guidance is model agnostic and applies to Claude, Codex, and other LLMs."
      },
      {
        "question": "What are the limits of this skill?",
        "answer": "It provides guidance and examples but no executable evaluation pipeline in this directory."
      },
      {
        "question": "How do I integrate with my stack?",
        "answer": "Map the metrics and workflows to your existing evaluation or CI tools."
      },
      {
        "question": "Does it access or store my data?",
        "answer": "No, it is static documentation and does not read or transmit data."
      },
      {
        "question": "What if scores are unstable?",
        "answer": "Increase sample size, review variance, and add human validation before decisions."
      },
      {
        "question": "How is this different from a benchmark list?",
        "answer": "It combines metrics, human review, and testing strategy rather than only listing benchmarks."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}