{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-03T02:35:53.993Z",
    "slug": "wshobson-embedding-strategies",
    "source_url": "https://github.com/wshobson/agents/tree/main/plugins/llm-application-dev/skills/embedding-strategies",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "embedding-strategies",
    "description": "Select and optimize embedding models for semantic search and RAG applications. Use when choosing embedding models, implementing chunking strategies, or optimizing embedding quality for specific domains.",
    "summary": "Select and optimize embedding models for semantic search and RAG applications. Use when choosing emb...",
    "icon": "ðŸ§­",
    "version": "1.0.0",
    "author": "wshobson",
    "license": "MIT",
    "category": "data",
    "tags": [
      "embeddings",
      "rag",
      "semantic-search",
      "chunking",
      "evaluation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No credential access, exfiltration, or persistence patterns found. The content is documentation with example code and external resource links only.",
    "risk_factors": [
      "network"
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 693,
    "audit_model": "codex",
    "audited_at": "2026-01-03T02:35:53.992Z"
  },
  "content": {
    "user_title": "Choose and optimize embedding strategies",
    "value_statement": "Choosing embedding models and chunking settings is complex for RAG systems. This skill provides model comparisons, templates, and evaluation guidance to improve retrieval quality.",
    "seo_keywords": [
      "embedding strategies",
      "RAG embeddings",
      "semantic search",
      "chunking strategy",
      "OpenAI embeddings",
      "Sentence Transformers",
      "Claude",
      "Codex",
      "Claude Code",
      "vector similarity"
    ],
    "actual_capabilities": [
      "Compare embedding models by dimensions, token limits, and best use cases",
      "Provide OpenAI embedding batching and dimension reduction templates",
      "Provide local embedding templates using sentence-transformers and E5",
      "Offer token, sentence, header, and recursive chunking strategies",
      "Explain retrieval evaluation metrics such as precision, recall, MRR, and nDCG"
    ],
    "limitations": [
      "Does not run or deploy models automatically",
      "Requires external libraries such as openai and sentence-transformers",
      "Does not provide datasets or benchmarks",
      "Templates require adaptation for your infrastructure"
    ],
    "use_cases": [
      {
        "target_user": "RAG developer",
        "title": "Select a model",
        "description": "Compare embedding options to balance accuracy, cost, and language coverage for a new RAG system."
      },
      {
        "target_user": "ML engineer",
        "title": "Tune chunking",
        "description": "Design chunk size and overlap settings to improve recall while controlling index size."
      },
      {
        "target_user": "Product analyst",
        "title": "Evaluate retrieval",
        "description": "Apply standard metrics to compare retrieval quality across releases and experiments."
      }
    ],
    "prompt_templates": [
      {
        "title": "Model shortlist",
        "scenario": "New RAG for support docs",
        "prompt": "Recommend an embedding model for English support documents with moderate budget. Include dimensions, token limits, and tradeoffs."
      },
      {
        "title": "Chunking plan",
        "scenario": "Long markdown content",
        "prompt": "Design a chunking strategy for long markdown documents. Include chunk size, overlap, and a method to keep headings with content."
      },
      {
        "title": "Local embeddings",
        "scenario": "On prem deployment",
        "prompt": "Provide a local embedding setup using sentence-transformers. Include normalization and query prefixes for search."
      },
      {
        "title": "Quality metrics",
        "scenario": "Evaluate retrieval results",
        "prompt": "Show how to compute precision at k, recall at k, MRR, and nDCG for retrieval results."
      }
    ],
    "output_examples": [
      {
        "input": "Help me choose an embedding model for multilingual support tickets with limited GPU.",
        "output": [
          "Shortlist multilingual-e5-large for multilingual coverage",
          "Consider text-embedding-3-small for lower cost and latency",
          "Use chunk size near 512 tokens with 50 overlap",
          "Measure recall at 10 and nDCG before final selection"
        ]
      }
    ],
    "best_practices": [
      "Match the model to language and domain needs",
      "Normalize embeddings when using cosine similarity",
      "Evaluate changes with consistent retrieval metrics"
    ],
    "anti_patterns": [
      "Mixing embeddings from different models in one index",
      "Skipping preprocessing for noisy text sources",
      "Overlapping chunks so heavily that context repeats"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes. It is a guidance skill and works in any of those platforms."
      },
      {
        "question": "What are the limits of the templates?",
        "answer": "They are examples only and require library installs and environment configuration."
      },
      {
        "question": "How do I integrate it with my vector database?",
        "answer": "Use the chunking and embedding steps, then store vectors using your database API."
      },
      {
        "question": "Does it access or transmit my data?",
        "answer": "No. The skill is static documentation without data collection or network calls."
      },
      {
        "question": "What if I get poor retrieval results?",
        "answer": "Adjust chunking, try another model, and remeasure with the provided metrics."
      },
      {
        "question": "How does it compare to generic RAG guides?",
        "answer": "It focuses on embedding choices, chunking patterns, and evaluation metrics for retrieval."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
