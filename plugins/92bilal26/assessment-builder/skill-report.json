{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T09:35:53.601Z",
    "slug": "92bilal26-assessment-builder",
    "source_url": "https://github.com/92Bilal26/TaskPilotAI/tree/main/.claude/skills/assessment-builder",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "ffaef19dd64d956c7621e80b73a04e4363a64a29ea49828b856d321b906355a3",
    "tree_hash": "8768ebbde3727c828583cbd3a8dc1aa85a05c0c9a32a14a819501880c01093b0"
  },
  "skill": {
    "name": "Assessment: [Title]",
    "description": "---",
    "summary": "---",
    "icon": "üìù",
    "version": "1.0.0",
    "author": "92Bilal26",
    "license": "MIT",
    "category": "documentation",
    "tags": [
      "assessment",
      "education",
      "rubrics",
      "question-design",
      "bloom's-taxonomy"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a documentation-focused skill with minimal code execution. The only executable script reads and validates YAML files using standard libraries. No network access, no command execution, no credential handling. Safe for distribution.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/validate-assessment.py",
            "line_start": 1,
            "line_end": 231
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/validate-assessment.py",
            "line_start": 196,
            "line_end": 204
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [
      {
        "title": "File read capability",
        "description": "The validation script reads YAML files from paths specified via command-line arguments. The script uses `yaml.safe_load()` which is safe. Code: `with open(assessment_file, 'r') as f: assessment = yaml.safe_load(f)` at lines 203-204. This is appropriate for the skill's purpose of validating assessments.",
        "locations": [
          {
            "file": "scripts/validate-assessment.py",
            "line_start": 203,
            "line_end": 204
          }
        ]
      }
    ],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 2048,
    "audit_model": "claude",
    "audited_at": "2026-01-10T09:35:53.601Z"
  },
  "content": {
    "user_title": "Create balanced programming assessments",
    "value_statement": "Creating fair programming assessments is time-consuming. This skill generates varied question types aligned to learning objectives with meaningful distractors, rubrics for open-ended questions, and cognitive distribution analysis.",
    "seo_keywords": [
      "assessment builder",
      "programming assessment",
      "question generator",
      "rubric creator",
      "Bloom's taxonomy",
      "MCQ distractors",
      "educational assessment",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Design varied question types including MCQ, code-completion, debugging, and projects",
      "Align questions to Bloom's taxonomy cognitive levels with 60%+ non-recall target",
      "Create meaningful MCQ distractors based on common student misconceptions",
      "Generate analytic rubrics with criteria, levels, and scoring guidance",
      "Validate assessment balance including cognitive distribution and question variety"
    ],
    "limitations": [
      "Does not execute or test student code submissions",
      "Does not grade assessments or provide automated scoring",
      "Does not integrate with learning management systems",
      "Does not store assessment data or track student progress"
    ],
    "use_cases": [
      {
        "target_user": "Educators",
        "title": "Create course exams",
        "description": "Design balanced programming exams aligned to learning objectives with varied question types and rubrics"
      },
      {
        "target_user": "Instructional designers",
        "title": "Build quiz banks",
        "description": "Develop MCQ item pools with diagnostic distractors and cognitive level analysis"
      },
      {
        "target_user": "Training teams",
        "title": "Assess skill mastery",
        "description": "Create project-based assessments with analytic rubrics for code quality evaluation"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic assessment",
        "scenario": "Create a simple quiz",
        "prompt": "Create a 5-question Python assessment for beginners covering functions. Include MCQ, code-tracing, and code-writing questions with Bloom's alignment."
      },
      {
        "title": "MCQ with distractors",
        "scenario": "Add diagnostic options",
        "prompt": "Generate 3 MCQs about list mutations. Each question should have 4 options with meaningful distractors based on common student misconceptions. Include distractor analysis."
      },
      {
        "title": "Complete rubric",
        "scenario": "Build evaluation criteria",
        "prompt": "Create an analytic rubric for a code-writing project that evaluates correctness, code quality, efficiency, and error handling. Include 4 performance levels for each criterion."
      },
      {
        "title": "Validate balance",
        "scenario": "Check assessment quality",
        "prompt": "Review this assessment for cognitive distribution. Check that 60%+ questions test application or higher. Identify any issues with question variety, missing rubrics, or unbalanced Bloom's levels."
      }
    ],
    "output_examples": [
      {
        "input": "Create a 5-question Python assessment on loops for beginners",
        "output": [
          "Q1 (MCQ, Remember): What keyword starts a loop in Python?",
          "Q2 (Code-tracing, Understand): Predict output of nested for loops",
          "Q3 (Code-completion, Apply): Fill in missing range() parameters",
          "Q4 (Debugging, Analyze): Find and fix infinite loop bug",
          "Q5 (Code-writing, Apply): Write a function using for loop to count vowels",
          "Cognitive distribution: 20% Remember, 40% Understand, 40% Apply ‚úì"
        ]
      }
    ],
    "best_practices": [
      "Start with learning objectives and define success criteria before creating questions",
      "Include at least 3 different question types to assess varied skills",
      "Ensure 60%+ questions test application or higher Bloom's levels",
      "Write distractors based on actual student misconceptions, not random wrong answers"
    ],
    "anti_patterns": [
      "Creating 10 identical MCQ types that only test memorization",
      "Writing distractors that are obviously wrong or trick questions",
      "Skipping rubrics for open-ended code-writing questions",
      "Failing to map questions back to specific learning objectives"
    ],
    "faq": [
      {
        "question": "Which question types are supported?",
        "answer": "MCQ, code-completion, code-tracing, debugging, code-writing, explanation, code-review, prediction, comparison, and project-based."
      },
      {
        "question": "What cognitive levels does it support?",
        "answer": "Bloom's taxonomy: Remember, Understand, Apply, Analyze, Evaluate, and Create. Target is 60%+ non-recall questions."
      },
      {
        "question": "Does it integrate with LMS platforms?",
        "answer": "No. This skill generates assessment content in markdown. Export to your LMS manually."
      },
      {
        "question": "Is student data handled securely?",
        "answer": "Yes. The skill only generates assessment content. No student data is collected or stored."
      },
      {
        "question": "Why are my MCQ distractors being flagged?",
        "answer": "Distractors must be based on common misconceptions. Obviously wrong options or trick answers reduce diagnostic value."
      },
      {
        "question": "How does this compare to other assessment tools?",
        "answer": "This skill focuses on balanced assessment design with cognitive alignment. It generates content, not auto-grading."
      }
    ]
  },
  "file_structure": [
    {
      "name": "reference",
      "type": "dir",
      "path": "reference",
      "children": [
        {
          "name": "blooms-assessment-alignment.md",
          "type": "file",
          "path": "reference/blooms-assessment-alignment.md"
        },
        {
          "name": "distractor-design.md",
          "type": "file",
          "path": "reference/distractor-design.md"
        },
        {
          "name": "question-types.md",
          "type": "file",
          "path": "reference/question-types.md"
        },
        {
          "name": "rubric-guidelines.md",
          "type": "file",
          "path": "reference/rubric-guidelines.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "validate-assessment.py",
          "type": "file",
          "path": "scripts/validate-assessment.py"
        }
      ]
    },
    {
      "name": "templates",
      "type": "dir",
      "path": "templates",
      "children": [
        {
          "name": "assessment-template.yml",
          "type": "file",
          "path": "templates/assessment-template.yml"
        },
        {
          "name": "rubric-template.yml",
          "type": "file",
          "path": "templates/rubric-template.yml"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
