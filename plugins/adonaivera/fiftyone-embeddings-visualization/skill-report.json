{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T10:00:33.767Z",
    "slug": "adonaivera-fiftyone-embeddings-visualization",
    "source_url": "https://github.com/AdonaiVera/fiftyone-skills/tree/main/embeddings-visualization/skills/fiftyone-embeddings-visualization",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "cd9859994100a4c2c50633d2a0fb90708c4ce543508b3019132cf46ac6657ff0",
    "tree_hash": "e8a989075c33c274567cb90a246a3b7dd63c602f1550e32fa4e975c1c0d22d62"
  },
  "skill": {
    "name": "fiftyone-embeddings-visualization",
    "description": "Visualize datasets in 2D using embeddings with UMAP or t-SNE dimensionality reduction. Use when users want to explore dataset structure, find clusters in images, identify outliers, color samples by class or metadata, or understand data distribution. Requires FiftyOne MCP server with @voxel51/brain plugin installed.",
    "summary": "Visualize datasets in 2D using embeddings with UMAP or t-SNE dimensionality reduction. Use when user...",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "AdonaiVera",
    "license": "Apache-2.0",
    "category": "data",
    "tags": [
      "embeddings",
      "visualization",
      "umap",
      "tsne",
      "fiftyone"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure prompt-based skill containing only documentation in SKILL.md format. No executable code, scripts, network operations, filesystem access, or external command execution is present. The skill provides instructions for using FiftyOne MCP server tools for embeddings visualization.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 531,
    "audit_model": "claude",
    "audited_at": "2026-01-10T10:00:33.767Z"
  },
  "content": {
    "user_title": "Visualize dataset embeddings in 2D",
    "value_statement": "Understanding complex image datasets requires seeing how samples relate in embedding space. This skill guides you through computing embeddings and using UMAP or t-SNE to create 2D visualizations that reveal clusters, outliers, and class distributions in your FiftyOne datasets.",
    "seo_keywords": [
      "fiftyone embeddings visualization",
      "UMAP t-SNE visualization",
      "Claude Code skill",
      "dataset cluster analysis",
      "image embeddings",
      "FiftyOne MCP",
      "outlier detection AI",
      "visualize CLIP embeddings",
      "data exploration skill",
      "AI dataset analysis"
    ],
    "actual_capabilities": [
      "Compute deep learning embeddings using CLIP, DINOv2, or ResNet models",
      "Generate 2D visualizations with UMAP, t-SNE, or PCA dimensionality reduction",
      "Color samples by class labels or metadata fields",
      "Identify outliers and anomalies using uniqueness scoring",
      "Find natural clusters in image datasets",
      "Filter and sort samples using embedding-based similarity"
    ],
    "limitations": [
      "Requires FiftyOne MCP server with @voxel51/brain plugin installed",
      "Large datasets (100K+ images) may take hours to process",
      "Visualization only works with image datasets that have embeddings",
      "Requires local FiftyOne App running to view embeddings panel"
    ],
    "use_cases": [
      {
        "target_user": "Machine learning engineers",
        "title": "Explore training data",
        "description": "Visualize class distributions and find mislabeled samples in image datasets before training."
      },
      {
        "target_user": "Data scientists",
        "title": "Analyze model predictions",
        "description": "Compare ground truth vs predictions in embedding space to identify confusion patterns."
      },
      {
        "target_user": "Researchers",
        "title": "Find dataset outliers",
        "description": "Detect anomalous or unique samples that may indicate data quality issues or rare cases."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic visualization",
        "scenario": "Explore dataset structure",
        "prompt": "Set context to my dataset and launch the app. Compute CLIP embeddings and create a UMAP visualization. Then tell me how to view it in the FiftyOne App embeddings panel."
      },
      {
        "title": "Find outliers",
        "scenario": "Detect anomalous samples",
        "prompt": "My dataset has CLIP embeddings already. Compute a visualization and then find the 50 most unique (outlier) samples. Show me how to identify them in the embeddings panel."
      },
      {
        "title": "Compare classes",
        "scenario": "Analyze class clustering",
        "prompt": "Generate a visualization for my classification dataset and color points by ground truth label. Describe what well-separated vs overlapping clusters indicate about class similarity."
      },
      {
        "title": "Publication plots",
        "scenario": "Create quality visualizations",
        "prompt": "Create a t-SNE visualization using DINOv2 embeddings for publication-quality plots. What are the advantages of t-SNE over UMAP for this use case?"
      }
    ],
    "output_examples": [
      {
        "input": "Visualize my coco-subset dataset in 2D",
        "output": [
          "Computing CLIP embeddings for 500 samples...",
          "Running UMAP dimensionality reduction...",
          "âœ“ Visualization ready (brain_key: 'exploration')",
          "â†’ Open http://localhost:5151/",
          "â†’ Click the Embeddings panel icon",
          "â†’ Select 'exploration' from the dropdown",
          "â†’ Use 'Color by' to select ground_truth.label"
        ]
      }
    ],
    "best_practices": [
      "Use CLIP embeddings for semantic similarity; use DINOv2 for visual similarity only",
      "Start with UMAP for faster exploration; switch to t-SNE for publication-quality local structure",
      "Compute uniqueness scores to find outliers more reliably than visual inspection"
    ],
    "anti_patterns": [
      "Skipping the launch_app() call - brain operators require the app executor",
      "Forgetting to compute embeddings before running compute_visualization",
      "Using the wrong brain_key between compute_similarity and compute_visualization"
    ],
    "faq": [
      {
        "question": "What models generate the best embeddings?",
        "answer": "CLIP models (clip-vit-base32-torch) work best for semantic similarity; DINOv2 excels at visual similarity only."
      },
      {
        "question": "How long does visualization take?",
        "answer": "1,000 images take ~2 minutes for embeddings and 30 seconds for UMAP. 10,000 images take ~15 minutes and 5 minutes respectively."
      },
      {
        "question": "Can I reuse embeddings for multiple visualizations?",
        "answer": "Yes, store embeddings in a named field (like clip_embeddings) and reference it across different brain_key operations."
      },
      {
        "question": "Is my data sent to external servers?",
        "answer": "No, all computation happens locally using FiftyOne and scikit-learn. No data leaves your machine."
      },
      {
        "question": "Why is my visualization slow?",
        "answer": "Use UMAP instead of t-SNE, faster models like MobileNet-V2, or process a subset first with set_view(limit=1000)."
      },
      {
        "question": "What is the difference between UMAP and t-SNE?",
        "answer": "UMAP is faster and preserves global structure better; t-SNE shows better local cluster detail but is slower and requires more memory."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
