{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T13:34:27.845Z",
    "slug": "dnyoussef-when-debugging-ml-training-use-ml-training-debugger",
    "source_url": "https://github.com/DNYoussef/ai-chrome-extension/tree/main/.claude/skills/machine-learning/when-debugging-ml-training-use-ml-training-debugger",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "c95ef2472d20ae73bd8bcae1c1bb7d1f40116585fe47089d58fe75872d382419",
    "tree_hash": "6ce094ec5e3ba118bcc0057e7d084d3e10751d689bf1ece20f894115ee325131"
  },
  "skill": {
    "name": "when-debugging-ml-training-use-ml-training-debugger",
    "description": "Debug ML training issues and optimize performance including loss divergence, overfitting, and slow convergence",
    "summary": "Debug ML training issues and optimize performance including loss divergence, overfitting, and slow c...",
    "icon": "ðŸ”§",
    "version": "1.0.0",
    "author": "DNYoussef",
    "license": "MIT",
    "category": "machine-learning",
    "tags": [
      "debugging",
      "ml",
      "training",
      "optimization",
      "troubleshooting"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Documentation-only skill containing example Python code snippets for ML training debugging. No executable scripts, network calls, filesystem access, or external commands. Pure educational content matching stated purpose.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 4,
    "total_lines": 716,
    "audit_model": "claude",
    "audited_at": "2026-01-10T13:34:27.845Z"
  },
  "content": {
    "user_title": "Debug ML training issues",
    "value_statement": "ML training can fail silently with issues like loss divergence, overfitting, and gradient problems. This skill provides a systematic 5-phase workflow to diagnose root causes, apply fixes, and validate improvements with proven techniques.",
    "seo_keywords": [
      "ml training debugger",
      "machine learning debugging",
      "loss divergence fix",
      "overfitting solution",
      "gradient clipping",
      "training optimization",
      "TensorFlow debugging",
      "PyTorch troubleshooting",
      "Claude",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Diagnose training issues (loss divergence, NaN, overfitting, slow convergence)",
      "Analyze root causes with gradient, data, and architecture review",
      "Apply fixes (learning rate adjustment, dropout, L2 regularization, gradient clipping)",
      "Validate fixes with before/after comparison and visualization",
      "Generate optimization recommendations for further improvement"
    ],
    "limitations": [
      "Requires training history data (JSON format) to be available",
      "Designed for TensorFlow/PyTorch models only",
      "Does not handle distributed training or multi-GPU scenarios",
      "Assumes access to compute resources for retraining"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Fix broken training runs",
        "description": "Diagnose why training loss exploded to NaN or stopped converging unexpectedly"
      },
      {
        "target_user": "Data Scientists",
        "title": "Reduce overfitting",
        "description": "Identify and fix models that perform well on training data but poorly on validation"
      },
      {
        "target_user": "Research Teams",
        "title": "Optimize model performance",
        "description": "Systematically improve training stability and convergence speed for deep learning projects"
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick diagnosis",
        "scenario": "Training failing",
        "prompt": "My model training loss is NaN after epoch 10. Use ml-training-debugger to diagnose and fix the issue."
      },
      {
        "title": "Overfitting fix",
        "scenario": "High validation loss",
        "prompt": "My model has 95% training accuracy but only 60% validation accuracy. Use ml-training-debugger to identify and fix the overfitting."
      },
      {
        "title": "Slow convergence",
        "scenario": "Training plateau",
        "prompt": "My training loss has not changed in 50 epochs. Use ml-training-debugger to analyze convergence rate and recommend fixes."
      },
      {
        "title": "Complete debugging",
        "scenario": "Multiple issues",
        "prompt": "Debug my training run with training_history.json. Diagnose all issues, apply fixes, and validate improvement with before/after comparison."
      }
    ],
    "output_examples": [
      {
        "input": "My CNN classifier has 98% training accuracy but 55% validation accuracy after 50 epochs. Help me fix this overfitting.",
        "output": [
          "Issue Identified: Overfitting (train >> val gap of 43%)",
          "Root Cause: Model too complex relative to data size, insufficient regularization",
          "Fixes Applied: Added L2 regularization (0.01), Dropout (0.3) after dense layers",
          "Before: Val Loss 2.1, Val Acc 55% | After: Val Loss 0.8, Val Acc 82%",
          "Improvement: 49% relative reduction in validation loss"
        ]
      }
    ],
    "best_practices": [
      "Save training history at every epoch to enable post-hoc diagnosis",
      "Monitor gradients during training to catch vanishing/exploding issues early",
      "Always validate fixes with a complete retraining run, not just a few iterations",
      "Use learning rate schedulers and early stopping callbacks as standard practice"
    ],
    "anti_patterns": [
      "Training without saving history or monitoring metrics",
      "Ignoring validation loss while tracking only training loss",
      "Making multiple changes simultaneously without isolating variables",
      "Training on insufficient data and expecting good generalization"
    ],
    "faq": [
      {
        "question": "Which ML frameworks does this skill support?",
        "answer": "TensorFlow/Keras and PyTorch are fully supported. The diagnostic patterns apply broadly to most deep learning frameworks."
      },
      {
        "question": "What training history format is required?",
        "answer": "The skill expects a JSON file with loss, val_loss, accuracy, and val_accuracy arrays for each training epoch."
      },
      {
        "question": "Can this skill fix gradient explosion in RNNs?",
        "answer": "Yes. The gradient analysis phase detects exploding gradients and applies gradient clipping (clipnorm=1.0) as a fix."
      },
      {
        "question": "Does this skill modify my model files?",
        "answer": "No. The skill generates a fixed_model.h5 file with corrected architecture. Your original model file remains unchanged."
      },
      {
        "question": "What if multiple issues are detected?",
        "answer": "The skill prioritizes issues by severity (CRITICAL > HIGH > MEDIUM > LOW) and applies fixes in that order systematically."
      },
      {
        "question": "How does this compare to TensorBoard debugging?",
        "answer": "This skill provides automated diagnosis and code-level fixes. TensorBoard provides visualization. Use both together for best results."
      }
    ]
  },
  "file_structure": [
    {
      "name": "process-diagram.gv",
      "type": "file",
      "path": "process-diagram.gv"
    },
    {
      "name": "PROCESS.md",
      "type": "file",
      "path": "PROCESS.md"
    },
    {
      "name": "README.md",
      "type": "file",
      "path": "README.md"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
