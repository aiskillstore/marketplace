{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T17:03:17.478Z",
    "slug": "k-dense-ai-dask",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/dask",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "919e5d190e85bb5f0ecd02fa626471e4ba7fbae079ca15d366664b4fbd556814",
    "tree_hash": "5f9747108f3040ff8beec949f1fec52064d91b6daf9dc6af65827207278e6bd6"
  },
  "skill": {
    "name": "dask",
    "description": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-file processing, task graphs, for larger-than-RAM datasets and parallel workflows.",
    "summary": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-...",
    "icon": "âš¡",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "BSD-3-Clause license",
    "category": "data",
    "tags": [
      "dask",
      "parallel-computing",
      "dataframes",
      "arrays",
      "distributed"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Documentation-only skill containing markdown files and JSON configuration. No executable code, scripts, network calls, or file system access. Pure knowledge base for Dask parallel computing library.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 9,
    "total_lines": 3159,
    "audit_model": "claude",
    "audited_at": "2026-01-04T17:03:17.478Z"
  },
  "content": {
    "user_title": "Scale data processing with Dask",
    "value_statement": "Pandas and NumPy struggle with datasets larger than available RAM. Dask provides parallel and distributed computing to scale existing workflows to terabyte-scale data using familiar APIs.",
    "seo_keywords": [
      "dask",
      "parallel computing",
      "distributed computing",
      "pandas",
      "numpy",
      "dataframes",
      "arrays",
      "Claude Code",
      "Claude",
      "Codex"
    ],
    "actual_capabilities": [
      "Process datasets larger than RAM using blocked algorithms and lazy evaluation",
      "Parallelize pandas operations across multiple DataFrame partitions",
      "Scale NumPy array computations with chunked processing",
      "Build custom parallel workflows using task graphs and futures",
      "Process unstructured data with Bags before converting to structured formats"
    ],
    "limitations": [
      "Not all pandas and NumPy functions are implemented",
      "Chunk size selection significantly impacts performance",
      "Requires understanding of lazy evaluation and task graphs",
      "Network overhead in distributed cluster mode"
    ],
    "use_cases": [
      {
        "target_user": "Data scientists",
        "title": "Scale pandas workflows",
        "description": "Process large tabular datasets that exceed available memory using familiar pandas-like syntax."
      },
      {
        "target_user": "ML engineers",
        "title": "Handle large arrays",
        "description": "Apply NumPy operations to arrays too large for memory using chunked blocked algorithms."
      },
      {
        "target_user": "Python developers",
        "title": "Build parallel pipelines",
        "description": "Create custom parallel workflows with task dependencies using Dask delayed and futures."
      }
    ],
    "prompt_templates": [
      {
        "title": "Load large CSV",
        "scenario": "Process multi-GB CSV files",
        "prompt": "Read all CSV files matching a pattern into a single Dask DataFrame and show the row count and column types."
      },
      {
        "title": "Parallel computation",
        "scenario": "Apply custom function",
        "prompt": "Apply a custom Python function to each partition of a Dask DataFrame efficiently using map_partitions."
      },
      {
        "title": "Aggregate large data",
        "scenario": "Group and summarize",
        "prompt": "Group a Dask DataFrame by a categorical column and compute multiple aggregations (sum, mean, count) efficiently."
      },
      {
        "title": "Distributed cluster",
        "scenario": "Scale to cluster",
        "prompt": "Set up a local Dask distributed cluster with specific worker resources and show how to submit tasks asynchronously."
      }
    ],
    "output_examples": [
      {
        "input": "Load a 10GB CSV file that exceeds available RAM and compute basic statistics",
        "output": [
          "Created Dask DataFrame with 50 partitions (~200MB each)",
          "Schema: timestamp (datetime64), category (string), value (float64)",
          "Mean value: 42.7 | Std: 15.3 | Rows: 150,000,000",
          "Computed in 2.3 minutes using 8 cores with threaded scheduler"
        ]
      }
    ],
    "best_practices": [
      "Load data directly with Dask methods instead of pandas to avoid loading entire dataset in memory",
      "Target approximately 100MB per chunk and ensure workers can hold 10 chunks per core",
      "Use map_partitions or map_blocks to fuse operations and reduce task graph size"
    ],
    "anti_patterns": [
      "Avoid calling compute() inside loops which prevents Dask from optimizing across operations",
      "Do not load data with pandas first and then convert to Dask - defeats parallelization",
      "Avoid row-wise apply operations which create excessive tasks - use map_partitions instead"
    ],
    "faq": [
      {
        "question": "What file formats does Dask support?",
        "answer": "Parquet (recommended), CSV, HDF5, Zarr, JSON, and text files. Parquet offers best compression and columnar access."
      },
      {
        "question": "What chunk size should I use?",
        "answer": "Target 100MB per chunk. Too large causes memory issues; too small creates excessive scheduling overhead."
      },
      {
        "question": "How does Dask integrate with existing code?",
        "answer": "Dask implements pandas and NumPy APIs. Replace import pandas as pd with dask.dataframe and code often works with .compute()."
      },
      {
        "question": "Is my data safe with Dask?",
        "answer": "Dask processes data locally or on your cluster. No data is sent to external services without explicit configuration."
      },
      {
        "question": "Why is my computation slow?",
        "answer": "Check for tiny tasks (increase chunk size), excessive compute() calls, or using threads with pure Python code (switch to processes)."
      },
      {
        "question": "How does Dask compare to Spark?",
        "answer": "Dask uses pure Python and integrates better with the scientific Python ecosystem. Spark offers more enterprise features but requires JVM."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "arrays.md",
          "type": "file",
          "path": "references/arrays.md"
        },
        {
          "name": "bags.md",
          "type": "file",
          "path": "references/bags.md"
        },
        {
          "name": "best-practices.md",
          "type": "file",
          "path": "references/best-practices.md"
        },
        {
          "name": "dataframes.md",
          "type": "file",
          "path": "references/dataframes.md"
        },
        {
          "name": "futures.md",
          "type": "file",
          "path": "references/futures.md"
        },
        {
          "name": "schedulers.md",
          "type": "file",
          "path": "references/schedulers.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
