{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T01:58:24.959Z",
    "slug": "k-dense-ai-dask",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/dask",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "dask",
    "description": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-file processing, task graphs, for larger-than-RAM datasets and parallel workflows.",
    "summary": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-...",
    "icon": "ðŸ§®",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "BSD-3-Clause license",
    "category": "data",
    "tags": [
      "dask",
      "parallel-computing",
      "dataframes",
      "arrays",
      "distributed"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill contains only documentation and examples. No code reads local credentials, accesses environment variables, or performs network calls. No malicious execution patterns are present.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 7,
    "total_lines": 3108,
    "audit_model": "codex",
    "audited_at": "2026-01-02T01:58:24.959Z"
  },
  "content": {
    "user_title": "Scale data processing with Dask guidance",
    "value_statement": "Large datasets and slow pandas workflows block progress. This skill explains how to scale with Dask collections, schedulers, and best practices.",
    "seo_keywords": [
      "Dask skill",
      "parallel pandas",
      "distributed computing",
      "Dask DataFrame",
      "Dask Array",
      "Claude",
      "Codex",
      "Claude Code",
      "data processing",
      "Python scaling"
    ],
    "actual_capabilities": [
      "Explain when to use Dask DataFrames, Arrays, Bags, or Futures",
      "Provide scheduler selection guidance for threads, processes, or distributed",
      "Offer chunking and partitioning strategies for performance",
      "Outline multi file ingestion patterns for CSV, Parquet, and JSON",
      "Share best practices for compute, persist, and task graph sizing"
    ],
    "limitations": [
      "Does not run code or benchmark performance",
      "Does not provision clusters or install packages",
      "Does not cover every NumPy or pandas API detail",
      "Requires user context for optimal chunk sizes"
    ],
    "use_cases": [
      {
        "target_user": "Data analyst",
        "title": "Scale pandas workflows",
        "description": "Plan a migration from pandas to Dask for larger than memory CSV analysis."
      },
      {
        "target_user": "Data engineer",
        "title": "Design parallel ETL",
        "description": "Choose Dask collections and scheduler settings for multi file ETL pipelines."
      },
      {
        "target_user": "Researcher",
        "title": "Process large arrays",
        "description": "Select chunk sizes and operations for large scientific arrays and HDF5 or Zarr data."
      }
    ],
    "prompt_templates": [
      {
        "title": "Pick a Dask tool",
        "scenario": "Beginner choosing collections",
        "prompt": "I have CSV files totaling 200 GB. Which Dask collection and scheduler should I start with and why?"
      },
      {
        "title": "Plan a DataFrame pipeline",
        "scenario": "Intermediate ETL design",
        "prompt": "Outline a Dask DataFrame ETL flow for filtering, joining, and aggregating daily parquet files."
      },
      {
        "title": "Chunking guidance",
        "scenario": "Intermediate array tuning",
        "prompt": "Suggest chunk sizes for a 3D float64 array and explain the tradeoffs for memory and speed."
      },
      {
        "title": "Distributed futures workflow",
        "scenario": "Advanced dynamic tasks",
        "prompt": "Design a Dask Futures workflow for parameter sweeps with shared inputs and progress monitoring."
      }
    ],
    "output_examples": [
      {
        "input": "Explain how to choose a Dask scheduler for log parsing and numeric aggregation.",
        "output": [
          "Use processes for log parsing to avoid GIL limits.",
          "Use threads for numeric aggregation on DataFrames.",
          "Switch to distributed if you need the dashboard or scale out."
        ]
      }
    ],
    "best_practices": [
      "Load data directly with Dask instead of pandas",
      "Minimize repeated compute calls by batching",
      "Target chunk sizes around 10 to 100 MB"
    ],
    "anti_patterns": [
      "Building millions of tiny tasks in one graph",
      "Converting large Dask objects to pandas early",
      "Using threads for pure Python text processing"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes, the guidance is platform neutral and works with Claude, Codex, and Claude Code."
      },
      {
        "question": "What data sizes does this support?",
        "answer": "It covers patterns from laptop scale to multi machine clusters, but you must size chunks for your system."
      },
      {
        "question": "Does it integrate with Dask Distributed or Dask-ML?",
        "answer": "Yes, it includes guidance for distributed clients and mentions Dask-ML integration points."
      },
      {
        "question": "Does it read or upload my data?",
        "answer": "No, it provides documentation only and does not access local files or networks."
      },
      {
        "question": "What if performance is still slow?",
        "answer": "Review chunk sizes, reduce task graph size, and choose a scheduler that matches your workload."
      },
      {
        "question": "How does this compare to pure pandas or NumPy?",
        "answer": "It focuses on scaling those APIs with parallelism and out of core execution when data exceeds memory."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "arrays.md",
          "type": "file",
          "path": "references/arrays.md"
        },
        {
          "name": "bags.md",
          "type": "file",
          "path": "references/bags.md"
        },
        {
          "name": "best-practices.md",
          "type": "file",
          "path": "references/best-practices.md"
        },
        {
          "name": "dataframes.md",
          "type": "file",
          "path": "references/dataframes.md"
        },
        {
          "name": "futures.md",
          "type": "file",
          "path": "references/futures.md"
        },
        {
          "name": "schedulers.md",
          "type": "file",
          "path": "references/schedulers.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
