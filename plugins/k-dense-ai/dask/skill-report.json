{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-03T04:45:08.005Z",
    "slug": "k-dense-ai-dask",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/dask",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "dask",
    "description": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-file processing, task graphs, for larger-than-RAM datasets and parallel workflows.",
    "summary": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-...",
    "icon": "ðŸ§®",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "BSD-3-Clause license",
    "category": "data",
    "tags": [
      "dask",
      "parallel-computing",
      "dataframes",
      "arrays",
      "distributed"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill contains documentation and examples only. No code executes commands, accesses the network, or reads local credentials or environment variables.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 9,
    "total_lines": 3342,
    "audit_model": "codex",
    "audited_at": "2026-01-03T04:45:08.004Z"
  },
  "content": {
    "user_title": "Scale data processing with Dask guidance",
    "value_statement": "Large datasets can exceed memory and slow pandas or NumPy workflows. This skill explains how to scale with Dask collections, schedulers, and best practices.",
    "seo_keywords": [
      "Dask skill",
      "parallel pandas",
      "distributed computing",
      "Dask DataFrame",
      "Dask Array",
      "Claude",
      "Codex",
      "Claude Code",
      "data processing",
      "Python scaling"
    ],
    "actual_capabilities": [
      "Explain when to use Dask DataFrames, Arrays, Bags, or Futures",
      "Provide scheduler selection guidance for threads, processes, and distributed",
      "Describe chunking and partitioning strategies for performance",
      "Outline multi file ingestion patterns for CSV, Parquet, and JSON",
      "Share best practices for compute calls and task graph sizing"
    ],
    "limitations": [
      "Does not run code or benchmark performance",
      "Does not provision clusters or install packages",
      "Does not cover every NumPy or pandas API detail",
      "Requires user context to choose optimal chunk sizes"
    ],
    "use_cases": [
      {
        "target_user": "Data analyst",
        "title": "Scale pandas workflows",
        "description": "Plan a migration from pandas to Dask for larger than memory CSV analysis."
      },
      {
        "target_user": "Data engineer",
        "title": "Design parallel ETL",
        "description": "Choose Dask collections and scheduler settings for multi file ETL pipelines."
      },
      {
        "target_user": "Researcher",
        "title": "Process large arrays",
        "description": "Select chunk sizes and operations for large scientific arrays and HDF5 or Zarr data."
      }
    ],
    "prompt_templates": [
      {
        "title": "Choose a Dask tool",
        "scenario": "Beginner choosing collections",
        "prompt": "I have CSV files totaling 200 GB. Which Dask collection and scheduler should I start with and why?"
      },
      {
        "title": "Plan a DataFrame pipeline",
        "scenario": "Intermediate ETL design",
        "prompt": "Outline a Dask DataFrame ETL flow for filtering, joining, and aggregating daily Parquet files."
      },
      {
        "title": "Tune chunk sizes",
        "scenario": "Intermediate array tuning",
        "prompt": "Suggest chunk sizes for a 3D float64 array and explain the tradeoffs for memory and speed."
      },
      {
        "title": "Design futures workflow",
        "scenario": "Advanced dynamic tasks",
        "prompt": "Design a Dask Futures workflow for parameter sweeps with shared inputs and progress monitoring."
      }
    ],
    "output_examples": [
      {
        "input": "Explain how to choose a Dask scheduler for log parsing and numeric aggregation.",
        "output": [
          "Use processes for log parsing to avoid GIL limits",
          "Use threads for numeric aggregation on DataFrames",
          "Switch to distributed if you need the dashboard or scale out"
        ]
      }
    ],
    "best_practices": [
      "Load data directly with Dask instead of pandas",
      "Batch compute calls to reduce overhead",
      "Target chunk sizes around 10 to 100 MB"
    ],
    "anti_patterns": [
      "Building millions of tiny tasks in one graph",
      "Converting large Dask objects to pandas early",
      "Using threads for pure Python text processing"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes, the guidance is platform neutral and works with Claude, Codex, and Claude Code."
      },
      {
        "question": "What data sizes does this support?",
        "answer": "It covers laptop scale to cluster scale, but you must size chunks for your system."
      },
      {
        "question": "Does it integrate with Dask Distributed or Dask-ML?",
        "answer": "It includes guidance for distributed clients and mentions integration points with Dask-ML."
      },
      {
        "question": "Does it read or upload my data?",
        "answer": "No, it provides documentation only and does not access local files or networks."
      },
      {
        "question": "What should I try if performance is slow?",
        "answer": "Review chunk sizes, reduce task graph size, and choose a scheduler that matches your workload."
      },
      {
        "question": "How does this compare to pandas or NumPy?",
        "answer": "It focuses on scaling those APIs with parallelism and out of core execution."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "arrays.md",
          "type": "file",
          "path": "references/arrays.md"
        },
        {
          "name": "bags.md",
          "type": "file",
          "path": "references/bags.md"
        },
        {
          "name": "best-practices.md",
          "type": "file",
          "path": "references/best-practices.md"
        },
        {
          "name": "dataframes.md",
          "type": "file",
          "path": "references/dataframes.md"
        },
        {
          "name": "futures.md",
          "type": "file",
          "path": "references/futures.md"
        },
        {
          "name": "schedulers.md",
          "type": "file",
          "path": "references/schedulers.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
