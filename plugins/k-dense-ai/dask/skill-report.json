{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T13:14:19.739Z",
    "slug": "k-dense-ai-dask",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/dask",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "919e5d190e85bb5f0ecd02fa626471e4ba7fbae079ca15d366664b4fbd556814",
    "tree_hash": "b5272f40c1478ae376b49d56db3f796826b9875c099b0eef683ae39c7ac1a635"
  },
  "skill": {
    "name": "dask",
    "description": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-file processing, task graphs, for larger-than-RAM datasets and parallel workflows.",
    "summary": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-...",
    "icon": "ðŸ§®",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "BSD-3-Clause license",
    "category": "data",
    "tags": [
      "dask",
      "parallel-computing",
      "dataframes",
      "arrays",
      "distributed"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "The skill contains documentation and examples only. No scripts, external commands, environment access, filesystem access, or network behavior were found.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 10,
    "total_lines": 3363,
    "audit_model": "codex",
    "audited_at": "2026-01-04T13:14:19.739Z"
  },
  "content": {
    "user_title": "Scale Dask workloads with clear guidance",
    "value_statement": "Large datasets can exceed memory and slow Python analytics. This skill provides clear guidance to use Dask collections, schedulers, and best practices.",
    "seo_keywords": [
      "Dask skill",
      "parallel pandas",
      "distributed computing",
      "Dask DataFrame",
      "Dask Array",
      "Dask Bag",
      "Claude",
      "Codex",
      "Claude Code",
      "Python scaling"
    ],
    "actual_capabilities": [
      "Explain when to use DataFrames, Arrays, Bags, or Futures",
      "Describe scheduler choices for threads, processes, and distributed",
      "Share chunking and partitioning strategies for performance",
      "Outline multi-file ingestion patterns for CSV, Parquet, and JSON",
      "Summarize common workflow patterns and best practices"
    ],
    "limitations": [
      "Does not run code or benchmark performance",
      "Does not provision clusters or install packages",
      "Does not access local files, network, or environment variables",
      "Needs user context to estimate optimal chunk sizes"
    ],
    "use_cases": [
      {
        "target_user": "Data analyst",
        "title": "Scale pandas workflows",
        "description": "Plan a migration from pandas to Dask for larger than memory CSV analysis."
      },
      {
        "target_user": "Data engineer",
        "title": "Design parallel ETL",
        "description": "Choose Dask collections and scheduler settings for multi-file ETL pipelines."
      },
      {
        "target_user": "Researcher",
        "title": "Process large arrays",
        "description": "Select chunk sizes and operations for large scientific arrays and Zarr or HDF5 data."
      }
    ],
    "prompt_templates": [
      {
        "title": "Choose a Dask tool",
        "scenario": "Beginner choosing collections",
        "prompt": "I have 200 GB of CSV files. Which Dask collection and scheduler should I start with and why?"
      },
      {
        "title": "Plan a DataFrame flow",
        "scenario": "Intermediate ETL design",
        "prompt": "Outline a Dask DataFrame pipeline for filtering, joining, and aggregating daily Parquet files."
      },
      {
        "title": "Tune chunk sizes",
        "scenario": "Intermediate array tuning",
        "prompt": "Suggest chunk sizes for a 3D float64 array and explain the memory and speed tradeoffs."
      },
      {
        "title": "Design futures workflow",
        "scenario": "Advanced dynamic tasks",
        "prompt": "Design a Dask Futures workflow for parameter sweeps with shared inputs and progress monitoring."
      }
    ],
    "output_examples": [
      {
        "input": "Explain how to choose a Dask scheduler for log parsing and numeric aggregation.",
        "output": [
          "Use processes for log parsing to reduce GIL contention",
          "Use threads for numeric aggregation on DataFrames",
          "Switch to distributed if you need a dashboard or scale out"
        ]
      }
    ],
    "best_practices": [
      "Load data directly with Dask instead of pandas",
      "Batch compute calls to reduce overhead",
      "Target chunk sizes around 10 to 100 MB"
    ],
    "anti_patterns": [
      "Building millions of tiny tasks in one graph",
      "Converting large Dask objects to pandas early",
      "Using threads for pure Python text processing"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes, the guidance is platform neutral and works with Claude, Codex, and Claude Code."
      },
      {
        "question": "What are the main limits of this skill?",
        "answer": "It provides guidance only and does not run code or measure performance."
      },
      {
        "question": "Does it cover Dask Distributed integrations?",
        "answer": "Yes, it explains distributed clients, schedulers, and monitoring."
      },
      {
        "question": "Does it read or upload my data?",
        "answer": "No, it provides documentation only and does not access local files or networks."
      },
      {
        "question": "What should I try if performance is slow?",
        "answer": "Review chunk sizes, reduce task graph size, and choose a scheduler that matches your workload."
      },
      {
        "question": "How does this compare to pandas or NumPy?",
        "answer": "It focuses on scaling those APIs with parallelism and out of core execution."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "arrays.md",
          "type": "file",
          "path": "references/arrays.md"
        },
        {
          "name": "bags.md",
          "type": "file",
          "path": "references/bags.md"
        },
        {
          "name": "best-practices.md",
          "type": "file",
          "path": "references/best-practices.md"
        },
        {
          "name": "dataframes.md",
          "type": "file",
          "path": "references/dataframes.md"
        },
        {
          "name": "futures.md",
          "type": "file",
          "path": "references/futures.md"
        },
        {
          "name": "schedulers.md",
          "type": "file",
          "path": "references/schedulers.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
