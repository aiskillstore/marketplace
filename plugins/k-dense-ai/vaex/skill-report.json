{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-05T16:44:50.425Z",
    "slug": "k-dense-ai-vaex",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/vaex",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "b633a0fbd57ba7849053aa61e7c878c315b292095134bb750304703a3106bcaa",
    "tree_hash": "ee68b12db136729c42bd4931ffce9bf59e6c50bd6c4ed2cd3e696d44b15981b7"
  },
  "skill": {
    "name": "vaex",
    "description": "Use this skill for processing and analyzing large tabular datasets (billions of rows) that exceed available RAM. Vaex excels at out-of-core DataFrame operations, lazy evaluation, fast aggregations, efficient visualization of big data, and machine learning on large datasets. Apply when users need to work with large CSV/HDF5/Arrow/Parquet files, perform fast statistics on massive datasets, create visualizations of big data, or build ML pipelines that don't fit in memory.",
    "summary": "Use this skill for processing and analyzing large tabular datasets (billions of rows) that exceed av...",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "K-Dense-AI",
    "license": "MIT license",
    "category": "data",
    "tags": [
      "dataframes",
      "big-data",
      "visualization",
      "machine-learning",
      "python"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill with no executable code. Contains only reference materials for the Vaex Python library. No security risks identified.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 7,
    "total_lines": 3984,
    "audit_model": "claude",
    "audited_at": "2026-01-05T16:44:50.425Z"
  },
  "content": {
    "user_title": "Analyze Billion-Row Datasets with Vaex",
    "value_statement": "Process massive datasets that don't fit in memory using Vaex's out-of-core DataFrame operations. Get instant results on billion-row files without expensive hardware upgrades.",
    "seo_keywords": [
      "vaex",
      "big data",
      "out-of-core",
      "dataframes",
      "billion rows",
      "Claude",
      "Codex",
      "Claude Code",
      "large datasets",
      "data analysis"
    ],
    "actual_capabilities": [
      "Processes billion-row datasets exceeding RAM limits",
      "Provides lazy evaluation for memory-efficient operations",
      "Creates visualizations of massive datasets without sampling",
      "Integrates with scikit-learn, XGBoost, and other ML frameworks",
      "Converts between CSV, HDF5, Arrow, and Parquet formats",
      "Performs fast statistical aggregations on large data"
    ],
    "limitations": [
      "Requires Vaex library installation in Python environment",
      "HDF5/Arrow formats recommended for optimal performance",
      "Some pandas operations may not be available",
      "Complex joins may require data restructuring"
    ],
    "use_cases": [
      {
        "target_user": "Financial Analysts",
        "title": "Analyze Trading Data at Scale",
        "description": "Process years of tick-by-tick trading data to identify patterns, calculate risk metrics, and build predictive models without memory constraints."
      },
      {
        "target_user": "Scientific Researchers",
        "title": "Explore Astronomical Datasets",
        "description": "Visualize and analyze telescope observations with billions of celestial objects to discover patterns in the universe."
      },
      {
        "target_user": "Business Intelligence Teams",
        "title": "Process Customer Analytics",
        "description": "Analyze petabytes of customer interaction data to extract insights, segment users, and optimize business strategies."
      }
    ],
    "prompt_templates": [
      {
        "title": "Load and Explore Large Data",
        "scenario": "First-time Vaex user with big data",
        "prompt": "I have a 50GB CSV file with customer transaction data. Show me how to load it efficiently with Vaex and get a quick overview of the dataset structure and statistics."
      },
      {
        "title": "Optimize Performance",
        "scenario": "Slow pandas operations on large data",
        "prompt": "My pandas code is running out of memory on a 20GB dataset. Convert this aggregation operation to Vaex and show me performance optimization techniques: df.groupby('category')['revenue'].sum()"
      },
      {
        "title": "Create Big Data Visualizations",
        "scenario": "Visualizing millions of data points",
        "prompt": "I need to create a heatmap of GPS coordinates from 10 million location records. Show me how to use Vaex visualization for this large dataset without sampling."
      },
      {
        "title": "Build ML Pipeline at Scale",
        "scenario": "Machine learning on large datasets",
        "prompt": "I want to build a random forest model on a dataset with 100 million rows and 50 features. Show me the complete Vaex ML pipeline including preprocessing, training, and evaluation."
      }
    ],
    "output_examples": [
      {
        "input": "Load a large HDF5 file and show basic statistics",
        "output": [
          "Dataset: 2.3 billion rows Ã— 15 columns",
          "Memory usage: 0 GB (memory-mapped)",
          "File size: 127 GB on disk",
          "Columns: customer_id, transaction_date, amount, category, location",
          "Amount statistics: mean=$156.42, std=$892.31, min=$0.01, max=$125,432.00",
          "Loading time: 0.8 seconds"
        ]
      }
    ],
    "best_practices": [
      "Convert large CSV files to HDF5 format for instant future loading",
      "Use virtual columns instead of materializing data to save memory",
      "Batch multiple operations with delay=True for single-pass efficiency",
      "Apply percentile-based limits (99%) for visualization to handle outliers"
    ],
    "anti_patterns": [
      "Don't use .to_pandas_df() on large datasets - defeats Vaex's purpose",
      "Avoid materializing virtual columns unless necessary for export",
      "Don't iterate row-by-row; use vectorized operations instead",
      "Never process billion-row CSV files directly - convert to HDF5 first"
    ],
    "faq": [
      {
        "question": "Is Vaex compatible with my existing pandas code?",
        "answer": "Vaex has similar API to pandas but handles large data differently. Most operations have direct equivalents, and you can convert small results to pandas when needed."
      },
      {
        "question": "What's the maximum dataset size Vaex can handle?",
        "answer": "Vaex can handle datasets larger than your available RAM and even larger than your local disk by streaming data efficiently. There's no theoretical limit."
      },
      {
        "question": "Can I use Vaex with my existing machine learning tools?",
        "answer": "Yes, Vaex integrates seamlessly with scikit-learn, XGBoost, LightGBM, CatBoost, and Keras through built-in wrappers and transformers."
      },
      {
        "question": "How does Vaex achieve such fast performance?",
        "answer": "Vaex uses memory-mapping, lazy evaluation, and an optimized C++ backend that processes data in parallel across all CPU cores without loading it into RAM."
      },
      {
        "question": "What file formats work best with Vaex?",
        "answer": "HDF5 and Apache Arrow provide instant loading and best performance. Parquet works well for distributed systems. Convert CSV files to HDF5 for repeated use."
      },
      {
        "question": "Can Vaex replace pandas completely?",
        "answer": "For large datasets, Vaex is superior to pandas. For small datasets or specific operations, pandas may still be useful. They complement each other well."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "core_dataframes.md",
          "type": "file",
          "path": "references/core_dataframes.md"
        },
        {
          "name": "data_processing.md",
          "type": "file",
          "path": "references/data_processing.md"
        },
        {
          "name": "io_operations.md",
          "type": "file",
          "path": "references/io_operations.md"
        },
        {
          "name": "machine_learning.md",
          "type": "file",
          "path": "references/machine_learning.md"
        },
        {
          "name": "performance.md",
          "type": "file",
          "path": "references/performance.md"
        },
        {
          "name": "visualization.md",
          "type": "file",
          "path": "references/visualization.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
