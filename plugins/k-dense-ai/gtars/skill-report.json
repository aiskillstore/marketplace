{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T02:34:13.145Z",
    "slug": "K-Dense-AI-gtars",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/gtars",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "gtars",
    "description": "High-performance toolkit for genomic interval analysis in Rust with Python bindings. Use when working with genomic regions, BED files, coverage tracks, overlap detection, tokenization for ML models, or fragment analysis in computational genomics and machine learning applications.",
    "summary": "High-performance toolkit for genomic interval analysis in Rust with Python bindings. Use when workin...",
    "icon": "ðŸ§¬",
    "version": "0.1",
    "author": "K-Dense Inc.",
    "license": "Unknown",
    "category": "data",
    "tags": [
      "genomics",
      "intervals",
      "coverage",
      "tokenization",
      "rust"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No data theft, exfiltration, or malicious execution patterns detected. Content is documentation and usage examples only. Behavior aligns with genomic analysis purpose.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 7,
    "total_lines": 1293,
    "audit_model": "codex",
    "audited_at": "2026-01-02T02:34:13.144Z"
  },
  "content": {
    "user_title": "Analyze genomic intervals with gtars",
    "value_statement": "You need fast overlap, coverage, and tokenization workflows for genomic intervals. This skill guides you through gtars Python and CLI tasks with clear examples.",
    "seo_keywords": [
      "gtars",
      "genomic intervals",
      "BED analysis",
      "coverage tracks",
      "overlap detection",
      "tokenization",
      "Claude",
      "Codex",
      "Claude Code",
      "Rust genomics"
    ],
    "actual_capabilities": [
      "Build and query IGD indexes for overlap detection",
      "Generate WIG or BigWig coverage tracks with uniwig",
      "Tokenize genomic regions for machine learning workflows",
      "Compute refget digests and extract reference subsequences",
      "Split and score fragment files using CLI commands"
    ],
    "limitations": [
      "Requires gtars installed in your environment",
      "Works with genomic formats like BED, WIG, BigWig, FASTA, and TSV",
      "Does not provide visualization or genome browser rendering",
      "No cloud upload or remote storage features are documented"
    ],
    "use_cases": [
      {
        "target_user": "Bioinformatician",
        "title": "Overlap analysis",
        "description": "Compare peak sets and identify overlapping regulatory regions using IGD and overlaprs."
      },
      {
        "target_user": "Genomics engineer",
        "title": "Coverage pipeline",
        "description": "Generate BigWig coverage tracks for ATAC-seq or ChIP-seq workflows."
      },
      {
        "target_user": "ML researcher",
        "title": "Genomic tokenization",
        "description": "Create tokenized inputs for genomic transformer models from BED regions."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick overlap check",
        "scenario": "You have two BED files and need overlaps",
        "prompt": "Show Python steps to find overlaps between regions_a.bed and regions_b.bed, then save overlaps to a BED file."
      },
      {
        "title": "Coverage track run",
        "scenario": "You want a BigWig coverage file",
        "prompt": "Provide the gtars CLI commands to create a BigWig coverage track from fragments.bed at resolution 10."
      },
      {
        "title": "Tokenizer setup",
        "scenario": "You need tokens for ML training",
        "prompt": "Explain how to build a TreeTokenizer from training_peaks.bed and tokenize chr1:1000-2000."
      },
      {
        "title": "Reference validation",
        "scenario": "You must verify genome digests",
        "prompt": "Give a Python example to load hg38.fa, compute refget digests, and verify chr1 against an expected digest."
      }
    ],
    "output_examples": [
      {
        "input": "Generate overlaps between chip_peaks.bed and promoters.bed and save results.",
        "output": [
          "Load both BED files into RegionSet objects",
          "Run overlap filtering to keep peaks in promoters",
          "Write the overlapping peaks to peaks_in_promoters.bed"
        ]
      }
    ],
    "best_practices": [
      "Use parallel processing and memory-mapped options for large BED files",
      "Prefer BigWig output for large coverage tracks",
      "Pre-sort regions before heavy overlap operations"
    ],
    "anti_patterns": [
      "Tokenizing without a consistent training region set",
      "Generating WIG for very large datasets without BigWig output",
      "Running overlap queries without building an IGD index"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes, prompts and workflows are plain text and work across those platforms."
      },
      {
        "question": "What data sizes can it handle?",
        "answer": "It is designed for large genomic datasets with streaming and parallel options."
      },
      {
        "question": "Can it integrate with my Python pipeline?",
        "answer": "Yes, the Python API supports RegionSet operations and NumPy integration."
      },
      {
        "question": "Does it access or send my data externally?",
        "answer": "No external network behavior is documented in the skill content."
      },
      {
        "question": "What if commands fail or files are missing?",
        "answer": "Enable verbose logging and check input formats and file paths."
      },
      {
        "question": "How does it compare to basic BED tools?",
        "answer": "It adds Rust performance, IGD indexing, and ML tokenization features."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "cli.md",
          "type": "file",
          "path": "references/cli.md"
        },
        {
          "name": "coverage.md",
          "type": "file",
          "path": "references/coverage.md"
        },
        {
          "name": "overlap.md",
          "type": "file",
          "path": "references/overlap.md"
        },
        {
          "name": "python-api.md",
          "type": "file",
          "path": "references/python-api.md"
        },
        {
          "name": "refget.md",
          "type": "file",
          "path": "references/refget.md"
        },
        {
          "name": "tokenizers.md",
          "type": "file",
          "path": "references/tokenizers.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}