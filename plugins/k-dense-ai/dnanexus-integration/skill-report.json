{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T02:07:54.887Z",
    "slug": "k-dense-ai-dnanexus-integration",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/dnanexus-integration",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community"
  },
  "skill": {
    "name": "dnanexus-integration",
    "description": "DNAnexus cloud genomics platform. Build apps/applets, manage data (upload/download), dxpy Python SDK, run workflows, FASTQ/BAM/VCF, for genomics pipeline development and execution.",
    "summary": "DNAnexus cloud genomics platform. Build apps/applets, manage data (upload/download), dxpy Python SDK...",
    "icon": "ðŸ§¬",
    "version": "unknown",
    "author": "K-Dense Inc.",
    "license": "Unknown",
    "category": "research",
    "tags": [
      "genomics",
      "dnanexus",
      "dxpy",
      "bioinformatics",
      "workflows"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No credential harvesting, exfiltration, or hidden network behavior detected. Content is documentation and example usage aligned with DNAnexus platform tasks.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 6,
    "total_lines": 2608,
    "audit_model": "codex",
    "audited_at": "2026-01-02T02:07:54.887Z"
  },
  "content": {
    "user_title": "Build DNAnexus genomics workflows",
    "value_statement": "Genomics apps and data workflows can be complex to set up. This skill guides you through DNAnexus app builds, data handling, and job execution.",
    "seo_keywords": [
      "DNAnexus",
      "dxpy",
      "genomics workflows",
      "bioinformatics pipelines",
      "applet development",
      "data management",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Create and configure DNAnexus apps and applets with dxapp.json",
      "Upload, download, and search DNAnexus files and records using dxpy",
      "Run applets, apps, and workflows and monitor job states",
      "Define dependencies, assets, and Docker usage for apps",
      "Automate data organization with metadata, tags, and folders"
    ],
    "limitations": [
      "Requires an active DNAnexus account and access permissions",
      "Examples assume dxpy and dx toolkit are installed",
      "Cannot execute or validate DNAnexus jobs without platform access",
      "Does not cover non DNAnexus cloud providers"
    ],
    "use_cases": [
      {
        "target_user": "Bioinformatics engineer",
        "title": "Pipeline deployment",
        "description": "Build and deploy alignment or variant calling applets with correct inputs and outputs."
      },
      {
        "target_user": "Research analyst",
        "title": "Data organization",
        "description": "Upload, tag, and structure FASTQ, BAM, or VCF files for analysis."
      },
      {
        "target_user": "Platform operator",
        "title": "Workflow orchestration",
        "description": "Design and run multi step workflows with job monitoring and retries."
      }
    ],
    "prompt_templates": [
      {
        "title": "Start with dxpy",
        "scenario": "First time DNAnexus user",
        "prompt": "Show how to authenticate and upload a FASTQ file with dxpy, then download it by ID."
      },
      {
        "title": "Build an applet",
        "scenario": "Simple pipeline step",
        "prompt": "Draft a minimal dxapp.json and Python entry point for a FASTQ quality filter applet."
      },
      {
        "title": "Run a workflow",
        "scenario": "Multi step analysis",
        "prompt": "Explain how to chain two applets using output references and monitor job states."
      },
      {
        "title": "Scale batch runs",
        "scenario": "Process many samples",
        "prompt": "Provide a dxpy pattern to find all BAM files and launch parallel jobs with tags."
      }
    ],
    "output_examples": [
      {
        "input": "Create a checklist for running a DNAnexus applet on new FASTQ data",
        "output": [
          "Confirm dx login and project access",
          "Upload FASTQ files to a project folder",
          "Run the applet with dxpy or dx run",
          "Track job state and review logs",
          "Download outputs and verify results"
        ]
      }
    ],
    "best_practices": [
      "Use dxpy high level functions and handle errors with try except",
      "Choose instance types and timeouts that fit data size and runtime",
      "Store metadata and tags for search and reproducibility"
    ],
    "anti_patterns": [
      "Hardcoding API tokens in scripts or dxapp.json",
      "Running large jobs without validating small test inputs",
      "Requesting broad network access without a clear need"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes. The guidance is tool agnostic and works with Claude, Codex, and Claude Code."
      },
      {
        "question": "What are the main limits?",
        "answer": "You need DNAnexus access and cannot run jobs without platform credentials and permissions."
      },
      {
        "question": "How does it integrate with existing pipelines?",
        "answer": "It uses dxpy and dxapp.json to wrap or connect pipeline steps on DNAnexus."
      },
      {
        "question": "Is my data safe?",
        "answer": "The skill does not collect data. DNAnexus access and data controls are managed by your account."
      },
      {
        "question": "What if a job fails?",
        "answer": "Check job logs, verify inputs are closed, and retry with adjusted resources or inputs."
      },
      {
        "question": "How does it compare to generic cloud tools?",
        "answer": "It targets DNAnexus APIs and workflows, rather than general cloud storage or compute services."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "app-development.md",
          "type": "file",
          "path": "references/app-development.md"
        },
        {
          "name": "configuration.md",
          "type": "file",
          "path": "references/configuration.md"
        },
        {
          "name": "data-operations.md",
          "type": "file",
          "path": "references/data-operations.md"
        },
        {
          "name": "job-execution.md",
          "type": "file",
          "path": "references/job-execution.md"
        },
        {
          "name": "python-sdk.md",
          "type": "file",
          "path": "references/python-sdk.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
