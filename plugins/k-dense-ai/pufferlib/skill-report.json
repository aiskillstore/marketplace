{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T16:24:48.338Z",
    "slug": "k-dense-ai-pufferlib",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/pufferlib",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "5c288fbd3dc6fa34d2ff94f5ac1f2601ccda21a4ba34c6399e8bdffe77dcb900",
    "tree_hash": "1c70caa68af38d0642c2f9e034bd801388e48e97642a69537d6aede707a51c63"
  },
  "skill": {
    "name": "pufferlib",
    "description": "This skill should be used when working with reinforcement learning tasks including high-performance RL training, custom environment development, vectorized parallel simulation, multi-agent systems, or integration with existing RL environments (Gymnasium, PettingZoo, Atari, Procgen, etc.). Use this skill for implementing PPO training, creating PufferEnv environments, optimizing RL performance, or developing policies with CNNs/LSTMs.",
    "summary": "This skill should be used when working with reinforcement learning tasks including high-performance ...",
    "icon": "ðŸš€",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "reinforcement-learning",
      "pytorch",
      "vectorization",
      "multi-agent",
      "ppo"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill contains documentation and template scripts for reinforcement learning. No credential harvesting, data exfiltration, code obfuscation, or malicious patterns were found. All capabilities (training loops, checkpointing, logging) are standard ML operations that align with the stated RL training purpose.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/train_template.py",
            "line_start": 1,
            "line_end": 240
          },
          {
            "file": "scripts/env_template.py",
            "line_start": 1,
            "line_end": 341
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "scripts/train_template.py",
            "line_start": 95,
            "line_end": 108
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/train_template.py",
            "line_start": 231,
            "line_end": 232
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 11,
    "total_lines": 4500,
    "audit_model": "claude",
    "audited_at": "2026-01-04T16:24:48.338Z"
  },
  "content": {
    "user_title": "Build fast RL training with PufferLib",
    "value_statement": "Training RL agents at scale is slow and complex. This skill provides templates and guidance for fast PPO training, vectorization, and custom environments using PufferLib.",
    "seo_keywords": [
      "PufferLib",
      "reinforcement learning",
      "PPO training",
      "vectorized environments",
      "multi-agent RL",
      "Gymnasium integration",
      "Claude",
      "Codex",
      "Claude Code",
      "PufferEnv"
    ],
    "actual_capabilities": [
      "Provides PPO training script template with PuffeRL, logging, and checkpointing",
      "Defines PyTorch policy template with actor and critic heads using layer_init",
      "Includes single-agent and multi-agent PufferEnv templates with space definitions",
      "Documents vectorized environment creation with pufferlib.make and emulate",
      "Lists integration patterns for Gymnasium and PettingZoo environments",
      "Explains distributed training and hyperparameter tuning workflows"
    ],
    "limitations": [
      "Templates require customization for specific environment logic and reward functions",
      "External loggers need separate accounts and API tokens",
      "Performance depends on hardware and vectorization configuration",
      "No built-in evaluation suite beyond training loop examples"
    ],
    "use_cases": [
      {
        "target_user": "RL researcher",
        "title": "Prototype PPO baselines",
        "description": "Start reproducible PPO training with vectorized environments, logging, and checkpointing."
      },
      {
        "target_user": "Game AI developer",
        "title": "Create custom environments",
        "description": "Build single or multi-agent PufferEnv tasks with defined observation and action spaces."
      },
      {
        "target_user": "ML engineer",
        "title": "Scale training throughput",
        "description": "Configure workers and environment counts to maximize steps per second."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick start PPO",
        "scenario": "Beginner setup",
        "prompt": "Create a minimal training plan using train_template.py for procgen-coinrun with CPU settings and basic hyperparameters."
      },
      {
        "title": "Custom environment",
        "scenario": "Single-agent task",
        "prompt": "Outline a PufferEnv skeleton for a grid world with discrete actions and vector observations."
      },
      {
        "title": "Multi-agent design",
        "scenario": "Parallel agents",
        "prompt": "Describe a multi-agent PufferEnv structure with per-agent observations and shared policy."
      },
      {
        "title": "Performance tuning",
        "scenario": "Advanced scaling",
        "prompt": "Recommend vectorization settings for 1024 environments across 16 workers and explain tradeoffs."
      }
    ],
    "output_examples": [
      {
        "input": "Help me set up PPO training for gym CartPole with WandB logging.",
        "output": [
          "Use the training template with env name gym-CartPole-v1",
          "Set num_envs to 64 for initial CPU testing",
          "Choose wandb logger and configure project name",
          "Start with default PPO hyperparameters (lr=3e-4, batch=32768)",
          "Run train.py and monitor metrics in WandB dashboard"
        ]
      }
    ],
    "best_practices": [
      "Start with template scripts and change one parameter at a time to isolate effects",
      "Profile steps per second before optimizing architecture or environment code",
      "Validate environment reset and step outputs with assertions before scaling"
    ],
    "anti_patterns": [
      "Copying templates without adjusting environment-specific observation and action spaces",
      "Scaling num_envs without sufficient CPU cores or memory for parallel simulation",
      "Ignoring reward scaling and done condition semantics when vectorizing environments"
    ],
    "faq": [
      {
        "question": "Is this compatible with Gymnasium and PettingZoo?",
        "answer": "Yes. The integration guide documents patterns for wrapping Gymnasium and PettingZoo environments."
      },
      {
        "question": "What are the limits of the template scripts?",
        "answer": "They are starting templates. Customize observation/action spaces, rewards, and network architecture for your task."
      },
      {
        "question": "Can I integrate external experiment loggers?",
        "answer": "Yes. The training template shows Weights & Biases and Neptune logger integrations."
      },
      {
        "question": "Does this skill access my data or credentials?",
        "answer": "No. Templates run locally. External loggers require user-provided API tokens."
      },
      {
        "question": "What if training throughput is lower than expected?",
        "answer": "Reduce num_envs to profile baseline. Then tune workers, batch size, and device settings systematically."
      },
      {
        "question": "How does PufferLib compare to standard Gymnasium vectorization?",
        "answer": "PufferLib uses shared memory and async workers for higher throughput at scale."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "environments.md",
          "type": "file",
          "path": "references/environments.md"
        },
        {
          "name": "integration.md",
          "type": "file",
          "path": "references/integration.md"
        },
        {
          "name": "policies.md",
          "type": "file",
          "path": "references/policies.md"
        },
        {
          "name": "training.md",
          "type": "file",
          "path": "references/training.md"
        },
        {
          "name": "vectorization.md",
          "type": "file",
          "path": "references/vectorization.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "env_template.py",
          "type": "file",
          "path": "scripts/env_template.py"
        },
        {
          "name": "train_template.py",
          "type": "file",
          "path": "scripts/train_template.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
