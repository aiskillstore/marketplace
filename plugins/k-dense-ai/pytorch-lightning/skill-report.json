{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T16:38:51.427Z",
    "slug": "k-dense-ai-pytorch-lightning",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/pytorch-lightning",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "f0580a650082569c27228e4fc5e23a3500495554c3fcb7f9fe1b1e01f2df39c7",
    "tree_hash": "56dc2ffb88ff81839a081d5450b23f787e00b10bdd9bfab6f6ad2d84ba64c527"
  },
  "skill": {
    "name": "pytorch-lightning",
    "description": "Deep learning framework (PyTorch Lightning). Organize PyTorch code into LightningModules, configure Trainers for multi-GPU/TPU, implement data pipelines, callbacks, logging (W&B, TensorBoard), distributed training (DDP, FSDP, DeepSpeed), for scalable neural network training.",
    "summary": "Deep learning framework (PyTorch Lightning). Organize PyTorch code into LightningModules, configure ...",
    "icon": "âš¡",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "Apache-2.0 license",
    "category": "research",
    "tags": [
      "pytorch",
      "lightning",
      "training",
      "distributed",
      "deep-learning"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "All files are static templates and documentation for PyTorch Lightning. No data exfiltration, credential theft, or malicious execution patterns detected. Code contains only educational templates and reference documentation.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/quick_trainer_setup.py",
            "line_start": 1,
            "line_end": 455
          },
          {
            "file": "scripts/template_datamodule.py",
            "line_start": 1,
            "line_end": 329
          },
          {
            "file": "scripts/template_lightning_module.py",
            "line_start": 1,
            "line_end": 220
          }
        ]
      },
      {
        "factor": "network",
        "evidence": [
          {
            "file": "references/logging.md",
            "line_start": 1,
            "line_end": 655
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 14,
    "total_lines": 6745,
    "audit_model": "claude",
    "audited_at": "2026-01-04T16:38:51.427Z"
  },
  "content": {
    "user_title": "Build PyTorch Lightning training workflows",
    "value_statement": "Complex training setups slow research and create boilerplate. This skill provides clear templates and guidance to build scalable Lightning projects quickly.",
    "seo_keywords": [
      "PyTorch Lightning",
      "LightningModule",
      "Trainer setup",
      "distributed training",
      "FSDP",
      "DeepSpeed",
      "Claude",
      "Codex",
      "Claude Code"
    ],
    "actual_capabilities": [
      "Provides LightningModule boilerplate with training_step, validation_step, and optimizer hooks",
      "Offers LightningDataModule templates for data preparation and DataLoader configuration",
      "Documents Trainer configurations for single GPU, multi GPU, FSDP, and DeepSpeed",
      "Explains callback implementation and logging integrations with TensorBoard, W&B, MLflow",
      "Describes distributed training strategies and mixed precision setup"
    ],
    "limitations": [
      "Does not run training or execute model code",
      "Does not install dependencies or configure environments",
      "Does not generate custom neural network architectures",
      "Does not validate hardware availability"
    ],
    "use_cases": [
      {
        "target_user": "Research engineer",
        "title": "Prototype a new model",
        "description": "Start with a LightningModule template and basic Trainer to test ideas quickly."
      },
      {
        "target_user": "ML engineer",
        "title": "Scale to multi GPU",
        "description": "Select DDP or FSDP configurations and apply best practices for distributed runs."
      },
      {
        "target_user": "Experiment owner",
        "title": "Add logging and callbacks",
        "description": "Integrate TensorBoard or W&B and add checkpoints and early stopping."
      }
    ],
    "prompt_templates": [
      {
        "title": "Start a module",
        "scenario": "Create a minimal LightningModule",
        "prompt": "Create a LightningModule template with training_step, validation_step, and configure_optimizers for a classification model."
      },
      {
        "title": "Add data module",
        "scenario": "Organize data loading",
        "prompt": "Draft a LightningDataModule with prepare_data, setup, train_dataloader, and val_dataloader for an image dataset."
      },
      {
        "title": "Configure Trainer",
        "scenario": "Set up multi GPU training",
        "prompt": "Suggest a Trainer configuration for 4 GPUs using DDP with checkpointing and learning rate monitoring."
      },
      {
        "title": "Scale large model",
        "scenario": "Train a large transformer",
        "prompt": "Provide an FSDP Trainer setup with activation checkpointing and mixed precision for a large transformer model."
      }
    ],
    "output_examples": [
      {
        "input": "Set up a production single GPU Trainer with checkpoints and TensorBoard logging",
        "output": [
          "Trainer uses GPU acceleration with mixed precision (16-mixed)",
          "Checkpointing saves best and last models to chosen directory",
          "TensorBoard logger writes runs under named experiment folder",
          "Includes early stopping and learning rate monitoring callbacks",
          "Gradient clipping configured at value 1.0"
        ]
      }
    ],
    "best_practices": [
      "Use save_hyperparameters and self.log for consistent tracking",
      "Start with single GPU and fast_dev_run before scaling to multi GPU",
      "Choose DDP for models under 500M parameters and FSDP for larger models"
    ],
    "anti_patterns": [
      "Manually invoking callback methods outside the Trainer lifecycle",
      "Hard coding CUDA device calls instead of using self.device",
      "Scaling to multi GPU without using sync_dist for metric aggregation"
    ],
    "faq": [
      {
        "question": "Is this skill compatible with current PyTorch Lightning versions?",
        "answer": "It targets modern Lightning APIs but verify version-specific arguments in your environment."
      },
      {
        "question": "What are the limits of this skill?",
        "answer": "It provides templates and guidance only, and does not run training or install dependencies."
      },
      {
        "question": "How do I integrate it with my existing project?",
        "answer": "Copy the templates into your codebase and adapt Trainer and module settings to your dataset."
      },
      {
        "question": "Does this skill access or send my data externally?",
        "answer": "No. The files are static templates and documentation with no data access or network logic."
      },
      {
        "question": "What should I do if training fails?",
        "answer": "Start with debug Trainer settings, reduce batch size, and verify hardware and precision settings."
      },
      {
        "question": "How does this skill compare to a full tutorial?",
        "answer": "It is shorter and template-focused, designed for fast setup rather than full conceptual coverage."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "best_practices.md",
          "type": "file",
          "path": "references/best_practices.md"
        },
        {
          "name": "callbacks.md",
          "type": "file",
          "path": "references/callbacks.md"
        },
        {
          "name": "data_module.md",
          "type": "file",
          "path": "references/data_module.md"
        },
        {
          "name": "distributed_training.md",
          "type": "file",
          "path": "references/distributed_training.md"
        },
        {
          "name": "lightning_module.md",
          "type": "file",
          "path": "references/lightning_module.md"
        },
        {
          "name": "logging.md",
          "type": "file",
          "path": "references/logging.md"
        },
        {
          "name": "trainer.md",
          "type": "file",
          "path": "references/trainer.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "quick_trainer_setup.py",
          "type": "file",
          "path": "scripts/quick_trainer_setup.py"
        },
        {
          "name": "template_datamodule.py",
          "type": "file",
          "path": "scripts/template_datamodule.py"
        },
        {
          "name": "template_lightning_module.py",
          "type": "file",
          "path": "scripts/template_lightning_module.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
