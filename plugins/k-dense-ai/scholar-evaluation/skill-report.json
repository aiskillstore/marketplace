{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T16:48:27.774Z",
    "slug": "k-dense-ai-scholar-evaluation",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/scholar-evaluation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "feff140312e5818eae39a97caa38108b25f5f5a66ef8393a97317e60109b6629",
    "tree_hash": "f07e0970aa31a49656046412e0a7718a71381440328f21b6478b46e6d5862642"
  },
  "skill": {
    "name": "scholar-evaluation",
    "description": "Systematically evaluate scholarly work using the ScholarEval framework, providing structured assessment across research quality dimensions including problem formulation, methodology, analysis, and writing with quantitative scoring and actionable feedback.",
    "summary": "Systematically evaluate scholarly work using the ScholarEval framework, providing structured assessm...",
    "icon": "ðŸ”¬",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "evaluation",
      "rubric",
      "academia",
      "research-quality"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation and a local scoring script. No network operations, no credential access, no code execution beyond local file I/O. Python script reads user-provided JSON scores and generates text reports locally.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 1,
            "line_end": 379
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 47,
            "line_end": 68
          },
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 70,
            "line_end": 93
          },
          {
            "file": "scripts/calculate_scores.py",
            "line_start": 234,
            "line_end": 242
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 6,
    "total_lines": 1582,
    "audit_model": "claude",
    "audited_at": "2026-01-04T16:48:27.774Z"
  },
  "content": {
    "user_title": "Evaluate scholarly work with ScholarEval",
    "value_statement": "Scholarly reviews can be inconsistent and subjective. This skill provides a structured evaluation framework and scoring workflow for clear, repeatable assessments of research quality across eight dimensions.",
    "seo_keywords": [
      "ScholarEval",
      "scholarly evaluation",
      "research quality rubric",
      "peer review support",
      "academic scoring",
      "Claude",
      "Codex",
      "Claude Code",
      "research assessment",
      "methodology review"
    ],
    "actual_capabilities": [
      "Guide evaluation across eight ScholarEval dimensions with detailed criteria and checklists",
      "Provide qualitative strengths, weaknesses, and priority recommendations for each dimension",
      "Use a 1 to 5 scoring scale with defined quality levels from poor to exceptional",
      "Calculate weighted aggregate scores using the local Python scoring script",
      "Generate ASCII bar charts and comprehensive summary reports from scores"
    ],
    "limitations": [
      "Requires the user to provide the scholarly text or structured notes for evaluation",
      "Does not replace domain expert judgment for specialized or technical topics",
      "Scoring output quality depends on complete and accurate dimension ratings by the user",
      "No automatic retrieval of external literature or citations from databases"
    ],
    "use_cases": [
      {
        "target_user": "Journal editor",
        "title": "Screen submissions",
        "description": "Apply a consistent rubric to identify strong and weak papers before assigning full peer review."
      },
      {
        "target_user": "Graduate student",
        "title": "Improve thesis drafts",
        "description": "Get structured feedback on research questions, methods, analysis, and writing clarity."
      },
      {
        "target_user": "Research manager",
        "title": "Compare proposals",
        "description": "Score competing proposals across shared criteria for transparent funding decisions."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick dimension check",
        "scenario": "Single section review",
        "prompt": "Evaluate the methodology section for rigor, reproducibility, and limitations. Provide strengths, weaknesses, and a 1 to 5 score."
      },
      {
        "title": "Full paper review",
        "scenario": "Comprehensive evaluation",
        "prompt": "Assess this research paper across all ScholarEval dimensions. Provide scores, key strengths, weaknesses, and top five recommendations."
      },
      {
        "title": "Targeted improvement",
        "scenario": "Revision planning",
        "prompt": "Focus on literature review and analysis dimensions. Identify gaps, list specific fixes, and propose a priority order for revisions."
      },
      {
        "title": "Weighted assessment",
        "scenario": "Custom scoring",
        "prompt": "Use custom weights: methodology 0.3, analysis 0.25, literature review 0.2, writing 0.15, problem formulation 0.1. Provide an overall score and summary."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate this methodology section for clarity, rigor, and reproducibility.",
        "output": [
          "Clarity is strong with well-defined procedures, but key variables need operational definitions.",
          "Methodology uses appropriate research design for the questions, yet sample size justification is missing.",
          "Score: 3.5 out of 5 with priority recommendations on sampling, measurement, and replicability details."
        ]
      }
    ],
    "best_practices": [
      "Specify the work type and evaluation scope before beginning your assessment",
      "Cite concrete sections, paragraphs, or quotes to support each assessment",
      "Use the scoring script for consistent and reproducible aggregate reporting"
    ],
    "anti_patterns": [
      "Scoring without reading or analyzing the full provided text",
      "Applying every dimension when some do not apply to the work type",
      "Skipping critical weaknesses or issues to be overly polite"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes. The evaluation guidance is platform agnostic and the scoring script runs locally with Python."
      },
      {
        "question": "What are the limits of the scoring output?",
        "answer": "Scores reflect the quality of provided inputs and assessment choices. They do not replace subject matter expertise."
      },
      {
        "question": "Can I integrate this with other skills?",
        "answer": "Yes. Pair it with drafting or scientific schematic skills for complete research review workflows."
      },
      {
        "question": "Does this skill access or send my data?",
        "answer": "No. The Python script operates entirely locally with no network calls or external data transfers."
      },
      {
        "question": "What if a dimension does not apply to the work?",
        "answer": "Skip the dimension and note the reason in your report to keep the evaluation fair and transparent."
      },
      {
        "question": "How does this compare to human peer review?",
        "answer": "It provides a structured rubric for consistent assessments but does not replace expert human peer review."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "evaluation_framework.md",
          "type": "file",
          "path": "references/evaluation_framework.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "calculate_scores.py",
          "type": "file",
          "path": "scripts/calculate_scores.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
