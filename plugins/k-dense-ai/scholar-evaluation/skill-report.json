{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T03:56:48.211Z",
    "slug": "k-dense-ai-scholar-evaluation",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/scholar-evaluation",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "feff140312e5818eae39a97caa38108b25f5f5a66ef8393a97317e60109b6629",
    "tree_hash": "478f739647fdda15eef1142bd57bd8b6a510db23ffb0ed4886acc1d3382049d3"
  },
  "skill": {
    "name": "scholar-evaluation",
    "description": "Systematically evaluate scholarly work using the ScholarEval framework, providing structured assessment across research quality dimensions including problem formulation, methodology, analysis, and writing with quantitative scoring and actionable feedback.",
    "summary": "Systematically evaluate scholarly work using the ScholarEval framework, providing structured assessm...",
    "icon": "ðŸ”¬",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "evaluation",
      "rubric",
      "academia",
      "research-quality"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "network"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No data theft patterns or malicious execution logic detected. Files contain evaluation guidance and a local scoring script only. No network operations or hidden obfuscation found.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 1338,
    "audit_model": "codex",
    "audited_at": "2026-01-02T03:56:48.210Z"
  },
  "content": {
    "user_title": "Evaluate scholarly work with ScholarEval",
    "value_statement": "Scholarly reviews can be inconsistent and slow. This skill provides a structured rubric and scoring workflow for clear, repeatable evaluations.",
    "seo_keywords": [
      "ScholarEval",
      "scholarly evaluation",
      "research quality rubric",
      "peer review support",
      "academic scoring",
      "Claude",
      "Codex",
      "Claude Code",
      "research assessment",
      "methodology review"
    ],
    "actual_capabilities": [
      "Guide evaluation across eight ScholarEval dimensions with criteria and checklists",
      "Provide qualitative strengths, weaknesses, and priority recommendations",
      "Use a 1 to 5 scoring scale with defined quality levels",
      "Calculate weighted aggregate scores with `scripts/calculate_scores.py`",
      "Generate ASCII bar charts and summary reports from scores"
    ],
    "limitations": [
      "Requires the user to provide the scholarly text or structured notes",
      "Does not replace domain expert judgment for specialized topics",
      "Scoring output depends on complete and accurate dimension ratings",
      "No automatic retrieval of external literature or citations"
    ],
    "use_cases": [
      {
        "target_user": "Journal editor",
        "title": "Screen submissions",
        "description": "Apply a consistent rubric to identify strong and weak papers before full peer review."
      },
      {
        "target_user": "Graduate student",
        "title": "Improve a thesis draft",
        "description": "Get structured feedback on research questions, methods, and writing clarity."
      },
      {
        "target_user": "Research manager",
        "title": "Compare proposals",
        "description": "Score competing proposals across shared criteria for transparent decisions."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick dimension check",
        "scenario": "Single section review",
        "prompt": "Evaluate the methodology section for rigor, reproducibility, and limitations. Provide strengths, weaknesses, and a 1 to 5 score."
      },
      {
        "title": "Full paper review",
        "scenario": "Comprehensive evaluation",
        "prompt": "Assess this research paper across all ScholarEval dimensions. Provide scores, strengths, weaknesses, and top five recommendations."
      },
      {
        "title": "Targeted improvement plan",
        "scenario": "Revision planning",
        "prompt": "Focus on literature review and analysis. Identify gaps, list fixes, and propose a priority order for revisions."
      },
      {
        "title": "Scored report with weights",
        "scenario": "Weighted assessment",
        "prompt": "Use custom weights: methodology 0.3, analysis 0.25, literature review 0.2, writing 0.15, problem formulation 0.1. Provide an overall score and summary."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate this abstract and methods section for clarity and methodological rigor.",
        "output": [
          "Clarity is strong with a clear research question, but key variables need operational definitions.",
          "Methodology uses appropriate design, yet sample size justification is missing.",
          "Score: 3.8 out of 5 with priority recommendations on sampling and measurement detail."
        ]
      }
    ],
    "best_practices": [
      "Specify the work type and evaluation scope before scoring",
      "Cite concrete sections or quotes to support each assessment",
      "Use the scoring script for consistent aggregate reporting"
    ],
    "anti_patterns": [
      "Scoring without reading the full provided text",
      "Applying every dimension when some do not fit the work type",
      "Skipping weaknesses or critical issues to be polite"
    ],
    "faq": [
      {
        "question": "Is this compatible with Claude, Codex, and Claude Code?",
        "answer": "Yes. The guidance is platform agnostic and the script runs locally with Python."
      },
      {
        "question": "What are the limits of the scoring output?",
        "answer": "Scores reflect the provided inputs and do not replace subject matter expertise."
      },
      {
        "question": "Can I integrate this with other skills?",
        "answer": "Yes. You can pair it with drafting or schematic skills for complete review workflows."
      },
      {
        "question": "Does this skill access or send my data?",
        "answer": "No. There are no network calls or external data transfers in the files."
      },
      {
        "question": "What if a dimension does not apply?",
        "answer": "Skip it and note the reason in the report to keep the evaluation fair."
      },
      {
        "question": "How does this compare to peer review?",
        "answer": "It provides a structured rubric, but it does not replace expert peer review."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "evaluation_framework.md",
          "type": "file",
          "path": "references/evaluation_framework.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "calculate_scores.py",
          "type": "file",
          "path": "scripts/calculate_scores.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
