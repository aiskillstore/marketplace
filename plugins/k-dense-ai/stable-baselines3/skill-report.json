{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-04T17:11:56.520Z",
    "slug": "k-dense-ai-stable-baselines3",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/stable-baselines3",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "f609e8aadbfdee50cf3b8f90ea5ed6bfff3f379347d7219206000f8a8ccdae63",
    "tree_hash": "70520d4180628392d02c10aad1927091fbbc4287172ef4da3160ff29551cb707"
  },
  "skill": {
    "name": "stable-baselines3",
    "description": "Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DDPG, A2C, etc.), creating custom Gym environments, implementing callbacks for monitoring and control, using vectorized environments for parallel training, and integrating with deep RL workflows. This skill should be used when users request RL algorithm implementation, agent training, environment design, or RL experimentation.",
    "summary": "Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DD...",
    "icon": "ðŸ¤–",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "research",
    "tags": [
      "reinforcement-learning",
      "stable-baselines3",
      "gymnasium",
      "training",
      "evaluation"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "No security concerns detected. This skill provides legitimate reinforcement learning templates and documentation without any credential access, network exfiltration, or malicious code patterns.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/train_rl_agent.py",
            "line_start": 1,
            "line_end": 166
          },
          {
            "file": "scripts/evaluate_agent.py",
            "line_start": 1,
            "line_end": 246
          },
          {
            "file": "scripts/custom_env_template.py",
            "line_start": 1,
            "line_end": 315
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 3003,
    "audit_model": "claude",
    "audited_at": "2026-01-04T17:11:56.520Z"
  },
  "content": {
    "user_title": "Train reinforcement learning agents with Stable Baselines3",
    "value_statement": "Reinforcement learning implementation is complex and error-prone. This skill provides battle-tested templates for training agents, creating environments, and evaluating performance with industry-standard Stable Baselines3.",
    "seo_keywords": [
      "Stable Baselines3",
      "reinforcement learning",
      "PPO training",
      "SAC algorithm",
      "DQN implementation",
      "Claude Code",
      "Gymnasium environments",
      "RL agent evaluation",
      "vectorized training",
      "custom RL environments"
    ],
    "actual_capabilities": [
      "Train RL agents using PPO, SAC, DQN, TD3, and other SB3 algorithms",
      "Create custom Gymnasium environments with proper validation",
      "Evaluate trained models with statistical metrics",
      "Record training videos for visualization",
      "Implement callbacks for monitoring and early stopping",
      "Use vectorized environments for parallel training"
    ],
    "limitations": [
      "Requires stable-baselines3 and gymnasium Python packages",
      "Templates need adaptation for specific use cases",
      "No automatic hyperparameter optimization included",
      "Video recording requires environment rendering support"
    ],
    "use_cases": [
      {
        "target_user": "ML researcher",
        "title": "Benchmark RL algorithms efficiently",
        "description": "Compare PPO, SAC, and DQN on your environment with consistent evaluation metrics and proper statistical reporting."
      },
      {
        "target_user": "robotics engineer",
        "title": "Build continuous control systems",
        "description": "Use SAC or TD3 for robotic control tasks with custom observation spaces and reward functions."
      },
      {
        "target_user": "game developer",
        "title": "Create AI opponents",
        "description": "Train agents for your game environment and evaluate their performance against baseline strategies."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick PPO training",
        "scenario": "Train your first RL agent",
        "prompt": "Train a PPO agent on CartPole-v1 with 4 parallel environments for 100k timesteps. Include evaluation callbacks and save the best model."
      },
      {
        "title": "Custom environment",
        "scenario": "Create a new RL environment",
        "prompt": "Create a custom Gymnasium environment for a grid world task. Include proper observation/action spaces, reset and step methods, and validate with check_env."
      },
      {
        "title": "Model evaluation",
        "scenario": "Evaluate trained performance",
        "prompt": "Evaluate my saved model at ./models/best_model.zip on LunarLander-v2 for 20 episodes. Report mean reward and standard deviation."
      },
      {
        "title": "Continuous control",
        "scenario": "Solve robotics tasks",
        "prompt": "Set up SAC training on Pendulum-v1 with vectorized environments and VecNormalize wrapper. Include proper hyperparameters for continuous control."
      }
    ],
    "output_examples": [
      {
        "input": "Train PPO on CartPole and evaluate performance",
        "output": [
          "Created 4 parallel training environments",
          "Training PPO for 100000 timesteps...",
          "Best model saved at: ./models/best_model/",
          "Final model saved at: ./models/final_model",
          "Mean evaluation reward: 475.2 Â± 25.3"
        ]
      }
    ],
    "best_practices": [
      "Always validate custom environments with check_env before training",
      "Use vectorized environments with proper callback frequency adjustment",
      "Save VecNormalize statistics separately when using normalization wrappers",
      "Monitor training with TensorBoard for debugging and optimization"
    ],
    "anti_patterns": [
      "Training without evaluation callbacks or checkpoints",
      "Using SubprocVecEnv for lightweight environments like CartPole",
      "Ignoring observation normalization in continuous control tasks",
      "Not adjusting hyperparameters for different environment complexities"
    ],
    "faq": [
      {
        "question": "Which algorithm should I use for my task?",
        "answer": "Use PPO for general tasks, SAC/TD3 for continuous control, DQN for discrete actions. Check the algorithms reference guide for detailed selection criteria."
      },
      {
        "question": "How do I handle image observations?",
        "answer": "Use CnnPolicy for image inputs and ensure observations are uint8 in range [0,255] with channel-first format (C,H,W)."
      },
      {
        "question": "Can I use this with custom reward functions?",
        "answer": "Yes, implement your reward logic in the step() method of your custom environment. Ensure rewards are properly scaled."
      },
      {
        "question": "How do I resume training from a checkpoint?",
        "answer": "Load the model with algorithm.load() and continue training with model.learn() using the loaded model."
      },
      {
        "question": "What's the difference between DummyVecEnv and SubprocVecEnv?",
        "answer": "DummyVecEnv runs environments sequentially in one process. SubprocVecEnv uses multiple processes for true parallelism."
      },
      {
        "question": "How do I debug training issues?",
        "answer": "Use check_env for validation, monitor with TensorBoard, verify reward scaling, and test with simpler environments first."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "algorithms.md",
          "type": "file",
          "path": "references/algorithms.md"
        },
        {
          "name": "callbacks.md",
          "type": "file",
          "path": "references/callbacks.md"
        },
        {
          "name": "custom_environments.md",
          "type": "file",
          "path": "references/custom_environments.md"
        },
        {
          "name": "vectorized_envs.md",
          "type": "file",
          "path": "references/vectorized_envs.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "custom_env_template.py",
          "type": "file",
          "path": "scripts/custom_env_template.py"
        },
        {
          "name": "evaluate_agent.py",
          "type": "file",
          "path": "scripts/evaluate_agent.py"
        },
        {
          "name": "train_rl_agent.py",
          "type": "file",
          "path": "scripts/train_rl_agent.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
