{
  "skill": {
    "name": "zarr-python",
    "description": "Chunked N-D arrays for cloud storage. Compressed arrays, parallel I/O, S3/GCS integration, NumPy/Dask/Xarray compatible, for large-scale scientific computing pipelines.",
    "summary": "Store and process large N-dimensional arrays with chunked storage, compression, and cloud integration.",
    "icon": "ðŸ“¦",
    "version": "3.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT",
    "category": "data",
    "tags": ["scientific-computing", "cloud-storage", "data-arrays", "compression"],
    "supported_tools": ["claude", "codex", "claude-code"]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Static analysis detected 209 patterns in markdown documentation files, but all findings are false positives. The files contain only documentation about the Zarr Python library. No actual code execution, shell commands, or cryptographic operations exist in these documentation files. Backticks are markdown code formatting, not shell execution. URLs are documentation links, not credential exfiltration endpoints.",
    "static_findings_evaluation": [
      {
        "finding": "external_commands: Ruby/shell backtick execution (187 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "The analyzer detected backtick characters (`) in markdown documentation. Backticks in markdown are used for inline code formatting, not shell command execution. Files contain only documentation about array storage - no actual shell or Ruby code exists. This is a common false positive when scanning documentation files."
      },
      {
        "finding": "network: Hardcoded URL (7 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "URLs in SKILL.md lines 768-776 are legitimate documentation links to zarr.readthedocs.io, GitHub, and related projects. These are reference resources, not exfiltration endpoints. No credentials or sensitive data is transmitted."
      },
      {
        "finding": "sensitive: Certificate/key files (1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "The 'keys' reference at api_reference.md:202 refers to Zarr group keys (data structure keys), not cryptographic certificates or credentials. This is terminology confusion."
      },
      {
        "finding": "weak_cryptographic_algorithm: Weak cryptographic algorithm (12 locations)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "The analyzer incorrectly flagged compression codec names (zstd, gzip, lz4) and shuffle filter options (shuffle, bitshuffle). These are data compression algorithms, not cryptographic algorithms. No actual cryptographic operations exist in documentation."
      },
      {
        "finding": "obfuscation: DANGEROUS COMBINATION: Code execution + Network + Credential access (1 critical)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "This critical finding is a cascade of false positives. Documentation files do not contain executable code, network calls, or credential handling. The 'dangerous combination' pattern does not apply to markdown documentation about array storage."
      },
      {
        "finding": "network: Network reconnaissance (1 location)",
        "verdict": "false_positive",
        "confidence": "high",
        "reasoning": "No network reconnaissance code exists. Documentation references cloud storage services (S3, GCS) as storage backends, not scanning tools."
      }
    ],
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 1295
  },
  "content": {
    "user_title": "Store large N-dimensional arrays efficiently",
    "value_statement": "Working with large datasets that exceed memory limits. Zarr-python enables chunked array storage with compression for efficient cloud-native scientific computing workflows.",
    "seo_keywords": ["zarr", "zarr-python", "chunked arrays", "scientific computing", "cloud storage", "NumPy", "Dask", "Xarray", "compression", "parallel I/O"],
    "actual_capabilities": [
      "Create and manage chunked N-dimensional arrays with configurable compression",
      "Store arrays locally, in-memory, or in cloud storage (S3, GCS)",
      "Integrate with NumPy, Dask, and Xarray for seamless scientific workflows",
      "Enable parallel I/O operations for large-scale data processing",
      "Organize arrays hierarchically with groups for complex data structures",
      "Consolidate metadata for faster cloud storage access"
    ],
    "limitations": [
      "Python 3.11+ required (does not support older Python versions)",
      "Does not include data analysis or computation algorithms",
      "No built-in visualization or plotting capabilities",
      "Requires separate installation of cloud storage drivers (s3fs, gcsfs)"
    ],
    "use_cases": [
      {
        "target_user": "Climate Scientists",
        "title": "Store climate model data",
        "description": "Store terabyte-scale climate data with time dimensions. Enable efficient appending of new timesteps."
      },
      {
        "target_user": "Machine Learning Engineers",
        "title": "Manage model checkpoints",
        "description": "Store large embedding matrices and model weights. Integrate with Dask for distributed training."
      },
      {
        "target_user": "Bioinformatics Researchers",
        "title": "Process genomic datasets",
        "description": "Handle multi-terabyte genomic arrays. Use cloud storage for collaboration."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Array Setup",
        "scenario": "Create a simple Zarr array",
        "prompt": "Create a Zarr array with shape (10000, 10000), chunks of (1000, 1000), and float32 dtype. Store it at data/my_array.zarr."
      },
      {
        "title": "Cloud Storage",
        "scenario": "Configure S3 storage",
        "prompt": "Set up a Zarr array stored in S3 with s3fs. Use bucket my-bucket and path data/arrays.zarr."
      },
      {
        "title": "Dask Integration",
        "scenario": "Parallel computation",
        "prompt": "Load a Zarr array as a Dask array and compute the mean along axis 0 in parallel."
      },
      {
        "title": "Performance Tuning",
        "scenario": "Optimize for cloud access",
        "prompt": "Create a Zarr array optimized for cloud storage: 10MB chunks, consolidated metadata, and sharding enabled."
      }
    ],
    "output_examples": [
      {
        "input": "Create a Zarr array for storing temperature data with 365 time steps, 720 latitudes, and 1440 longitudes.",
        "output": [
          "Created Zarr array at 'temperature.zarr'",
          "Shape: (365, 720, 1440) | Chunks: (1, 720, 1440) | Dtype: float32",
          "Compression: Blosc (zstd, level 5) with shuffle filter",
          "Each chunk contains one complete daily snapshot (~4MB)",
          "Use z.append() to add new time steps efficiently"
        ]
      }
    ],
    "best_practices": [
      "Choose chunk sizes of 1-10 MB for optimal I/O performance",
      "Align chunk shape with your data access pattern (e.g., time-first for time series)",
      "Consolidate metadata when using cloud storage to reduce latency"
    ],
    "anti_patterns": [
      "Avoid loading entire large arrays into memory - process in chunks",
      "Do not use small chunks (<1MB) as they create excessive metadata overhead",
      "Avoid frequent writes to the same cloud storage location without synchronization"
    ],
    "faq": [
      {
        "question": "What is the difference between Zarr v2 and v3 formats?",
        "answer": "V3 supports sharding and has improved metadata. V2 is widely compatible with older tools. Zarr auto-detects format."
      },
      {
        "question": "How do I choose the right chunk size?",
        "answer": "Target 1-10 MB per chunk. For float32 data, 512x512 elements equals approximately 1 MB."
      },
      {
        "question": "Can Zarr handle arrays larger than available memory?",
        "answer": "Yes. Zarr only loads chunks needed for current operations. Use Dask for parallel out-of-core processing."
      },
      {
        "question": "What compression should I use?",
        "answer": "Use Blosc with lz4 for speed, zstd for balanced compression, or gzip for maximum compression ratio."
      },
      {
        "question": "How does Zarr compare to HDF5?",
        "answer": "Zarr offers simpler cloud integration, better metadata handling, and native support for parallel access patterns."
      },
      {
        "question": "Can I use Zarr with existing HDF5 files?",
        "answer": "Yes. Use h5py to read HDF5 files and zarr.array() to convert them to Zarr format."
      }
    ]
  }
}
