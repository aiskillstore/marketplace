{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-02T04:40:23.587Z",
    "slug": "k-dense-ai-zarr-python",
    "source_url": "https://github.com/K-Dense-AI/claude-scientific-skills/tree/main/scientific-skills/zarr-python",
    "source_ref": "main",
    "model": "codex",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "d7f7812ea82558c954a006e71ffe7b2024d466bf46dd828c9602e97922e115c2",
    "tree_hash": "99331aaa19c1920b3e7f92c5b0a6f95d5abbfbe414e7bb4b502433a650923db9"
  },
  "skill": {
    "name": "zarr-python",
    "description": "Chunked N-D arrays for cloud storage. Compressed arrays, parallel I/O, S3/GCS integration, NumPy/Dask/Xarray compatible, for large-scale scientific computing pipelines.",
    "summary": "Chunked N-D arrays for cloud storage. Compressed arrays, parallel I/O, S3/GCS integration, NumPy/Das...",
    "icon": "ðŸ§Š",
    "version": "1.0.0",
    "author": "K-Dense Inc.",
    "license": "MIT license",
    "category": "data",
    "tags": [
      "zarr",
      "arrays",
      "cloud-storage",
      "compression",
      "scientific-computing"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "network",
      "filesystem",
      "external_commands"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Scanned documentation files only. No executable code, network behavior, or file access logic found. No data theft or malicious execution patterns detected.",
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 1291,
    "audit_model": "codex",
    "audited_at": "2026-01-02T04:40:23.587Z"
  },
  "content": {
    "user_title": "Master Zarr storage for large arrays",
    "value_statement": "Large arrays are slow and hard to store. This skill teaches efficient chunking, compression, and cloud storage with Zarr.",
    "seo_keywords": [
      "Zarr Python",
      "chunked arrays",
      "cloud storage",
      "compression",
      "Dask",
      "Xarray",
      "Claude",
      "Codex",
      "Claude Code",
      "scientific data"
    ],
    "actual_capabilities": [
      "Create and open Zarr arrays with chunking and compression settings",
      "Select codecs like Blosc, Gzip, Zstd, and Bytes for storage tradeoffs",
      "Use LocalStore, MemoryStore, ZipStore, S3, and GCS backends",
      "Integrate Zarr with NumPy, Dask, and Xarray workflows",
      "Consolidate metadata and use thread or process synchronizers"
    ],
    "limitations": [
      "Requires Python 3.11 or newer",
      "Cloud backends need extra packages like s3fs or gcsfs",
      "Shards and chunks must fit in memory during writes",
      "Consolidated metadata can become stale after updates"
    ],
    "use_cases": [
      {
        "target_user": "Climate scientist",
        "title": "Store gridded climate data",
        "description": "Build chunked datasets with labels and fast access for long time series."
      },
      {
        "target_user": "Data engineer",
        "title": "Optimize cloud object storage",
        "description": "Plan chunk sizes and metadata consolidation for efficient S3 or GCS reads."
      },
      {
        "target_user": "ML researcher",
        "title": "Stream large training arrays",
        "description": "Use Dask and Zarr to train on arrays larger than memory."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Array Setup",
        "scenario": "Create a new Zarr array on disk",
        "prompt": "Show a minimal example to create a Zarr array with shape, chunks, dtype, and a local store."
      },
      {
        "title": "Choose Chunking",
        "scenario": "Tune chunks for access patterns",
        "prompt": "Recommend chunk shapes for row-wise and column-wise access on a 2D array and explain why."
      },
      {
        "title": "Cloud Store Plan",
        "scenario": "Store data on S3 or GCS",
        "prompt": "Provide a setup plan for writing Zarr data to S3 or GCS, including metadata consolidation tips."
      },
      {
        "title": "Parallel Workflow Design",
        "scenario": "Concurrent writes at scale",
        "prompt": "Design a parallel write workflow using Dask and synchronizers, and include safeguards for chunk boundaries."
      }
    ],
    "output_examples": [
      {
        "input": "Help me choose chunk sizes for a 3D array and store it on S3.",
        "output": [
          "Recommended chunk shape for balanced access and 1 to 10 MB targets",
          "S3 store setup using s3fs and S3Map",
          "Reminder to consolidate metadata after writing",
          "Note about shard size and memory limits during writes"
        ]
      }
    ],
    "best_practices": [
      "Align chunk shapes with your most common access pattern",
      "Consolidate metadata for cloud storage reads",
      "Process data in chunks or use Dask to avoid full memory loads"
    ],
    "anti_patterns": [
      "Using very small chunks that create too many files",
      "Loading entire arrays into memory when streaming works",
      "Forgetting to close ZipStore after writes"
    ],
    "faq": [
      {
        "question": "Is this compatible with Zarr v2 and v3?",
        "answer": "Yes. It covers workflows that work with both Zarr v2 and v3 formats."
      },
      {
        "question": "What are the practical size limits?",
        "answer": "Limits depend on storage and chunk size. Arrays can exceed memory if accessed in chunks."
      },
      {
        "question": "Can I integrate this with Dask or Xarray?",
        "answer": "Yes. It includes examples for Dask arrays and Xarray datasets with Zarr backends."
      },
      {
        "question": "Does it access my files or credentials?",
        "answer": "No. The skill is documentation only and does not access local files or secrets."
      },
      {
        "question": "What if performance is slow?",
        "answer": "Check chunk size, compression choice, and metadata consolidation for cloud stores."
      },
      {
        "question": "How does this compare to HDF5?",
        "answer": "Zarr is more cloud friendly and supports chunked parallel access without monolithic files."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "api_reference.md",
          "type": "file",
          "path": "references/api_reference.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
