{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-06T07:46:55.294Z",
    "slug": "muratcankoylan-evaluation",
    "source_url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering/tree/main/skills/evaluation",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "25d3bac5a761f7cd7fd177b00b11638b6415e0138d13c5493ce4090cdeb1c9fc",
    "tree_hash": "ddb75fdaca4bebbdf33ee142f6be4c6b5c856c33d918080c93772aaf81fd7161"
  },
  "skill": {
    "name": "evaluation",
    "description": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\", \"measure agent quality\", \"create evaluation rubrics\", or mentions LLM-as-judge, multi-dimensional evaluation, agent testing, or quality gates for agent pipelines.",
    "summary": "This skill should be used when the user asks to \"evaluate agent performance\", \"build test framework\"...",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "muratcankoylan",
    "license": "MIT",
    "category": "data",
    "tags": [
      "evaluation",
      "testing",
      "metrics",
      "quality-assurance",
      "agent-performance"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure Python evaluation library with no network access, file system operations, or command execution. Uses only standard library imports for computational scoring and test management.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 697,
    "audit_model": "claude",
    "audited_at": "2026-01-06T07:46:55.294Z"
  },
  "content": {
    "user_title": "Evaluate agent performance with rubrics",
    "value_statement": "Measuring agent quality requires structured approaches. This skill provides multi-dimensional evaluation rubrics, test set management, and production monitoring to continuously assess and improve agent performance.",
    "seo_keywords": [
      "agent evaluation",
      "LLM evaluation",
      "agent testing framework",
      "quality metrics",
      "Claude Code",
      "Codex",
      "test rubrics",
      "agent performance",
      "context engineering",
      "quality gates"
    ],
    "actual_capabilities": [
      "Create multi-dimensional evaluation rubrics with weighted scoring",
      "Build and manage test sets with complexity stratification",
      "Run automated evaluations against test cases",
      "Monitor production performance with sampling and alerts",
      "Calculate overall scores from dimension-level assessments"
    ],
    "limitations": [
      "Does not execute agent code directly",
      "Requires integration with your evaluation pipeline",
      "Heuristic scoring for demonstration only"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Build test frameworks",
        "description": "Create structured test suites with rubrics to systematically evaluate agent quality across multiple dimensions."
      },
      {
        "target_user": "DevOps Teams",
        "title": "Quality gates in CI/CD",
        "description": "Integrate evaluation checks into deployment pipelines to catch regressions before agents go to production."
      },
      {
        "target_user": "Research Teams",
        "title": "Compare agent configurations",
        "description": "Measure how different context strategies or model choices affect agent output quality over time."
      }
    ],
    "prompt_templates": [
      {
        "title": "Create rubric",
        "scenario": "Set up new evaluation dimensions",
        "prompt": "Create an evaluation rubric for [task_type] that measures factual_accuracy, completeness, and tool_efficiency. Assign weights to each dimension based on importance."
      },
      {
        "title": "Build test set",
        "scenario": "Design test cases",
        "prompt": "Build a test set with 10 cases spanning simple, medium, and complex complexity levels for evaluating [agent_type] on [use_case]."
      },
      {
        "title": "Run evaluation",
        "scenario": "Assess agent outputs",
        "prompt": "Evaluate the following agent outputs against the test set. Calculate dimension scores and overall pass/fail. Report: [agent_outputs]"
      },
      {
        "title": "Monitor production",
        "scenario": "Track quality trends",
        "prompt": "Analyze production samples from the last 24 hours. Calculate pass rate, average score, and identify any quality degradation patterns."
      }
    ],
    "output_examples": [
      {
        "input": "Evaluate my research agent on 5 test cases about technology trends",
        "output": [
          "Overall Pass Rate: 80% (4/5 tests passed)",
          "Factual Accuracy: 0.85 (Good)",
          "Completeness: 0.72 (Good)",
          "Citation Accuracy: 0.65 (Acceptable)",
          "Tool Efficiency: 0.78 (Good)",
          "Dimension requiring attention: Citation Accuracy"
        ]
      }
    ],
    "best_practices": [
      "Use multi-dimensional rubrics instead of single metrics to capture different quality aspects",
      "Stratify test sets by complexity level to identify where agents struggle",
      "Run evaluations continuously to catch regressions before production deployment"
    ],
    "anti_patterns": [
      "Evaluating specific execution steps instead of outcomes",
      "Using only automated LLM evaluation without human review",
      "Neglecting edge cases in test set design"
    ],
    "faq": [
      {
        "question": "Which AI platforms support this skill?",
        "answer": "Works with Claude, Codex, and Claude Code. Evaluation logic is platform-agnostic."
      },
      {
        "question": "How many test cases should I include?",
        "answer": "Start with 10-20 cases covering different complexity levels. Expand based on coverage gaps."
      },
      {
        "question": "Can I integrate with existing CI/CD pipelines?",
        "answer": "Yes. The evaluation classes return structured results suitable for programmatic pipeline integration."
      },
      {
        "question": "Does this skill store any user data?",
        "answer": "No. This is a computation-only library. All evaluation data stays in your session memory."
      },
      {
        "question": "What dimensions should I prioritize?",
        "answer": "Depends on your use case. Factual accuracy matters most for information retrieval. Completeness matters more for synthesis tasks."
      },
      {
        "question": "How does this compare to standard LLM benchmarks?",
        "answer": "Standard benchmarks test general capability. This skill lets you create custom evaluations specific to your agent and use case."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "metrics.md",
          "type": "file",
          "path": "references/metrics.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "evaluator.py",
          "type": "file",
          "path": "scripts/evaluator.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
