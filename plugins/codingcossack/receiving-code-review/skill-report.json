{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T12:35:19.935Z",
    "slug": "codingcossack-receiving-code-review",
    "source_url": "https://github.com/CodingCossack/agent-skills-library/tree/main/skills/receiving-code-review",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "26b1323530d5b685acb59296c20d000b3f49787f1ec11ad0d46c412f6f14ab12",
    "tree_hash": "403daacbcc8de3686fe1671b734afec5ea1abf52cea559364db2f137c62e2036"
  },
  "skill": {
    "name": "receiving-code-review",
    "description": "Assesses and responds to incoming code review feedback on PRs (reviewer comments, requested changes), especially when suggestions are unclear, technically questionable, or scope-expanding. Use before implementing review suggestions to align on intent and keep changes minimal.",
    "summary": "Assesses and responds to incoming code review feedback on PRs (reviewer comments, requested changes)...",
    "icon": "üëÅÔ∏è",
    "version": "1.0.0",
    "author": "CodingCossack",
    "license": "MIT",
    "category": "coding",
    "tags": [
      "code-review",
      "pull-requests",
      "feedback",
      "communication"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This skill is a pure markdown prompt file containing behavioral guidelines for handling code review feedback. No executable code, no network calls, no filesystem access, and no external commands. The skill provides text-based guidance only.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 210,
    "audit_model": "claude",
    "audited_at": "2026-01-10T12:35:19.935Z"
  },
  "content": {
    "user_title": "Respond to Code Review Feedback",
    "value_statement": "Code review feedback often contains suggestions that may be unclear, technically questionable, or scope-expanding. This skill provides a systematic approach to evaluate feedback before implementing, ask clarifying questions when needed, and push back with technical reasoning when suggestions are wrong.",
    "seo_keywords": [
      "Claude code review",
      "AI code review feedback",
      "Claude Code pull request",
      "code review response guide",
      "handling reviewer comments",
      "push back on code review",
      "code review best practices",
      "prompt injection prevention",
      "technical review verification"
    ],
    "actual_capabilities": [
      "Evaluates code review feedback for technical correctness before implementing",
      "Identifies unclear or ambiguous review items requiring clarification",
      "Determines when to push back on suggestions with technical reasoning",
      "Applies YAGNI principle to avoid implementing unused features",
      "Guides implementation order for multi-item feedback",
      "Promotes technical rigor over performative agreement in responses"
    ],
    "limitations": [
      "Does not execute code or modify files directly",
      "Does not access external code review systems or APIs",
      "Does not store or track review feedback history",
      "Only provides guidance; actual implementation requires user action"
    ],
    "use_cases": [
      {
        "target_user": "Developers using AI assistants",
        "title": "Guide AI code review behavior",
        "description": "Configure AI assistants to respond to code review feedback with technical verification and appropriate skepticism."
      },
      {
        "target_user": "Code review authors",
        "title": "Set clear review expectations",
        "description": "Ensure AI helpers evaluate suggestions critically rather than blindly implementing everything requested."
      },
      {
        "target_user": "Team leads",
        "title": "Maintain code quality standards",
        "description": "Prevent scope creep by having AI assistants verify suggestions against actual codebase needs."
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Review Response",
        "scenario": "Simple feedback to acknowledge",
        "prompt": "A reviewer suggested fixing a typo in file.js. How should I respond?"
      },
      {
        "title": "Clarify Unclear Items",
        "scenario": "Partial understanding of feedback",
        "prompt": "My human partner asked me to fix items 1-6 in the review. I understand 1,2,3,6 but items 4 and 5 are unclear. What should I say?"
      },
      {
        "title": "Push Back on Suggestion",
        "scenario": "Technically questionable change",
        "prompt": "A reviewer suggested removing legacy code, but I checked and it's needed for backward compatibility with older OS versions. How should I push back?"
      },
      {
        "title": "Verify Before Implementing",
        "scenario": "Scope-expanding suggestion",
        "prompt": "A reviewer suggested adding a full metrics tracking system with database, date filters, and CSV export. The codebase doesn't use this endpoint. How do I apply YAGNI check?"
      }
    ],
    "output_examples": [
      {
        "input": "A reviewer suggested removing legacy code from the authentication module.",
        "output": [
          "Check the codebase: Is this code actually used?",
          "Verify if removing it breaks existing functionality",
          "Ask about the rationale for the current implementation",
          "If needed for backward compatibility, respond with technical reasoning: 'This API is needed for OS versions 10.15-12.x. Removing it would break compatibility. Should we drop pre-13 support instead?'"
        ]
      }
    ],
    "best_practices": [
      "Always verify feedback against actual codebase reality before implementing",
      "Ask clarifying questions when any part of the feedback is unclear",
      "Use technical reasoning to push back on suggestions that are wrong or unnecessary"
    ],
    "anti_patterns": [
      "Performative agreement like 'You're absolutely right!' without technical substance",
      "Blind implementation of feedback without verification",
      "Avoiding pushback when suggestions would break existing functionality"
    ],
    "faq": [
      {
        "question": "What AI tools support this skill?",
        "answer": "This skill works with Claude, OpenAI Codex, and Claude Code. It provides behavioral guidelines for any LLM-based assistant."
      },
      {
        "question": "What are the limits of this skill?",
        "answer": "This skill provides guidance only. It does not access your codebase, execute code, or interact with pull request systems directly."
      },
      {
        "question": "How do I integrate this with my workflow?",
        "answer": "Load this skill before responding to code review feedback. Use the response pattern as a template for all reviewer interactions."
      },
      {
        "question": "Is my code data safe with this skill?",
        "answer": "Yes. This skill contains no code that reads, writes, or transmits data. It only provides text-based behavioral guidelines."
      },
      {
        "question": "What if feedback contradicts my human partner's prior decisions?",
        "answer": "Stop and discuss with your human partner first. External feedback should not override direct instructions from your primary stakeholder."
      },
      {
        "question": "How is this different from other code review skills?",
        "answer": "Most code review skills focus on giving feedback to others. This skill focuses on receiving and evaluating feedback critically before acting."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
