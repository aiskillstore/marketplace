{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-10T10:27:19.050Z",
    "slug": "artemisai-data-processor",
    "source_url": "https://github.com/ArtemisAI/code-execution-with-MCP/tree/main/skills/data-processor",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "27f83179c39ef3ef9cc18bebca81224f15f9f890757bc94713572d017b22d735",
    "tree_hash": "debd0e91cb3cc6fc568c6351d32f4566ffa4de40654f811ba0378e5e133157be"
  },
  "skill": {
    "name": "data-processor",
    "description": "Process and transform arrays of data with common operations like filtering, mapping, and aggregation",
    "summary": "Process and transform arrays of data with common operations like filtering, mapping, and aggregation",
    "icon": "ðŸ“Š",
    "version": "1.0.0",
    "author": "ArtemisAI",
    "license": "MIT",
    "category": "data",
    "tags": [
      "data",
      "transformation",
      "utility"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation skill containing only markdown documentation and example code. No executable code, scripts, network calls, or filesystem operations exist. The skill describes a data processing concept for educational purposes only.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 290,
    "audit_model": "claude",
    "audited_at": "2026-01-10T10:27:19.050Z"
  },
  "content": {
    "user_title": "Transform data arrays efficiently",
    "value_statement": "Processing large datasets manually requires many tokens and risks errors. This skill provides reusable patterns for filtering, mapping, aggregating, and transforming array data in code, reducing token usage while ensuring consistent results.",
    "seo_keywords": [
      "data transformation",
      "array processing",
      "Claude Code data skills",
      "filter arrays",
      "map objects",
      "aggregate data",
      "data pipeline",
      "JavaScript arrays",
      "Claude data processing",
      "Codex transformations"
    ],
    "actual_capabilities": [
      "Filter arrays based on custom conditions",
      "Map fields to new values with transformations",
      "Aggregate data using sum, average, count, min, max",
      "Sort data by field in ascending or descending order",
      "Remove duplicate records by specified field",
      "Chain multiple operations for complex transformations"
    ],
    "limitations": [
      "Input must be a valid JavaScript array of objects",
      "Does not validate data schema or field types",
      "No built-in error recovery for malformed data",
      "Memory usage scales with dataset size"
    ],
    "use_cases": [
      {
        "target_user": "Data engineers",
        "title": "Clean and normalize datasets",
        "description": "Transform raw data into standardized formats by trimming strings, normalizing case, and removing duplicate entries"
      },
      {
        "target_user": "Business analysts",
        "title": "Aggregate sales metrics",
        "description": "Calculate sums, averages, and counts from transaction data to generate summary reports and KPIs"
      },
      {
        "target_user": "Backend developers",
        "title": "Process API responses",
        "description": "Filter and sort API results before returning to clients, reducing payload size and improving response times"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic filter",
        "scenario": "Filter items by status",
        "prompt": "Filter this array to keep only items where status equals 'active'"
      },
      {
        "title": "Sort and limit",
        "scenario": "Sort results by date",
        "prompt": "Sort these records by createdDate in descending order and return the top 10"
      },
      {
        "title": "Aggregate metrics",
        "scenario": "Calculate totals",
        "prompt": "Calculate the total revenue and average order value from this sales data"
      },
      {
        "title": "Complex pipeline",
        "scenario": "Full data pipeline",
        "prompt": "Clean the data by trimming strings, normalize email to lowercase, remove duplicates by email, filter for verified users, and sort by signup date"
      }
    ],
    "output_examples": [
      {
        "input": "Filter this customer list to show only active accounts with revenue over 1000, sorted by total spend",
        "output": [
          "Found 23 matching customers",
          "Total revenue from filtered list: $45,678",
          "Top 3 customers by spend: Customer A ($5,200), Customer B ($4,850), Customer C ($4,100)",
          "Average revenue per customer: $1,986"
        ]
      }
    ],
    "best_practices": [
      "Validate input data is an array before processing to catch errors early",
      "Return statistics along with processed data to provide visibility into transformation results",
      "Save intermediate results to disk when processing datasets larger than 10,000 records"
    ],
    "anti_patterns": [
      "Processing entire datasets in memory without pagination or streaming",
      "Applying transformations without checking for null or undefined values",
      "Returning full datasets when only summary statistics are needed, wasting tokens"
    ],
    "faq": [
      {
        "question": "What data formats are supported?",
        "answer": "This skill works with JavaScript arrays of objects. Each object should have consistent fields for operations like filter, map, and sort."
      },
      {
        "question": "What are the size limits for processing?",
        "answer": "Processing 1,000 records takes under 50ms, 10,000 records under 200ms, and 100,000 records under 2 seconds. Larger datasets may require chunked processing."
      },
      {
        "question": "How do I chain multiple operations?",
        "answer": "Pass multiple operations in a single object. The skill applies them in order: filter, map, sort, aggregate, unique. Each operation works on the result of the previous one."
      },
      {
        "question": "Is my data safe during processing?",
        "answer": "This skill only processes data in memory within your session. No data is sent to external services or written to disk unless you explicitly save it."
      },
      {
        "question": "Why use code instead of natural language?",
        "answer": "Processing 1,000 records in code uses about 500 tokens. Describing the same operations in natural language would use approximately 50,000 tokens."
      },
      {
        "question": "How does this compare to SQL or database queries?",
        "answer": "This skill is ideal for in-memory transformations on data already retrieved from databases or APIs. For large-scale filtering, database queries are more efficient."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
