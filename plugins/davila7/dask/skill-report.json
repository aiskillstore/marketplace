{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T00:55:46.841Z",
    "slug": "davila7-dask",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/dask",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "2d61f4775092db8cdcdc8efc106b31084e6ad582ffbfbf763d5b8ecfd5c83288",
    "tree_hash": "98cca5db18c1f2836a84d7a265fb5708f19d3ec36ce11901913fc021c0195976"
  },
  "skill": {
    "name": "dask",
    "description": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-file processing, task graphs, for larger-than-RAM datasets and parallel workflows.",
    "summary": "Parallel/distributed computing. Scale pandas/NumPy beyond memory, parallel DataFrames/Arrays, multi-...",
    "icon": "⚡",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "data",
    "tags": [
      "dask",
      "parallel computing",
      "distributed computing",
      "data science",
      "pandas",
      "numpy"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation and guidance skill containing only markdown files with example Python code in code blocks. No executable code, network calls, file system access, or external commands detected. All content is informational documentation about Dask library usage.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 7,
    "total_lines": 2758,
    "audit_model": "claude",
    "audited_at": "2026-01-07T00:55:46.841Z"
  },
  "content": {
    "user_title": "Scale pandas and NumPy beyond memory with Dask",
    "value_statement": "Processing large datasets that exceed available RAM causes memory errors and slow performance. Dask provides parallel computing abstractions that scale pandas and NumPy operations to handle terabyte-scale data on laptops or clusters.",
    "seo_keywords": [
      "dask",
      "parallel computing",
      "distributed computing",
      "pandas",
      "numpy",
      "claude code",
      "claude",
      "codex",
      "dataframes",
      "dask arrays",
      "big data processing"
    ],
    "actual_capabilities": [
      "Scale pandas operations to datasets larger than RAM using parallel DataFrames",
      "Process NumPy arrays beyond memory limits with chunked parallel computations",
      "Handle multi-file datasets (CSV, Parquet, JSON) with glob patterns",
      "Build custom parallel workflows with task graphs and lazy evaluation",
      "Process unstructured data (logs, JSON) with Bags before converting to structured formats",
      "Execute fine-grained parallel tasks immediately with Futures API"
    ],
    "limitations": [
      "Not all pandas and NumPy functions are implemented in Dask equivalents",
      "Chunk size selection requires understanding to avoid memory or performance issues",
      "Debugging parallel code is more complex than single-threaded code",
      "Performance depends on proper chunking and scheduler selection"
    ],
    "use_cases": [
      {
        "target_user": "Data scientists",
        "title": "Large dataset analysis",
        "description": "Analyze datasets too large for pandas memory limits on a single machine"
      },
      {
        "target_user": "ML engineers",
        "title": "Distributed preprocessing",
        "description": "Preprocess training data across multiple cores or cluster nodes"
      },
      {
        "target_user": "Researchers",
        "title": "Scientific computing at scale",
        "description": "Run numerical simulations and process large scientific datasets"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic DataFrame scaling",
        "scenario": "Load large CSV files",
        "prompt": "Use Dask to read all CSV files matching 'data/year=2024/month=*/day=*.csv' into a single DataFrame, filter for records where status='valid', and compute the mean and sum of the 'amount' column grouped by category."
      },
      {
        "title": "Array operations",
        "scenario": "Process large arrays",
        "prompt": "Create a Dask array from a Zarr file containing 100GB of image data, normalize each chunk by subtracting the mean and dividing by standard deviation, then save the normalized result back to Zarr format."
      },
      {
        "title": "Custom parallel tasks",
        "scenario": "Build dynamic workflows",
        "prompt": "Set up a local Dask distributed client and submit 100 independent parameter sweep tasks where each task runs a simulation function with different parameters, then gather results as they complete."
      },
      {
        "title": "Log analysis pipeline",
        "scenario": "Process unstructured logs",
        "prompt": "Read all JSON log files from 'logs/*.json', filter for error entries, extract the 'message' and 'timestamp' fields, convert to a DataFrame, and save aggregated error counts by message pattern to Parquet."
      }
    ],
    "output_examples": [
      {
        "input": "Load a 50GB CSV file that exceeds RAM and compute average sales by region",
        "output": [
          "• Created Dask DataFrame with 500 partitions (~100MB each)",
          "• Filtered to valid records (status='completed'): 45.2M rows",
          "• Grouped by region and computed mean('sale_amount')",
          "• Result: 8 regions with averages ranging from $127 to $892",
          "• Peak memory usage: 12GB (vs 50GB total data)",
          "• Processing time: 4 minutes on 8 cores"
        ]
      }
    ],
    "best_practices": [
      "Let Dask load data directly rather than loading into pandas first and then converting",
      "Use a single compute() call for multiple operations to enable parallel execution",
      "Target chunk sizes around 100MB per partition to balance parallelism and overhead"
    ],
    "anti_patterns": [
      "Loading entire dataset into pandas before converting to Dask DataFrame",
      "Calling compute() inside loops instead of batching operations",
      "Creating millions of tiny tasks by not using map_partitions for batch operations"
    ],
    "faq": [
      {
        "question": "What Dask collection should I use for my data type?",
        "answer": "Use DataFrames for tabular data, Arrays for numeric matrices, Bags for unstructured text or JSON, and Futures for dynamic custom workflows."
      },
      {
        "question": "How large of a dataset can Dask handle?",
        "answer": "Dask scales from laptops (~100GB) to clusters (~100TB). Actual limits depend on available storage and cluster resources."
      },
      {
        "question": "Can Dask read from cloud storage like S3?",
        "answer": "Yes, Dask natively supports S3, GCS, and Azure Blob Storage using the same read functions with s3://, gcs://, or azure:// prefixes."
      },
      {
        "question": "Which scheduler should I choose?",
        "answer": "Use threads for pandas/NumPy operations, processes for pure Python code, synchronous for debugging, and distributed for dashboard access and clusters."
      },
      {
        "question": "Is my existing pandas code compatible with Dask?",
        "answer": "Dask implements most of the pandas API. Most pandas code works with minor changes like adding .compute() and adjusting chunk-sensitive operations."
      },
      {
        "question": "How does Dask compare to Spark for big data?",
        "answer": "Dask is lighter weight and integrates better with Python scientific stack. Spark has richer ecosystem but higher overhead and requires JVM infrastructure."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "arrays.md",
          "type": "file",
          "path": "references/arrays.md"
        },
        {
          "name": "bags.md",
          "type": "file",
          "path": "references/bags.md"
        },
        {
          "name": "best-practices.md",
          "type": "file",
          "path": "references/best-practices.md"
        },
        {
          "name": "dataframes.md",
          "type": "file",
          "path": "references/dataframes.md"
        },
        {
          "name": "futures.md",
          "type": "file",
          "path": "references/futures.md"
        },
        {
          "name": "schedulers.md",
          "type": "file",
          "path": "references/schedulers.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
