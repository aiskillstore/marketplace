{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:07:48.973Z",
    "slug": "davila7-transformers",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/transformers",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "119e970faa0800379f3377c48c9f8b8bccb21a1fee2fcf4d32a5ff6d91f129ab",
    "tree_hash": "802b92fa3091daa47c84f3a67702da6769c2e9b7cfbea7177427961a39f1b6f7"
  },
  "skill": {
    "name": "transformers",
    "description": "This skill should be used when working with pre-trained transformer models for natural language processing, computer vision, audio, or multimodal tasks. Use for text generation, classification, question answering, translation, summarization, image classification, object detection, speech recognition, and fine-tuning models on custom datasets.",
    "summary": "This skill should be used when working with pre-trained transformer models for natural language proc...",
    "icon": "ðŸ§ ",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "scientific",
    "tags": [
      "machine-learning",
      "nlp",
      "hugging-face",
      "deep-learning"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill containing only Markdown reference guides for Hugging Face Transformers library. No executable code, scripts, or potentially harmful capabilities. All capabilities shown (pip install, model loading, training) are standard ML workflows using trusted sources.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 6,
    "total_lines": 1773,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:07:48.973Z"
  },
  "content": {
    "user_title": "Work with pre-trained transformer models",
    "value_statement": "Building AI solutions requires understanding transformer models for text, images, and audio. This skill provides complete documentation for the Hugging Face Transformers library including pipelines, model loading, text generation, tokenization, and training workflows.",
    "seo_keywords": [
      "transformers",
      "Hugging Face",
      "Claude",
      "Codex",
      "Claude Code",
      "machine learning",
      "NLP",
      "text generation",
      "fine-tuning",
      "BERT",
      "GPT"
    ],
    "actual_capabilities": [
      "Load pre-trained transformer models using AutoModel classes for NLP, vision, audio, and multimodal tasks",
      "Use the Pipeline API for quick inference with text classification, NER, question answering, summarization, and translation",
      "Generate text with various decoding strategies including greedy, beam search, sampling, and contrastive search",
      "Fine-tune models on custom datasets using the Trainer API with mixed precision and distributed training support",
      "Tokenize text with padding, truncation, and special tokens using AutoTokenizer",
      "Optimize inference with quantization, device placement, and attention implementations"
    ],
    "limitations": [
      "Does not include pre-trained models - users must download from Hugging Face Hub or provide local paths",
      "Requires PyTorch, TensorFlow, or JAX to run inference - skill only provides documentation and guidance",
      "Does not handle data collection or preprocessing pipelines beyond basic tokenization examples",
      "Model performance depends on the quality and size of models chosen from the Hub"
    ],
    "use_cases": [
      {
        "target_user": "Data scientists",
        "title": "Fine-tune models on custom datasets",
        "description": "Use the Trainer API to adapt pre-trained models for specific tasks like sentiment analysis or document classification."
      },
      {
        "target_user": "ML engineers",
        "title": "Build production inference pipelines",
        "description": "Deploy optimized models with batching, quantization, and GPU acceleration for high-throughput applications."
      },
      {
        "target_user": "Researchers",
        "title": "Experiment with transformer architectures",
        "description": "Access state-of-the-art models for experiments with text generation, vision-language tasks, and audio processing."
      }
    ],
    "prompt_templates": [
      {
        "title": "Quick text classification",
        "scenario": "Classify text sentiment",
        "prompt": "Use the transformers pipeline to classify this text: 'Your product exceeded all my expectations.' Show me the label and confidence score."
      },
      {
        "title": "Custom model loading",
        "scenario": "Load specific model with device control",
        "prompt": "Load the 'bert-base-uncased' model for sequence classification. Move it to GPU and show me the model architecture summary."
      },
      {
        "title": "Controlled text generation",
        "scenario": "Generate with specific parameters",
        "prompt": "Generate creative text using GPT-2 with temperature=0.8, top-k=50, and max_new_tokens=100. Show the complete generation process."
      },
      {
        "title": "Full fine-tuning workflow",
        "scenario": "Train on custom dataset",
        "prompt": "Set up a complete fine-tuning workflow using the Trainer API. Include training arguments, data collator, and evaluation metrics for a classification task."
      }
    ],
    "output_examples": [
      {
        "input": "Classify this review: 'The interface is confusing and the documentation is incomplete.'",
        "output": [
          "Pipeline: text-classification",
          "Model: distilbert-base-uncased-finetuned-sst-2-english",
          "Result: NEGATIVE (0.98 confidence)",
          "âœ“ Model correctly identified negative sentiment with high confidence"
        ]
      }
    ],
    "best_practices": [
      "Use AutoModel and AutoTokenizer classes for automatic architecture detection and compatibility",
      "Specify torch_dtype explicitly to control precision and reduce memory usage",
      "Enable GPU acceleration with device_map='auto' for large models to distribute across available hardware"
    ],
    "anti_patterns": [
      "Using default models without specifying explicit model identifiers that may change over time",
      "Loading models without setting appropriate max_length or truncation for production use",
      "Training without evaluation strategy or proper logging to track model performance"
    ],
    "faq": [
      {
        "question": "Which AI tools is this skill compatible with?",
        "answer": "This skill works with Claude, Codex, and Claude Code. All provide the same transformer model capabilities."
      },
      {
        "question": "What are the resource limits for running transformer models?",
        "answer": "Model size is limited by available GPU memory. Use quantization (8-bit or 4-bit) or CPU mode for large models."
      },
      {
        "question": "How do I integrate transformers with existing code?",
        "answer": "Import transformers library and use AutoModel/AutoTokenizer. Replace existing model calls with pipeline or direct model inference."
      },
      {
        "question": "Is my data safe when using these models?",
        "answer": "Yes. All processing happens locally. Models run on your machine and data never leaves your environment."
      },
      {
        "question": "Why is my model generating repetitive text?",
        "answer": "Enable repetition_penalty (1.2-1.5), use no_repeat_ngram_size (2-3), or try contrastive search with lower temperature."
      },
      {
        "question": "How does this compare to using OpenAI or Anthropic APIs?",
        "answer": "Transformers gives you full control to run models locally. OpenAI APIs offer easier setup but send data externally."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "generation.md",
          "type": "file",
          "path": "references/generation.md"
        },
        {
          "name": "models.md",
          "type": "file",
          "path": "references/models.md"
        },
        {
          "name": "pipelines.md",
          "type": "file",
          "path": "references/pipelines.md"
        },
        {
          "name": "tokenizers.md",
          "type": "file",
          "path": "references/tokenizers.md"
        },
        {
          "name": "training.md",
          "type": "file",
          "path": "references/training.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
