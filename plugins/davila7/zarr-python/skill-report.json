{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:17:41.417Z",
    "slug": "davila7-zarr-python",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/zarr-python",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "83f9e52e25e10b0c0291872a61e82371b22166f8aefd0cf4ef9f347641f79b59",
    "tree_hash": "87bbf100ef9677a2a0037c2f1031fe8719d1a10b7ebef28fca2e8b862614e764"
  },
  "skill": {
    "name": "zarr-python",
    "description": "Chunked N-D arrays for cloud storage. Compressed arrays, parallel I/O, S3/GCS integration, NumPy/Dask/Xarray compatible, for large-scale scientific computing pipelines.",
    "summary": "Chunked N-D arrays for cloud storage. Compressed arrays, parallel I/O, S3/GCS integration, NumPy/Das...",
    "icon": "ðŸ“¦",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "scientific",
    "tags": [
      "arrays",
      "scientific-computing",
      "cloud-storage",
      "data-science"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation skill containing only markdown files. No executable code, scripts, network calls, or system access. This skill teaches users how to use the zarr-python library for scientific data processing.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 2,
    "total_lines": 1290,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:17:41.417Z"
  },
  "content": {
    "user_title": "Work with chunked N-dimensional arrays using Zarr",
    "value_statement": "Managing large scientific datasets requires efficient storage and access patterns. Zarr provides chunked, compressed arrays with cloud storage support and seamless integration with NumPy, Dask, and Xarray for scalable scientific computing workflows.",
    "seo_keywords": [
      "zarr python",
      "chunked arrays",
      "scientific data storage",
      "cloud array storage",
      "S3 arrays",
      "NumPy compatible",
      "Dask integration",
      "Xarray backend",
      "compressed data storage",
      "large-scale scientific computing"
    ],
    "actual_capabilities": [
      "Create and manage chunked N-dimensional arrays with compression",
      "Read and write data to local files, memory, ZIP archives, and cloud storage (S3, GCS)",
      "Integrate with NumPy for array operations and Dask for parallel computing",
      "Optimize chunk sizes and compression codecs for performance",
      "Organize arrays hierarchically using groups with custom metadata",
      "Process datasets larger than memory using chunked access patterns"
    ],
    "limitations": [
      "Requires Python 3.11 or higher",
      "Cloud storage backends require additional packages (s3fs, gcsfs)",
      "Entire chunks must fit in memory during compression operations",
      "Sharded arrays require entire shards to fit in memory before writing"
    ],
    "use_cases": [
      {
        "target_user": "Climate scientists",
        "title": "Climate data analysis",
        "description": "Store and analyze large satellite imagery and climate model outputs with efficient chunked access patterns."
      },
      {
        "target_user": "Machine learning engineers",
        "title": "Large dataset management",
        "description": "Handle training datasets too large for memory by processing in chunks with parallel I/O."
      },
      {
        "target_user": "Bioinformatics researchers",
        "title": "Genomic data workflows",
        "description": "Store and access massive genomic datasets with compression and cloud storage integration."
      }
    ],
    "prompt_templates": [
      {
        "title": "Create basic array",
        "scenario": "Create a new Zarr array",
        "prompt": "Create a Zarr array with shape (10000, 10000), chunks (1000, 1000), and float32 dtype. Store it at data/my_array.zarr and write random data to it."
      },
      {
        "title": "Cloud storage setup",
        "scenario": "Configure S3 storage",
        "prompt": "Set up a Zarr array stored in S3 bucket my-bucket/path/to/data.zarr using s3fs. Create the array with appropriate chunking for cloud access."
      },
      {
        "title": "Performance optimization",
        "scenario": "Optimize chunk strategy",
        "prompt": "Analyze my access pattern (reading entire rows frequently) and suggest optimal chunk shape for a (10000, 10000) float32 array. Create the array with those chunks."
      },
      {
        "title": "Dask integration",
        "scenario": "Parallel processing",
        "prompt": "Load a Zarr array as a Dask array and compute the mean along axis 0 in parallel. Write the result back to a new Zarr array."
      }
    ],
    "output_examples": [
      {
        "input": "Create a Zarr array for storing time series temperature data with efficient append operations",
        "output": [
          "Created array with shape (0, 720, 1440) - starts empty for appending",
          "Chunks configured as (1, 720, 1440) - one time step per chunk",
          "Use z.append(new_data, axis=0) to add new time steps",
          "Each chunk ~2MB for float32 data",
          "Access specific time with z[0:10, :, :] for first 10 timesteps"
        ]
      }
    ],
    "best_practices": [
      "Choose chunk sizes of 1-10 MB and align chunk shape with your access patterns",
      "Consolidate metadata for cloud storage to reduce latency from N operations to 1",
      "Use Dask for processing arrays larger than memory and for parallel computation"
    ],
    "anti_patterns": [
      "Avoid loading entire large arrays into memory with z[:] - use chunked iteration instead",
      "Do not use small chunks (under 1 MB) as they create excessive metadata overhead",
      "Avoid writing to the same chunks from multiple processes without proper synchronization"
    ],
    "faq": [
      {
        "question": "What Python versions are supported?",
        "answer": "Zarr-python requires Python 3.11 or higher. Check your version with python --version."
      },
      {
        "question": "What is the maximum array size?",
        "answer": "Zarr can store arrays of any size limited only by your storage. For practical purposes, design chunk sizes that fit in memory."
      },
      {
        "question": "How do I migrate from HDF5 to Zarr?",
        "answer": "Use h5py to read HDF5 files and zarr.array() to write to Zarr format. Both support similar hierarchical group structures."
      },
      {
        "question": "Is my data safe when power fails during writes?",
        "answer": "Zarr writes are atomic per chunk. Use synchronizers for multi-process writes. Consider consolidate_metadata() after batch updates."
      },
      {
        "question": "Why is my array slow to read?",
        "answer": "Check chunk size and shape alignment with your access pattern. Small chunks or misaligned shapes cause excessive I/O operations."
      },
      {
        "question": "How does Zarr compare to NumPy .npy files?",
        "answer": "NumPy files store whole arrays at once. Zarr chunks data enabling partial reads, compression, and out-of-core computation with Dask."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "api_reference.md",
          "type": "file",
          "path": "references/api_reference.md"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
