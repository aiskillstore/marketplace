{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:25:45.360Z",
    "slug": "davila7-pytorch-lightning",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/pytorch-lightning",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "3399661bfbca82f65bb420ef36b8b9bca1ae57324ae60f876d9b8111c8a1452a",
    "tree_hash": "204c5b4e8c03bbb15dd350ef746e25559f8b3932e5564c52725410dc0cef285d"
  },
  "skill": {
    "name": "pytorch-lightning",
    "description": "Deep learning framework (PyTorch Lightning). Organize PyTorch code into LightningModules, configure Trainers for multi-GPU/TPU, implement data pipelines, callbacks, logging (W&B, TensorBoard), distributed training (DDP, FSDP, DeepSpeed), for scalable neural network training.",
    "summary": "Deep learning framework (PyTorch Lightning). Organize PyTorch code into LightningModules, configure ...",
    "icon": "âš¡",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "scientific",
    "tags": [
      "deep learning",
      "pytorch",
      "neural networks",
      "machine learning"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": [
      "scripts",
      "filesystem"
    ]
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure documentation and template skill for PyTorch Lightning. Contains no executable code that accesses sensitive data, makes network calls, or modifies system state beyond local project directories.",
    "risk_factor_evidence": [
      {
        "factor": "scripts",
        "evidence": [
          {
            "file": "scripts/quick_trainer_setup.py",
            "line_start": 1,
            "line_end": 455
          },
          {
            "file": "scripts/template_datamodule.py",
            "line_start": 1,
            "line_end": 329
          },
          {
            "file": "scripts/template_lightning_module.py",
            "line_start": 1,
            "line_end": 220
          }
        ]
      },
      {
        "factor": "filesystem",
        "evidence": [
          {
            "file": "scripts/template_datamodule.py",
            "line_start": 79,
            "line_end": 96
          },
          {
            "file": "scripts/quick_trainer_setup.py",
            "line_start": 62,
            "line_end": 114
          }
        ]
      }
    ],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 11,
    "total_lines": 5458,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:25:45.360Z"
  },
  "content": {
    "user_title": "Build Neural Networks with PyTorch Lightning",
    "value_statement": "PyTorch Lightning eliminates boilerplate code while keeping full PyTorch flexibility. Organize models into LightningModules, scale across multiple GPUs and TPUs, and automate training workflows with built-in callbacks, logging, and checkpointing.",
    "seo_keywords": [
      "pytorch lightning",
      "deep learning framework",
      "neural network training",
      "distributed training",
      "multi-gpu training",
      "lightning module",
      "claude code skill",
      "pytorch trainer"
    ],
    "actual_capabilities": [
      "Organize PyTorch models into LightningModules with structured training, validation, and test steps",
      "Configure Trainers for single GPU, multi-GPU, and TPU training with distributed strategies",
      "Build reusable data pipelines with LightningDataModules",
      "Implement callbacks for checkpointing, early stopping, and learning rate monitoring",
      "Set up distributed training with DDP, FSDP, and DeepSpeed strategies",
      "Integrate logging with TensorBoard, Weights & Biases, and MLflow"
    ],
    "limitations": [
      "Does not install PyTorch Lightning or manage dependencies",
      "Does not execute training or return model outputs",
      "Does not provide pre-trained models or datasets",
      "Assumes basic familiarity with PyTorch tensors and neural networks"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Scale Model Training",
        "description": "Configure distributed training across multiple GPUs with automatic mixed precision and checkpoint management."
      },
      {
        "target_user": "Researchers",
        "title": "Reproducible Experiments",
        "description": "Structure research code with organized modules, logging, and checkpointing for publication-ready workflows."
      },
      {
        "target_user": "Data Scientists",
        "title": "Production Pipelines",
        "description": "Build reusable data modules and training loops that can be shared across projects and teams."
      }
    ],
    "prompt_templates": [
      {
        "title": "Create Basic Model",
        "scenario": "Build a simple classification model",
        "prompt": "Create a LightningModule for image classification with a simple neural network, training step with cross-entropy loss, and Adam optimizer."
      },
      {
        "title": "Multi-GPU Setup",
        "scenario": "Configure distributed training",
        "prompt": "Set up a Trainer with DDP strategy for 4 GPUs, mixed precision, and ModelCheckpoint callback monitoring validation loss."
      },
      {
        "title": "Data Pipeline",
        "scenario": "Build data loading pipeline",
        "prompt": "Create a LightningDataModule that loads images from a directory, applies transforms, and provides train, validation, and test dataloaders."
      },
      {
        "title": "Custom Callback",
        "scenario": "Add training extension",
        "prompt": "Create a custom callback that logs gradient norms at the end of each training batch for monitoring training stability."
      }
    ],
    "output_examples": [
      {
        "input": "Create a LightningModule for a simple image classifier with 10 classes",
        "output": [
          "Define the model architecture in __init__ with save_hyperparameters()",
          "Implement training_step with cross-entropy loss and accuracy logging",
          "Configure Adam optimizer with learning rate scheduler",
          "Use self.device for tensor placement instead of .cuda()",
          "Return the loss tensor from training_step"
        ]
      }
    ],
    "best_practices": [
      "Use save_hyperparameters() in __init__ to track model configuration for reproducibility",
      "Log metrics with self.log() for automatic aggregation across distributed training",
      "Configure callbacks like ModelCheckpoint and EarlyStopping for production training",
      "Use self.device instead of explicit .cuda() calls for device-agnostic code"
    ],
    "anti_patterns": [
      "Do not use explicit .cuda() calls in training_step - Lightning handles device placement",
      "Do not manually create DistributedSampler - Lightning handles this automatically",
      "Do not mix research code with engineering code in LightningModule",
      "Do not forget to return the loss from training_step for backpropagation"
    ],
    "faq": [
      {
        "question": "How does PyTorch Lightning compare to raw PyTorch?",
        "answer": "Lightning provides structure without abstraction. Your code is still pure PyTorch but organized for scalability and reproducibility."
      },
      {
        "question": "What hardware does Lightning support?",
        "answer": "Lightning supports CPU, single/multi GPU, TPU, IPU, and HPU with automatic device detection and distributed training strategies."
      },
      {
        "question": "How do I resume training from a checkpoint?",
        "answer": "Pass ckpt_path parameter to trainer.fit() - trainer.fit(model, datamodule=dm, ckpt_path='last.ckpt') loads saved state."
      },
      {
        "question": "Is my data safe with this skill?",
        "answer": "Yes. This skill provides templates only. Data access is controlled by user-written code in their own project directory."
      },
      {
        "question": "Why is my validation metric not decreasing?",
        "answer": "Check learning rate, data preprocessing, and model architecture. Use fast_dev_run=True to test with 1 batch first."
      },
      {
        "question": "DDP vs FSDP vs DeepSpeed?",
        "answer": "DDP for models under 500M parameters. FSDP for larger models with memory-efficient sharding. DeepSpeed for very large models with advanced offloading."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "best_practices.md",
          "type": "file",
          "path": "references/best_practices.md"
        },
        {
          "name": "callbacks.md",
          "type": "file",
          "path": "references/callbacks.md"
        },
        {
          "name": "data_module.md",
          "type": "file",
          "path": "references/data_module.md"
        },
        {
          "name": "distributed_training.md",
          "type": "file",
          "path": "references/distributed_training.md"
        },
        {
          "name": "lightning_module.md",
          "type": "file",
          "path": "references/lightning_module.md"
        },
        {
          "name": "logging.md",
          "type": "file",
          "path": "references/logging.md"
        },
        {
          "name": "trainer.md",
          "type": "file",
          "path": "references/trainer.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "quick_trainer_setup.py",
          "type": "file",
          "path": "scripts/quick_trainer_setup.py"
        },
        {
          "name": "template_datamodule.py",
          "type": "file",
          "path": "scripts/template_datamodule.py"
        },
        {
          "name": "template_lightning_module.py",
          "type": "file",
          "path": "scripts/template_lightning_module.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
