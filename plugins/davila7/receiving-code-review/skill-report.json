{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:11:01.616Z",
    "slug": "davila7-receiving-code-review",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/development/receiving-code-review",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "2f6114025609bed0dcf13a9b4a583f625c9f27b91052c9c90a8833af9a83c804",
    "tree_hash": "78ce5bf886d094b382faf07ef477e959ccf33cbfd895727db602ae138283aea5"
  },
  "skill": {
    "name": "receiving-code-review",
    "description": "Use when receiving code review feedback, before implementing suggestions, especially if feedback seems unclear or technically questionable - requires technical rigor and verification, not performative agreement or blind implementation",
    "summary": "Use when receiving code review feedback, before implementing suggestions, especially if feedback see...",
    "icon": "üëÅÔ∏è",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "development",
    "tags": [
      "code-review",
      "communication",
      "technical-rigor",
      "feedback",
      "verification"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Pure prompt-based skill with no executable code. Contains only markdown instructions for AI behavior during code review feedback reception. No scripts, network calls, filesystem access, or external command execution detected.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 210,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:11:01.616Z"
  },
  "content": {
    "user_title": "Evaluate code review feedback with technical rigor",
    "value_statement": "AI assistants often implement code review feedback without verification. This skill ensures technical evaluation of suggestions, asking clarifying questions and pushing back when feedback is unclear or technically incorrect.",
    "seo_keywords": [
      "code review reception",
      "Claude code review",
      "technical feedback evaluation",
      "code review guidelines",
      "push back on code review",
      "verify code suggestions",
      "AI code review assistant",
      "Codex code review",
      "Claude Code code review",
      "software development feedback"
    ],
    "actual_capabilities": [
      "Restate feedback requirements in own words for verification",
      "Identify unclear feedback items before implementation",
      "Push back on technically incorrect suggestions with reasoning",
      "Check suggestions against codebase reality using grep",
      "Verify external reviewer context and full understanding",
      "Prioritize fixes by complexity and blocking status"
    ],
    "limitations": [
      "Does not execute code changes automatically",
      "Requires human partner for architectural decisions",
      "Does not replace human judgment on team standards",
      "Limited to evaluation, not implementation guidance"
    ],
    "use_cases": [
      {
        "target_user": "AI Development Assistants",
        "title": "Technical verification of suggestions",
        "description": "Ensure AI assistants verify code review feedback against codebase before implementing, preventing blind agreement"
      },
      {
        "target_user": "Development Teams",
        "title": "Consistent review handling",
        "description": "Maintain technical rigor when AI agents process external code review feedback from contributors"
      },
      {
        "target_user": "Code Reviewers",
        "title": "Quality feedback validation",
        "description": "Prevent AI assistants from implementing questionable suggestions without proper technical evaluation"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic review check",
        "scenario": "When receiving feedback",
        "prompt": "I am receiving code review feedback. Read all items completely. For each item: restate the requirement in my own words, verify against the codebase, and evaluate if technically sound. Ask clarification on unclear items before implementing anything."
      },
      {
        "title": "External reviewer",
        "scenario": "External contributor feedback",
        "prompt": "External reviewer suggested: [suggestion]. Before implementing: check if this is technically correct for this codebase, if it breaks existing functionality, and if the reviewer understands the full context. Push back with technical reasoning if needed."
      },
      {
        "title": "YAGNI verification",
        "scenario": "Feature suggestion",
        "prompt": "Reviewer suggests adding a feature. First grep the codebase to verify actual usage. If unused, suggest removal (YAGNI). If used, then implement properly. Report findings before proceeding."
      },
      {
        "title": "Multi-item clarification",
        "scenario": "Multiple feedback items",
        "prompt": "I received N items of feedback. Identify any unclear items. Do NOT implement partial changes. State what I understand and what needs clarification before touching any code."
      }
    ],
    "output_examples": [
      {
        "input": "Reviewer suggested removing legacy API code and adding proper metrics tracking",
        "output": [
          "‚úì Checking legacy API usage - grep shows 3 call sites for backward compat (OS 10.15+)",
          "‚úì Suggestion noted: verify if metrics endpoint is actually called before implementing",
          "Action: Will clarify scope before implementing either change"
        ]
      }
    ],
    "best_practices": [
      "Verify suggestions against actual codebase usage before implementing",
      "Ask clarifying questions on any unclear feedback items before proceeding",
      "Use technical reasoning when pushing back, not defensiveness"
    ],
    "anti_patterns": [
      "Blindly implementing feedback without verification",
      "Using performative agreement ('You're absolutely right!')",
      "Implementing partial items when some are unclear",
      "Avoiding pushback on technically incorrect suggestions"
    ],
    "faq": [
      {
        "question": "Is this skill compatible with all Claude models?",
        "answer": "Yes, the skill works with Claude, Codex, and Claude Code. It provides behavioral guidelines for code review handling."
      },
      {
        "question": "What happens if feedback is technically incorrect?",
        "answer": "The skill guides you to push back with technical reasoning, referencing tests or codebase evidence to support your position."
      },
      {
        "question": "Can this skill integrate with existing workflows?",
        "answer": "Yes, it overlays on any code review process. Apply the guidelines before implementing feedback in your current workflow."
      },
      {
        "question": "Does this skill access any files or data?",
        "answer": "No, this is a prompt-based skill. It provides behavioral instructions only and does not access any files or systems."
      },
      {
        "question": "What if I cannot verify a suggestion?",
        "answer": "State your limitation clearly: 'I cannot verify without X. Should I investigate further, ask, or proceed?'"
      },
      {
        "question": "How is this different from standard code review handling?",
        "answer": "It emphasizes technical verification over social performance, requiring evidence-based evaluation rather than blind agreement."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
