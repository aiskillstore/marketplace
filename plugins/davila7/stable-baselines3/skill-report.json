{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T00:58:01.651Z",
    "slug": "davila7-stable-baselines3",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/scientific/stable-baselines3",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "5cfa40c21dfe061b3833343a0d76f76175726ed9e1ce4827ef6032b1970af0c5",
    "tree_hash": "a7c8063ce67c1a931e6a3d33c2c6333d685f8a88a7dd543a99361b8c49d31221"
  },
  "skill": {
    "name": "stable-baselines3",
    "description": "Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DDPG, A2C, etc.), creating custom Gym environments, implementing callbacks for monitoring and control, using vectorized environments for parallel training, and integrating with deep RL workflows. This skill should be used when users request RL algorithm implementation, agent training, environment design, or RL experimentation.",
    "summary": "Use this skill for reinforcement learning tasks including training RL agents (PPO, SAC, DQN, TD3, DD...",
    "icon": "ðŸ§ ",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "scientific",
    "tags": [
      "reinforcement-learning",
      "machine-learning",
      "stable-baselines3",
      "pytorch",
      "robotics"
    ],
    "supported_tools": [
      "claude",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a pure documentation and template code skill for the Stable Baselines3 reinforcement learning library. All Python scripts are standard RL templates that use only local file operations for model/log directories, read from and write to user-specified paths, and rely entirely on legitimate Gymnasium and SB3 library APIs. No network calls, no command execution, no environment variable access, no sensitive data collection.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 8,
    "total_lines": 2408,
    "audit_model": "claude",
    "audited_at": "2026-01-07T00:58:01.651Z"
  },
  "content": {
    "user_title": "Train reinforcement learning agents",
    "value_statement": "Reinforcement learning requires deep expertise in algorithms, environments, and training workflows. This skill provides ready-to-use templates and guides for Stable Baselines3 so you can quickly train RL agents without learning all the details upfront.",
    "seo_keywords": [
      "stable-baselines3",
      "reinforcement learning",
      "PPO SAC DQN",
      "Claude Code skill",
      "custom gym environments",
      "vectorized environments",
      "RL agent training",
      "machine learning"
    ],
    "actual_capabilities": [
      "Train RL agents using PPO, SAC, DQN, TD3, A2C, DDPG, and HER algorithms",
      "Create custom Gymnasium environments compatible with Stable Baselines3",
      "Implement callbacks for monitoring, checkpointing, and early stopping",
      "Use vectorized environments for parallel training performance",
      "Evaluate trained agents with statistics and video recording"
    ],
    "limitations": [
      "Requires stable-baselines3 library installation",
      "Does not include pre-trained models",
      "Template code requires user to specify environment and paths",
      "Does not handle hyperparameter tuning automatically"
    ],
    "use_cases": [
      {
        "target_user": "ML engineers",
        "title": "Deploy RL for robotics",
        "description": "Train continuous control policies for robotics using SAC or TD3 algorithms with parallel environments."
      },
      {
        "target_user": "Game AI developers",
        "title": "Build game-playing agents",
        "description": "Create agents that learn to play games using PPO or DQN with custom game environments."
      },
      {
        "target_user": "Researchers",
        "title": "Experiment with RL algorithms",
        "description": "Prototype new RL approaches using callbacks, custom environments, and algorithm comparisons."
      }
    ],
    "prompt_templates": [
      {
        "title": "Train basic agent",
        "scenario": "I want to train a PPO agent",
        "prompt": "Train a PPO agent on the CartPole-v1 environment. Use 4 parallel environments, save checkpoints every 10000 steps, and log to TensorBoard. Show me the complete training code."
      },
      {
        "title": "Create custom environment",
        "scenario": "I need a custom Gym environment",
        "prompt": "Create a custom Gymnasium environment where an agent navigates a 10x10 grid to reach a goal. Include proper observation space, action space, and reward function. Validate it with check_env."
      },
      {
        "title": "Add monitoring callbacks",
        "scenario": "I want to monitor training",
        "prompt": "Set up evaluation callbacks that save the best model, stop training when reward exceeds 200, and log custom metrics like episode length to TensorBoard during PPO training."
      },
      {
        "title": "Optimize training",
        "scenario": "I want faster training",
        "prompt": "Show me how to use SubprocVecEnv with 8 parallel environments, VecNormalize for reward normalization, and proper checkpoint frequency settings for efficient off-policy training with SAC."
      }
    ],
    "output_examples": [
      {
        "input": "Train a PPO agent on CartPole with checkpoints and evaluation",
        "output": [
          "âœ“ Created 4 parallel training environments using SubprocVecEnv",
          "âœ“ Set up EvalCallback: evaluates every 2500 steps, saves best model to ./models/best_model/",
          "âœ“ Set up CheckpointCallback: saves every 2500 steps with prefix 'ppo_cartpole'",
          "âœ“ Initialized PPO with MlpPolicy, learning_rate=3e-4, n_steps=2048",
          "âœ“ Training for 100000 timesteps with TensorBoard logging to ./logs/",
          "âœ“ Run 'tensorboard --logdir ./logs/' to view progress"
        ]
      }
    ],
    "best_practices": [
      "Always validate custom environments with check_env() before training",
      "Divide eval_freq and save_freq by n_envs when using vectorized environments",
      "Save VecNormalize statistics separately from model for correct evaluation"
    ],
    "anti_patterns": [
      "Using DQN with continuous action spaces (only works with discrete)",
      "Not adjusting hyperparameters when switching between on-policy and off-policy algorithms",
      "Training without evaluation callbacks (hard to determine when to stop)"
    ],
    "faq": [
      {
        "question": "Which algorithm should I choose for my task?",
        "answer": "PPO for general-purpose tasks, SAC for continuous control, DQN for discrete actions. See algorithms.md for detailed comparison."
      },
      {
        "question": "Can I use this with GPU acceleration?",
        "answer": "Stable Baselines3 uses PyTorch automatically. Set device='cuda' in model constructor for GPU training."
      },
      {
        "question": "How do I save and load trained models?",
        "answer": "Use model.save(path) to save and PPO.load(path, env=env) to load. Save VecNormalize stats separately."
      },
      {
        "question": "Is my data safe during training?",
        "answer": "This skill only reads/writes to user-specified directories. No data is sent to external servers."
      },
      {
        "question": "Why is training unstable?",
        "answer": "Check reward scaling, try VecNormalize, verify observation space dtype is np.float32, and validate environment with check_env."
      },
      {
        "question": "How does this compare to RLlib or other frameworks?",
        "answer": "Stable Baselines3 offers simpler APIs for single-agent RL, while RLlib scales better for distributed training."
      }
    ]
  },
  "file_structure": [
    {
      "name": "references",
      "type": "dir",
      "path": "references",
      "children": [
        {
          "name": "algorithms.md",
          "type": "file",
          "path": "references/algorithms.md"
        },
        {
          "name": "callbacks.md",
          "type": "file",
          "path": "references/callbacks.md"
        },
        {
          "name": "custom_environments.md",
          "type": "file",
          "path": "references/custom_environments.md"
        },
        {
          "name": "vectorized_envs.md",
          "type": "file",
          "path": "references/vectorized_envs.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "custom_env_template.py",
          "type": "file",
          "path": "scripts/custom_env_template.py"
        },
        {
          "name": "evaluate_agent.py",
          "type": "file",
          "path": "scripts/evaluate_agent.py"
        },
        {
          "name": "train_rl_agent.py",
          "type": "file",
          "path": "scripts/train_rl_agent.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
