{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:10:35.846Z",
    "slug": "davila7-nowait-reasoning-optimizer",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/productivity/nowait",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "fd71e79f36014d423e296fa7e71e48aa5ec437c15a8d096033766a898afc8a25",
    "tree_hash": "5668fcda79efe82dc801baee5d7e7cbb4ecb632426a912ac791f4ff161121f16"
  },
  "skill": {
    "name": "nowait-reasoning-optimizer",
    "description": "Implements the NOWAIT technique for efficient reasoning in R1-style LLMs. Use when optimizing inference of reasoning models (QwQ, DeepSeek-R1, Phi4-Reasoning, Qwen3, Kimi-VL, QvQ), reducing chain-of-thought token usage by 27-51% while preserving accuracy. Triggers on \"optimize reasoning\", \"reduce thinking tokens\", \"efficient inference\", \"suppress reflection tokens\", or when working with verbose CoT outputs.",
    "summary": "Implements the NOWAIT technique for efficient reasoning in R1-style LLMs. Use when optimizing infere...",
    "icon": "⚡",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "productivity",
    "tags": [
      "llm-optimization",
      "reasoning-models",
      "token-reduction",
      "inference"
    ],
    "supported_tools": [
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "low",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "Legitimate ML optimization utility implementing a research paper technique. Contains Python code for token manipulation during LLM inference, with no network access, file I/O beyond tokenizer, or external command execution.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 3,
    "total_lines": 590,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:10:35.846Z"
  },
  "content": {
    "user_title": "Reduce LLM reasoning tokens by 50%",
    "value_statement": "Chain-of-thought reasoning models generate verbose self-reflection tokens that increase costs and latency. This skill implements the NOWAIT technique to suppress unnecessary reflection tokens during inference, reducing token usage by 27-51% while maintaining accuracy on RL-based reasoning models.",
    "seo_keywords": [
      "Claude Code reasoning optimizer",
      "NOWAIT technique LLM",
      "chain-of-thought token reduction",
      "QwQ-32B optimization",
      "DeepSeek-R1 inference",
      "reasoning model efficiency",
      "token suppression ML",
      "LLM cost optimization"
    ],
    "actual_capabilities": [
      "Suppresses self-reflection tokens (wait, hmm, but, however) during LLM generation",
      "Reduces chain-of-thought tokens by 27-51% on RL-based models",
      "Works with QwQ-32B, Phi4-Reasoning-Plus, Qwen3-32B, Kimi-VL, and QvQ",
      "Integrates with HuggingFace Transformers and vLLM frameworks",
      "Configurable keywords and exclusion patterns for custom tuning"
    ],
    "limitations": [
      "Less effective on distilled models (Qwen3-4B/8B/14B) which may show accuracy degradation",
      "Simple problems with minimal CoT see reduced benefits",
      "Some domains may require model-specific keyword tuning"
    ],
    "use_cases": [
      {
        "target_user": "ML Engineers",
        "title": "Optimize production inference",
        "description": "Deploy efficient reasoning models with reduced compute costs and latency for production systems"
      },
      {
        "target_user": "Researchers",
        "title": "Reduce benchmarking costs",
        "description": "Run large-scale reasoning benchmarks with 30-50% fewer tokens while preserving accuracy"
      },
      {
        "target_user": "API Developers",
        "title": "Cut token usage fees",
        "description": "Lower API costs when using reasoning models by suppressing verbose reflection patterns"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic NOWAIT setup",
        "scenario": "Initialize token suppression",
        "prompt": "Use the NOWAIT Reasoning Optimizer to suppress self-reflection tokens during generation. Initialize NOWAITLogitProcessor with the model's tokenizer and apply it during model.generate() with max_new_tokens=32768."
      },
      {
        "title": "vLLM integration",
        "scenario": "Optimize vLLM deployment",
        "prompt": "Configure vLLM to use NOWAIT by calling get_nowait_bad_words_ids() with the tokenizer and pass the result to SamplingParams for efficient batch inference."
      },
      {
        "title": "Custom keywords",
        "scenario": "Tune suppression for domain",
        "prompt": "Create a custom NOWAITConfig with domain-specific keywords to suppress, excluding false positives like 'butterfly' or 'checkout' that should not be filtered."
      },
      {
        "title": "Hybrid approach",
        "scenario": "Balance suppression and flexibility",
        "prompt": "Use NOWAITStoppingCriteria instead of full suppression to allow some reflection tokens but stop generation if reflection count exceeds a configurable threshold."
      }
    ],
    "output_examples": [
      {
        "input": "Optimize inference for QwQ-32B to reduce thinking tokens",
        "output": [
          "✓ Initialized NOWAIT with 17 reflection keywords",
          "✓ Suppressing: wait, hmm, but, however, alternatively...",
          "✓ Excluded false positives: ohio, button, checkout...",
          "✓ Token set built: ~N tokens identified for suppression",
          "✓ Ready for logits_processor integration"
        ]
      }
    ],
    "best_practices": [
      "Test token reduction on your specific model before production deployment",
      "Monitor accuracy on hard tasks when using NOWAIT on distilled models",
      "Use the exclusion patterns to prevent false positives on legitimate words"
    ],
    "anti_patterns": [
      "Applying NOWAIT to distilled small models without accuracy validation",
      "Using NOWAIT on non-reasoning models that do not generate reflection tokens",
      "Suppressing keywords without checking excluded patterns first"
    ],
    "faq": [
      {
        "question": "Which models work best with NOWAIT?",
        "answer": "RL-based models like QwQ-32B, Phi4-Reasoning-Plus, and Qwen3-32B show 27-51% token reduction. Distilled models may lose accuracy."
      },
      {
        "question": "What token reduction can I expect?",
        "answer": "Math tasks see 30% reduction, visual QA up to 50%, and video QA around 27%. Results vary by model and task complexity."
      },
      {
        "question": "Does NOWAIT affect answer accuracy?",
        "answer": "RL-based models maintain stable accuracy. Distilled models may show degradation on challenging tasks. Always validate for your use case."
      },
      {
        "question": "Can I customize which tokens are suppressed?",
        "answer": "Yes, provide custom keywords and excluded patterns via NOWAITConfig for domain-specific tuning."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "Yes, NOWAIT runs locally during inference and does not transmit data externally. It only manipulates model logits."
      },
      {
        "question": "How does NOWAIT compare to other optimization techniques?",
        "answer": "NOWAIT is training-free and works at inference time. It complements other techniques like quantization and KV cache optimization."
      }
    ]
  },
  "file_structure": [
    {
      "name": "refrences",
      "type": "dir",
      "path": "refrences",
      "children": [
        {
          "name": "keywords.md",
          "type": "file",
          "path": "refrences/keywords.md"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "dir",
      "path": "scripts",
      "children": [
        {
          "name": "nowait_processor.py",
          "type": "file",
          "path": "scripts/nowait_processor.py"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
