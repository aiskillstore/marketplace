{
  "schema_version": "2.0",
  "meta": {
    "generated_at": "2026-01-07T01:38:32.759Z",
    "slug": "davila7-verification-before-completion",
    "source_url": "https://github.com/davila7/claude-code-templates/tree/main/cli-tool/components/skills/development/verification-before-completion",
    "source_ref": "main",
    "model": "claude",
    "analysis_version": "2.0.0",
    "source_type": "community",
    "content_hash": "963f6b63d58d88e029181126528a1c720f76c4f8cfaec3532ece14795aef5b77",
    "tree_hash": "a47ef4ef5b04c2bc29852a290adc9789c5480fde2db005d4f45b021355653408"
  },
  "skill": {
    "name": "verification-before-completion",
    "description": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always",
    "summary": "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - req...",
    "icon": "üõ°Ô∏è",
    "version": "1.0.0",
    "author": "davila7",
    "license": "MIT",
    "category": "development",
    "tags": [
      "workflow",
      "quality",
      "verification",
      "testing",
      "best-practices"
    ],
    "supported_tools": [
      "claude",
      "codex",
      "claude-code"
    ],
    "risk_factors": []
  },
  "security_audit": {
    "risk_level": "safe",
    "is_blocked": false,
    "safe_to_publish": true,
    "summary": "This is a prompt-only behavioral guideline with no executable code. Contains only documentation instructing AI agents to verify commands before claiming completion. No scripts, network calls, filesystem access, or external commands present.",
    "risk_factor_evidence": [],
    "critical_findings": [],
    "high_findings": [],
    "medium_findings": [],
    "low_findings": [],
    "dangerous_patterns": [],
    "files_scanned": 1,
    "total_lines": 140,
    "audit_model": "claude",
    "audited_at": "2026-01-07T01:38:32.759Z"
  },
  "content": {
    "user_title": "Verify Before Claiming Completion",
    "value_statement": "AI agents often claim work is complete without running verification commands. This skill enforces a strict gate that requires running actual verification commands and reading output evidence before making any completion claims or expressing satisfaction.",
    "seo_keywords": [
      "Claude verification skill",
      "completion verification",
      "test verification before commit",
      "PR verification workflow",
      "code quality gate",
      "CLAUDE Code verification",
      "Codex testing skill",
      "assertion with evidence",
      "honest AI completion claims"
    ],
    "actual_capabilities": [
      "Prevents false completion claims by requiring fresh verification command output",
      "Creates a gate function for identifying, running, and reading verification commands",
      "Blocks expressions of satisfaction without evidence (Great!, Perfect!, Done!)",
      "Requires checking exit codes and failure counts, not just partial output",
      "Validates agent delegation by requiring independent VCS diff verification"
    ],
    "limitations": [
      "Does not run verification commands automatically - agent must execute them",
      "Only applies when explicitly invoked or included in context",
      "Cannot prevent humans from bypassing verification steps",
      "Relies on agent cooperation to follow the verification gate"
    ],
    "use_cases": [
      {
        "target_user": "Software Developers",
        "title": "PR Quality Gate",
        "description": "Verify tests, builds, and linters actually pass before creating pull requests"
      },
      {
        "target_user": "QA Engineers",
        "title": "Test Verification",
        "description": "Confirm bug fixes work by running regression tests with red-green verification"
      },
      {
        "target_user": "AI Agent Users",
        "title": "Trust Building",
        "description": "Prevent AI agents from claiming completion without actual evidence"
      }
    ],
    "prompt_templates": [
      {
        "title": "Basic Test Check",
        "scenario": "Before claiming tests pass",
        "prompt": "Have you run the test command and seen all tests pass with 0 failures in the output?"
      },
      {
        "title": "Build Verification",
        "scenario": "Before claiming build succeeds",
        "prompt": "Run the build command and verify the exit code is 0. Report what you observed."
      },
      {
        "title": "Bug Fix Confirmation",
        "scenario": "Before claiming bug is fixed",
        "prompt": "What was the original bug symptom? Run the command that reproduces it and confirm the fix works."
      },
      {
        "title": "Agent Delegation",
        "scenario": "Before trusting agent report",
        "prompt": "The agent reports success. Check the VCS diff independently and verify the changes address the original requirement."
      }
    ],
    "output_examples": [
      {
        "input": "The bug is fixed",
        "output": [
          "Incorrect: The bug should be fixed now",
          "Correct: Ran reproduction command. Expected error no longer appears. Exit code 0. Bug is confirmed fixed.",
          "Incorrect: Tests are passing",
          "Correct: Ran 'npm test'. Output shows 34/34 tests passed with 0 failures. All tests pass."
        ]
      }
    ],
    "best_practices": [
      "Always run the exact verification command fresh - do not rely on previous runs",
      "Read the full output including exit codes and failure counts",
      "Report what you actually observed, not what you assume or hope is true"
    ],
    "anti_patterns": [
      "Using words like should, probably, or seems to before verification",
      "Expressing satisfaction before running verification commands",
      "Trusting agent success reports without independent verification"
    ],
    "faq": [
      {
        "question": "Does this skill run commands automatically?",
        "answer": "No. The skill provides behavioral guidelines that the agent must follow. The agent must still run verification commands manually."
      },
      {
        "question": "What counts as a valid verification command?",
        "answer": "Any command whose output proves the claim. For tests, run the test suite. For builds, run the build. For bugs, run reproduction steps."
      },
      {
        "question": "Does this work with all testing frameworks?",
        "answer": "Yes. The skill is framework-agnostic. It works with Jest, Pytest, Go tests, Maven, and any tool with command-line output."
      },
      {
        "question": "Is my data safe when using this skill?",
        "answer": "Yes. This skill contains only text instructions. It does not access files, networks, or external services."
      },
      {
        "question": "Why does the skill block positive words like Great or Perfect?",
        "answer": "These words imply completion without evidence. The skill requires you to first run verification and then state the claim with actual evidence."
      },
      {
        "question": "How is this different from other testing skills?",
        "answer": "Most testing skills execute tests. This skill is a behavioral gate that prevents false completion claims regardless of what tool runs the tests."
      }
    ]
  },
  "file_structure": [
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "SKILL.md"
    }
  ]
}
